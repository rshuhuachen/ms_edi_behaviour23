[
  {
    "objectID": "qmd/1_summary_identities.html",
    "href": "qmd/1_summary_identities.html",
    "title": "2  Abstract submission, registration and awardees",
    "section": "",
    "text": "3 Map for Main Figure"
  },
  {
    "objectID": "qmd/1_summary_identities.html#abstract-submission",
    "href": "qmd/1_summary_identities.html#abstract-submission",
    "title": "2  Abstract submission, registration and awardees",
    "section": "2.1 Abstract submission",
    "text": "2.1 Abstract submission\nThere were a total of 729 abstract submissions, of which the majority used she/her pronouns (hereafter referred to as “women”) (456 / 729). (456 / 729) used he/him pronouns (hereafter referred to as “men”), whereas 10 used she/them, he/them, or they/them pronouns (hereafter referred to as “gender queer”).\nA total of 59 nationalities were represented among those who submitted an abstract. The majority of the abstract submitters originated from Europe (504/729), followed by Asia (101/729), North America (64/729), South America (21/729), Oceania (20/729) and lastly Africa (8/729). Out of all who submitted an abstract, 0 were affiliated with another country than their nationality (Expat status)."
  },
  {
    "objectID": "qmd/1_summary_identities.html#registration",
    "href": "qmd/1_summary_identities.html#registration",
    "title": "2  Abstract submission, registration and awardees",
    "section": "2.2 Registration",
    "text": "2.2 Registration\nThere were a total of 727 registrations, the majority of which were submitted by women (451 / 727). 233 registrants were men, 15 registrants identified as gender queer.\nA total of 59 nationalities were represented among those who registered. The majority of registrations originated from Europe (481/727), followed by Asia (85/727), North America (48/727), Oceania (20/727), South America (18/727), and Africa (5/727)."
  },
  {
    "objectID": "qmd/1_summary_identities.html#awards",
    "href": "qmd/1_summary_identities.html#awards",
    "title": "2  Abstract submission, registration and awardees",
    "section": "2.3 Awards",
    "text": "2.3 Awards\nA total of 14 awards were given at the congress. One Alcock award, 10 ‘best student talk’ awards and three poster awards. The winners were all affiliated with European (including UK) institutions, representing 7 nations. Most winners were associated with German institutions (7/13). The awards were given to 5 women and 3 men, however 6 did not wish to disclose their pronouns (See Figure 2.2)."
  },
  {
    "objectID": "qmd/1_summary_identities.html#figures",
    "href": "qmd/1_summary_identities.html#figures",
    "title": "2  Abstract submission, registration and awardees",
    "section": "2.4 Figures",
    "text": "2.4 Figures\n\n\n\n\n\nFigure 2.1: Gender distribution\n\n\n\n\n\n\n\n\n\nNationality chart\n\n\n\n\n\n\n\n\n\nFigure 2.2: ?(caption)"
  },
  {
    "objectID": "qmd/1_summary_identities.html#nationality-data",
    "href": "qmd/1_summary_identities.html#nationality-data",
    "title": "2  Abstract submission, registration and awardees",
    "section": "3.1 Nationality data",
    "text": "3.1 Nationality data\n\n3.1.1 Continent and country representation\nA total of 59 nationalities were represented.\n\n\n\nContinent representation \n\n\nContinent\nCountries\nParticipation\n\n\n\n\nAfrica\n5\n5\n\n\nAsia\n16\n85\n\n\nEurope\n27\n482\n\n\nNorth America\n4\n48\n\n\nOceania\n2\n20\n\n\nSouth America\n4\n18\n\n\n\nRepresentation pie chart\n\n\n\n\n\nRepresentation pie chart\n\n\n\n\n\n       Algeria      Argentina      Australia        Austria     Azerbaijan \n             1              1             17             24              1 \n    Bangladesh        Belarus        Belgium         Brazil         Canada \n             2              1              9              8             13 \n         China       Colombia     Costa Rica        Croatia         Cyprus \n             8              6              2              7              1 \nCzech Republic        Denmark        Ecuador        Finland         France \n             9              8              3             10             43 \n        Gambia        Germany         Greece      Hong Kong        Hungary \n             1            166              5              2             10 \n         India           Iran        Ireland         Israel          Italy \n            41              2              4              3             33 \n         Japan        Lebanon     Luxembourg      Mauritius         Mexico \n            10              1              1              1              5 \n    Montenegro    Netherlands    New Zealand        Nigeria         Norway \n             1             17              3              1              1 \n      Pakistan         Poland       Portugal         Russia         Serbia \n             1              4             14              1              1 \n     Singapore       Slovenia          Spain      Sri Lanka         Sweden \n             1              2             21              2              3 \n   Switzerland         Taiwan       Thailand         Turkey         Uganda \n            20              4              2              4              1 \n            UK        Ukraine            USA \n            66              1             28"
  },
  {
    "objectID": "qmd/1_summary_identities.html#map",
    "href": "qmd/1_summary_identities.html#map",
    "title": "2  Abstract submission, registration and awardees",
    "section": "3.2 Map",
    "text": "3.2 Map\nNow, we can build our map."
  },
  {
    "objectID": "qmd/2_question_asking_general.html#sec-mainmodel",
    "href": "qmd/2_question_asking_general.html#sec-mainmodel",
    "title": "3  Question asking",
    "section": "3.1 Observational data",
    "text": "3.1 Observational data\nTo identify a gender disparity in question asking behaviour from the observational data, we fitted a binomial generalized linear mixed effect model (GLMM), where the dependent variable indicates whether a question was asked by a woman (1) or a man (0), while accounting for the gender proportion of the audience and the nonindependence of talks within a session.\nFirst, let’s have a look at the data used to build this model. To answer this research question, we only focus on a subset of the data: those sessions that were unmanipulated, and we excluded questions that were a follow up question from the same person, asked without raising a hand first, and asked by the host. Before building the model, we make sure that some variables such as the day, room number, the duration of the Q&A, whether a talk was given as part of the general sessions or symposia, talk number within a session, and question number was not associated with the gender of the quesioner.\nLastly, we repeat the analysis with a conserved dataset that excludes any data that had some type of uncertainty.\n\n3.1.1 Preparing data and validation of potential covariates\n\n# subset data\n\ndata_control &lt;- subset(data_analysis, treatment == \"Control\" &\n                     is.na(followup) & is.na(jumper)&\n                     is.na(host_asks) & \n                    !grepl(\"speaker|questioner\", allocator_question))\n\ndata_control &lt;- droplevels(data_control)\n# second dataset to test robustness to data with uncertainty\n\ndata_conserved &lt;- subset(data_control, uncertainty_count_audience == 0 &\n                           uncertainty_count_hands == 0)\n\n# explore data\n\ndata_control %&gt;% select(c(session_id, talk_nr, question_nr, talk_id, gender_questioner_female, audience_total, audience_women_prop)) %&gt;% str()\n\ntibble [366 × 7] (S3: tbl_df/tbl/data.frame)\n $ session_id              : num [1:366] 28 28 28 28 28 28 28 28 28 28 ...\n $ talk_nr                 : num [1:366] 1 2 2 3 3 4 5 5 5 6 ...\n $ question_nr             : num [1:366] 1 1 2 1 2 2 1 2 3 3 ...\n $ talk_id                 : Factor w/ 130 levels \"15_1\",\"15_2\",..: 26 27 27 28 28 29 30 30 30 31 ...\n $ gender_questioner_female: num [1:366] 0 0 1 0 0 0 1 0 1 0 ...\n $ audience_total          : num [1:366] 63.5 53 53 70.5 70.5 32.5 33 33 33 46 ...\n $ audience_women_prop     : num [1:366] 0.591 0.623 0.623 0.752 0.752 ...\n\n# how many questions were asked by women (1) and men (0) per session?\ntable(data_control$session_id, data_control$gender_questioner_female) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\n0\n1\n\n\n\n\n4\n9\n12\n\n\n6\n5\n6\n\n\n15\n10\n11\n\n\n22\n6\n11\n\n\n25\n8\n6\n\n\n26\n10\n6\n\n\n28\n9\n6\n\n\n32\n9\n4\n\n\n34\n11\n6\n\n\n36\n10\n8\n\n\n37\n11\n7\n\n\n42\n0\n2\n\n\n44\n5\n7\n\n\n45\n12\n12\n\n\n48\n9\n8\n\n\n55\n8\n5\n\n\n61\n13\n5\n\n\n63\n4\n12\n\n\n69\n4\n3\n\n\n74\n10\n9\n\n\n80\n5\n6\n\n\n81\n3\n7\n\n\n83\n10\n12\n\n\n86\n5\n3\n\n\n\n\n\n\n# validation some potential confounding variables\nsummary(glmer(gender_questioner_female ~ symp_general + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_control)) #NS\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ symp_general + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   482.4    497.8   -237.2    474.4      346 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5200 -0.9364 -0.7031  1.0215  1.5987 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 1e-14    1e-07   \n session_id         (Intercept) 0e+00    0e+00   \nNumber of obs: 350, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            -0.8677     0.1856  -4.675 2.94e-06 ***\nsymp_generalSymposium   0.3189     0.2293   1.391    0.164    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nsymp_gnrlSy -0.809\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nsummary(glmer(gender_questioner_female ~ talk_nr + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_control)) #NS\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ talk_nr + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   483.7    499.1   -237.8    475.7      346 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6121 -0.9172 -0.6996  1.0542  1.5194 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 350, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.82656    0.23666  -3.493 0.000478 ***\ntalk_nr      0.04764    0.05991   0.795 0.426537    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr)\ntalk_nr -0.888\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nsummary(glmer(gender_questioner_female ~ question_nr + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_control)) #NS\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ question_nr + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   484.3    499.7   -238.1    476.3      346 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5876 -0.9170 -0.7136  1.0321  1.4711 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev. \n talk_id:session_id (Intercept) 4.949e-16 2.225e-08\n session_id         (Intercept) 0.000e+00 0.000e+00\nNumber of obs: 350, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.69176    0.20582  -3.361 0.000777 ***\nquestion_nr  0.01258    0.06845   0.184 0.854147    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nquestion_nr -0.849\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nsummary(glmer(gender_questioner_female ~ no_observers + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_control)) #NS\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ no_observers + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   483.4    498.8   -237.7    475.4      346 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6128 -0.9208 -0.7088  1.0663  1.5360 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev. \n talk_id:session_id (Intercept) 2.089e-16 1.445e-08\n session_id         (Intercept) 0.000e+00 0.000e+00\nNumber of obs: 350, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.8709     0.2447  -3.559 0.000372 ***\nno_observers   0.1179     0.1222   0.965 0.334629    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nno_observrs -0.896\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nsummary(glmer(gender_questioner_female ~ duration_qa + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_control)) #NS\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ duration_qa + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   441.5    456.5   -216.7    433.5      315 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5755 -0.9050 -0.7259  0.9971  1.4105 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 319, groups:  talk_id:session_id, 118; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.01371    0.28644  -3.539 0.000402 ***\nduration_qa  0.08294    0.05720   1.450 0.147087    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nduration_qa -0.917\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nsummary(glmer(gender_questioner_female ~ time + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_control)) #NS\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ time + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   483.6    499.0   -237.8    475.6      346 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4985 -0.9274 -0.7079  0.9989  1.5317 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 1e-14    1e-07   \n session_id         (Intercept) 0e+00    0e+00   \nNumber of obs: 350, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7822     0.1779  -4.397  1.1e-05 ***\ntimeMorning   0.1966     0.2250   0.874    0.382    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ntimeMorning -0.791\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNothing is significant, so let’s build the model\n\nm_qa_general &lt;- glmer(gender_questioner_female ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_control)\n\n# model output\nsummary(m_qa_general)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ 1 + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   482.3    493.9   -238.2    476.3      347 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5932 -0.9173 -0.7136  1.0366  1.4660 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev. \n talk_id:session_id (Intercept) 2.603e-15 5.102e-08\n session_id         (Intercept) 0.000e+00 0.000e+00\nNumber of obs: 350, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.6597     0.1088  -6.065 1.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n# use helper function to collect model output in data frame\nm_qa_general_out &lt;- collect_out(model = m_qa_general, null = NA,  name = \"QA_justIC\", n_factors = 0, type = \"qa\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_general_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_justIC\n\n\nAIC\n482.331\n\n\nn_obs\n350\n\n\nlrt_pval\nNA\n\n\nlrt_chisq\nNA\n\n\nintercept_estimate\n-0.66\n\n\nintercept_estimate_prop\n0.341\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-0.873\n\n\nintercept_ci_higher\n-0.447\n\n\nn_factors\n0\n\n\n\n\n\n\n\nLooking at the model output, The probability that a woman asks a question is 34.082 % while taking into account the gender proportion of the audience. The null hypothesis therefore predicts that this probability is around 50%"
  },
  {
    "objectID": "qmd/2_question_asking_general.html#plenary-sessions",
    "href": "qmd/2_question_asking_general.html#plenary-sessions",
    "title": "3  Question asking",
    "section": "3.2 Plenary sessions",
    "text": "3.2 Plenary sessions\nWe repeated the analysis above with data collected during plenary sessions (N = 11) only. During plenary sessions, we only collected data on the gender of people asking questions, as we could not count the audience reliability due to the size of the room. Instead, we use the proportion of women who registered for the congress to correct for the gender proportions in the audience.\n\n\n\nPlenary speakers\n\n\n\n# explore data\nplenary %&gt;% head() %&gt;% kbl() %&gt;% kable_classic_2() \n\n\n\n\nsession_id\nspeaker_pronoun\nquestion_nr\nquestioner_gender\nquestioner_female\nprop_audience_female\n\n\n\n\nP1\nShe/her\n1\nM\n0\n0.6452074\n\n\nP1\nShe/her\n2\nM\n0\n0.6452074\n\n\nP1\nShe/her\n3\nM\n0\n0.6452074\n\n\nP1\nShe/her\n4\nM\n0\n0.6452074\n\n\nP1\nShe/her\n5\nM\n0\n0.6452074\n\n\nP1\nShe/her\n6\nM\n0\n0.6452074\n\n\n\n\n\n\n# how many questions were asked by women (1) and men (0) per plenary talk?\n\ntable(plenary$session_id, plenary$questioner_female)\n\n     \n      0 1\n  P1  9 2\n  P10 5 0\n  P11 5 4\n  P2  4 0\n  P3  6 0\n  P4  4 3\n  P5  3 2\n  P6  0 1\n  P7  3 2\n  P9  4 3\n\n# model similar to above\nm_plenary &lt;- glmer(questioner_female ~ 1 + (1|session_id), data = plenary,\n              offset = boot::logit(prop_audience_female), family = \"binomial\")\n\nsummary(m_plenary)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: questioner_female ~ 1 + (1 | session_id)\n   Data: plenary\n Offset: boot::logit(prop_audience_female)\n\n     AIC      BIC   logLik deviance df.resid \n    75.5     79.7    -35.8     71.5       58 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.6458 -0.6340 -0.6099  1.5484  1.6361 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n session_id (Intercept) 0.04703  0.2169  \nNumber of obs: 60, groups:  session_id, 10\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.5360     0.3106  -4.945 7.61e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# helper function to collect output \nm_plenary_out &lt;- collect_out(model = m_plenary, null = NA, name = \"QA_plenary_justIC\", n_factors = 0,type = \"qa\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_plenary_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_plenary_justIC\n\n\nAIC\n75.515\n\n\nn_obs\n60\n\n\nlrt_pval\nNA\n\n\nlrt_chisq\nNA\n\n\nintercept_estimate\n-1.536\n\n\nintercept_estimate_prop\n0.177\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-2.418\n\n\nintercept_ci_higher\n-0.927\n\n\nn_factors\n0\n\n\n\n\n\n\n# we don't have enough power to present the output of the following model, but as a curiosity we wondered if the gender disparity changed depending on the gender of the speaker\n\nsummary(glmer(questioner_female ~ speaker_pronoun + (1|session_id), data = plenary,\n              offset = boot::logit(prop_audience_female), family = \"binomial\"))\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: questioner_female ~ speaker_pronoun + (1 | session_id)\n   Data: plenary\n Offset: boot::logit(prop_audience_female)\n\n     AIC      BIC   logLik deviance df.resid \n    75.0     81.3    -34.5     69.0       57 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.8528 -0.5303 -0.5303  1.1726  1.8856 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n session_id (Intercept) 0        0       \nNumber of obs: 60, groups:  session_id, 10\n\nFixed effects:\n                       Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)             -0.9165     0.4647  -1.972   0.0486 *\nspeaker_pronounShe/her  -0.9501     0.5986  -1.587   0.1125  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nspkr_prnnS/ -0.776\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n# although not significant, the gender bias seems to get worse when the speaker is female (intercept more negative)"
  },
  {
    "objectID": "qmd/2_question_asking_general.html#self-reports",
    "href": "qmd/2_question_asking_general.html#self-reports",
    "title": "3  Question asking",
    "section": "3.3 Self-reports",
    "text": "3.3 Self-reports\nSimilarly, we identified a gender disparity in question asking using the self-reports from the post-congress survey by fitting a binomial generalized linear model (GLM), using the binomial response to the question “Did you ask a question at the congress” (1 = yes, 0 = no) as the dependent variable and the self-reported gender identity (woman, man, non-binary, other) as the independent variable.\nFirst, let’s look at the data structure. Due to the difference in data structure, here we include gender as a fixed effect and assess the performance of this model compared to the null model (just the intercept) with a likelihood-ratio test.\n\nsurvey %&gt;% select(c(id, gender, pronoun, age, ask_questions)) %&gt;% head()\n\n  id gender pronoun         age ask_questions\n1  1   Male  He/him  &lt; 35 years            No\n2  2   Male  He/him  &gt; 50 years           Yes\n3  3   Male  He/him 35-50 years           Yes\n4  4   Male  He/him  &lt; 35 years            No\n5  5 Female She/her 35-50 years            No\n6  6   Male  He/him  &lt; 35 years            No\n\ntable(survey$gender, survey$ask_questions) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nNo\nYes\n\n\n\n\nFemale\n103\n151\n\n\nMale\n33\n79\n\n\nNon-binary\n1\n6\n\n\n\n\n\n\n# reorder factors\nsurvey$gender &lt;- factor(survey$gender, levels = c(\"Male\", \"Female\", \"Non-binary\"))\n\n# build model\nsurvey_qa_null &lt;- glm(ask_questions ~ 1, data = subset(survey, !is.na(gender)), family = \"binomial\")\n\nsurvey_qa &lt;- glm(ask_questions ~ gender, data = subset(survey, !is.na(gender)), family = \"binomial\")\n\nsummary(survey_qa)\n\n\nCall:\nglm(formula = ask_questions ~ gender, family = \"binomial\", data = subset(survey, \n    !is.na(gender)))\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        0.8729     0.2073   4.212 2.54e-05 ***\ngenderFemale      -0.4904     0.2435  -2.014    0.044 *  \ngenderNon-binary   0.9188     1.0998   0.835    0.403    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 490.49  on 372  degrees of freedom\nResidual deviance: 484.54  on 370  degrees of freedom\n  (7 observations deleted due to missingness)\nAIC: 490.54\n\nNumber of Fisher Scoring iterations: 4\n\nm_survey_out &lt;- collect_out(model = survey_qa, null = survey_qa_null, n_factors=2,name=\"qa_survey_general\",type=\"survey\", save = \"yes\", dir = \"../results/question-asking/\")\n\nm_survey_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nqa_survey_general\n\n\nAIC\n490.536\n\n\nn_obs\n373\n\n\nlrt_pval\n0.051\n\n\nlrt_chisq\n5.958\n\n\nintercept_estimate\n0.873\n\n\nintercept_estimate_prop\n0.705\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n0.467\n\n\nintercept_ci_higher\n1.279\n\n\nn_factors\n2\n\n\nest_genderFemale\n-0.49\n\n\nlowerCI_genderFemale\n-0.968\n\n\nhigherCI_genderFemale\n-0.013\n\n\npval_genderFemale\n0.044\n\n\nzval_genderFemale\n-2.014\n\n\nest_genderNon-binary\n0.919\n\n\nlowerCI_genderNon-binary\n-1.237\n\n\nhigherCI_genderNon-binary\n3.074\n\n\npval_genderNon-binary\n0.403\n\n\nzval_genderNon-binary\n0.835"
  },
  {
    "objectID": "qmd/2_question_asking_general.html#plot-output",
    "href": "qmd/2_question_asking_general.html#plot-output",
    "title": "3  Question asking",
    "section": "3.4 Plot output",
    "text": "3.4 Plot output\nNext, we can plot the model output in a single figure.\n\n# combine output from all sessions and plenaries \nm_main_general &lt;- rbind(m_qa_general_out, m_plenary_out)\nm_main_general$name &lt;- c(\"Asking questions\", \"Asking questions - plenary\") \n\nggplot(m_main_general) +\n  geom_point(aes(x = intercept_estimate, y = name), size = 5) + \n  geom_segment(aes(x = intercept_ci_lower, xend = intercept_ci_higher, y = name),\n               linewidth=0.5) +\n  geom_vline(xintercept = 0, col = \"red\", linetype = \"dotted\") +\n  labs(x = \"Intercept and 95% CI\", y = \"Model\") + \n  geom_text(aes(label = \"Male bias\", y = 2.5, x = -1, size = 4))+\n  geom_text(aes(label = \"Female bias\", y = 2.5, x = 1, size = 4)) +\n  geom_segment(aes(x = -0.2, xend = -2.3, y = 2.3),\n               col = \"black\", arrow = arrow(length=unit(0.2, \"cm\")))+\n  geom_segment(aes(x = 0.2, xend = 2.3, y = 2.3),\n               col = \"black\", arrow = arrow(length=unit(0.2, \"cm\"))) +\n  theme(legend.position = \"none\") -&gt; plot_1_qa\n\n### also add survey\n\n# convert to include both female and non-binary\n\nm_survey_out$name &lt;- c(\"Asked a question\") \n\nm_survey_out_long &lt;- data.frame(factor = c(\"Female\", \"Non-binary\"),\n                                estimate = c(m_survey_out$est_genderFemale, m_survey_out$`est_genderNon-binary`),\n                                lower = c(m_survey_out$lowerCI_genderFemale, m_survey_out$`lowerCI_genderNon-binary`),\n                                upper = c(m_survey_out$higherCI_genderFemale, m_survey_out$`higherCI_genderNon-binary`))\n\nggplot(m_survey_out_long) +\n  geom_point(aes(x = estimate, y = factor), size = 5) + \n  geom_segment(aes(x = lower, xend = upper, y = factor),\n               linewidth=1) +\n  geom_vline(xintercept = 0, col = \"red\", linetype = \"dotted\") +\n  labs(x = \"Gender effect estimate and 95% CI\", y = \"Model\") + \n  xlim(-2, 3.5) +\n  geom_text(aes(label = \"Male bias\", y = 2.5, x = -1, size = 4))+\n  geom_segment(aes(x = -0.3, xend = -1.8, y = 2.4),\n               col = \"black\", arrow = arrow(length=unit(0.2, \"cm\")))+\n  theme(legend.position = \"none\") -&gt; plot_2_qa_survey\n\ncowplot::plot_grid(plot_1_qa, plot_2_qa_survey, ncol = 1,\n                   align = \"hv\", axis = \"lb\", labels = c(\"a) Behavioural data\", \"b) Survey data\")) -&gt; plot_qa\n\nplot_qa"
  },
  {
    "objectID": "qmd/2_question_asking_general.html#supplementary-rerun-models-with-conservative-data",
    "href": "qmd/2_question_asking_general.html#supplementary-rerun-models-with-conservative-data",
    "title": "3  Question asking",
    "section": "3.5 Supplementary: rerun models with conservative data",
    "text": "3.5 Supplementary: rerun models with conservative data\nAs mentioned, we repeated the analysis on the observational data with the ‘conservative’ data set, which excluded any questions that some level of uncertainty in the collection. We don’t do this for the plenary sessions as the audience was not counted and any other uncertainty would have been excluded from the initial data set either way.\n\nsummary(glmer(gender_questioner_female ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = data_conserved))\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ 1 + (1 | session_id/talk_id)\n   Data: data_conserved\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   470.6    482.2   -232.3    464.6      339 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5818 -0.9108 -0.7139  1.0433  1.4765 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 342, groups:  talk_id:session_id, 124; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.6739     0.1101  -6.122 9.25e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe model results seem virtually identical!"
  },
  {
    "objectID": "qmd/2_question_asking_general.html#supplementary-inter-observer-reliability",
    "href": "qmd/2_question_asking_general.html#supplementary-inter-observer-reliability",
    "title": "3  Question asking",
    "section": "3.6 Supplementary: inter-observer reliability",
    "text": "3.6 Supplementary: inter-observer reliability\nThe data used above merge data collected by multiple observers, after we checked whether the reliability between different observers was high enough. Below you will find an example of how we calculated this IOR (inter-observer reliability) and the full script can be found under scripts/2_question_asking/ior.R.\n\n## Example of calculating Cohen's kappa\n\n###### Host gender ####\nhost_gender &lt;- subset(session, !is.na(host_1_gender)) # exclude missing data\n\nwide_host_gender &lt;- spread(host_gender[,c(\"session_id\", \"observer_talk\", \"host_1_gender\")], \n                           observer_talk, host_1_gender)\n\n# treat observer 1 and 2 as one unit, and observer 3 and 4 as one unit\nwide_host_gender_a &lt;- wide_host_gender[,c(1:3)]\nwide_host_gender_b &lt;- wide_host_gender[,c(1,4,5)]\nnames(wide_host_gender_a) &lt;- c(\"session_id\", \"observer_a\", \"observer_b\")\nnames(wide_host_gender_b) &lt;- c(\"session_id\", \"observer_a\", \"observer_b\")\n\n# merge\nwide_host_gender &lt;- rbind(wide_host_gender_a, wide_host_gender_b)\n\n# exclude sessions without double sampling\nwide_host_gender &lt;- subset(wide_host_gender, !is.na(observer_a) & !is.na(observer_b)) \n\nkappa_host_gender &lt;- kappa2(wide_host_gender[,2:3], weight = \"unweighted\")\n\nkappa_host_gender \n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 36 \n   Raters = 2 \n    Kappa = 0.945 \n\n        z = 5.92 \n  p-value = 3.3e-09 \n\n## Example of calculating ICC\n\n#### Audience total\naudience_total &lt;- subset(talk, !is.na(audience_total)) # exclude missing data\n\nwide_audience_total &lt;- spread(audience_total[,c(\"talk_id\", \"observer_talk\", \"audience_total\")], \n                              observer_talk, audience_total)\n\n# treat observer 1 and 2 as one unit, and observer 3 and 4 as one unit\nwide_audience_total_a &lt;- wide_audience_total[,c(1:3)]\nwide_audience_total_b &lt;- wide_audience_total[,c(1,4,5)]\nnames(wide_audience_total_a) &lt;- c(\"talk_id\", \"observer_a\", \"observer_b\")\nnames(wide_audience_total_b) &lt;- c(\"talk_id\", \"observer_a\", \"observer_b\")\n\n# merge\nwide_audience_total &lt;- rbind(wide_audience_total_a, wide_audience_total_b)\n\n# exclude sessions without double sampling\nwide_audience_total &lt;- subset(wide_audience_total, !is.na(observer_a) & !is.na(observer_b)) \n\nicc_audience_total &lt;- icc(wide_audience_total[,2:3],\n                          model = \"twoway\", #both column (observer) and row (talk) are random\n                          type = c(\"agreement\"),\n                          unit = c(\"single\"))\n\nicc_audience_total \n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 203 \n     Raters = 2 \n   ICC(A,1) = 0.963\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(202,202) = 52.4 , p = 5.53e-117 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.951 &lt; ICC &lt; 0.972"
  },
  {
    "objectID": "qmd/3_question_asking_per_age.html#how-many-female-senior-scientists",
    "href": "qmd/3_question_asking_per_age.html#how-many-female-senior-scientists",
    "title": "4  Gender disparity or demographic inertia?",
    "section": "4.1 How many female senior scientists?",
    "text": "4.1 How many female senior scientists?\nFirst, we explored the potential for age-related effects to bias our interpretation of gender disparity in question asking by calculating the proportion of senior women who attended the congress, based on collected data on career stage and pronouns during registration. We defined a female senior scientist as someone who uses she/her pronouns and has a “Professor” or “Associate Professor” title, as a male senior scientist as someone who uses he/him pronouns and has a “Professor” or “Associate Professor” title.\n\n# registration\nall_reg &lt;- fread(\"../data/metadata/Registration_clean.tsv\", quote=\"\")\n\n# exclude gender queer and no answer because this gets too complex and \n# sample size is low\nall_reg &lt;- subset(all_reg, (Pronouns == \"Female\" | Pronouns == \"Male\"))\n\n# look at career stage data across the entire congress\nsummary(as.factor(all_reg$Career))\n\n               Post doctorate or almost                  Professor or associate \n                                    157                                      87 \nSenior position, lecturers, researchers                Students (BSc, MSc, PhD) \n                                    103                                     337 \n\n# we assessed age categories in practise by dividing age into three classes: \n# &lt; 35, 35-50 and &gt;50. \n# Since we will line up the observational data with the registration data, \n# we only define \"Professor or associate\" as the oldest age class, as they \n# are most likely to be put in this age category. \n\nall_reg &lt;- all_reg %&gt;% mutate(age = case_when(\n  Career == \"Professor or associate\" ~ \"senior\",\n  TRUE ~ \"junior\"\n))\n\n# number of registrants per pronoun/gender\nreg_pronoun &lt;- table(all_reg$Pronouns) %&gt;% as.data.frame()\n\n# number of registrants per pronoun/gender\nreg_age &lt;- table(all_reg$age) %&gt;% as.data.frame()\n\n# number of registrants per pronoun/gender and age\nreg &lt;- table(all_reg$Pronouns, all_reg$age) %&gt;% as.data.frame()\n\n# combine and rename\nreg &lt;- left_join(reg, reg_pronoun, by = \"Var1\")\nreg &lt;- left_join(reg, reg_age, by = c(\"Var2\" = \"Var1\"))\nnames(reg) &lt;- c(\"gender\", \"age\", \"n\", \"n_gender\", \"n_age\")\n\n# calculate the proportion of attendees per gender and age (prop) and per gender only (prop_gender)\nreg$prop &lt;- reg$n / sum(reg$n) \nreg$prop_gender &lt;- reg$n / reg$n_gender\nreg$prop_age &lt;- reg$n / reg$n_age\n\nreg \n\n  gender    age   n n_gender n_age       prop prop_gender  prop_age\n1 Female junior 398      451   597 0.58187135   0.8824834 0.6666667\n2   Male junior 199      233   597 0.29093567   0.8540773 0.3333333\n3 Female senior  53      451    87 0.07748538   0.1175166 0.6091954\n4   Male senior  34      233    87 0.04970760   0.1459227 0.3908046\n\n\nHere, the ‘prop’ indicates the proportion across the entire congress, whereas the ‘prop_age’ indicates the proportion within that age class.\nSo: the majority of senior scientists was female.\nSince there were more female senior scientists than male senior scientists, we would expect more women to ask questions than men if the majority of questions are asked by senior scientists regardless of gender. Even though demographic inertia was therefore unlikely to be relevant for potential biases in questioning gender disparities caused by career stage, we investigated whether 1) senior scientists ask more questions than junior scientists and 2) whether the gender disparity in question-asking was similar when stratifying our analysis by juniors and seniors."
  },
  {
    "objectID": "qmd/3_question_asking_per_age.html#do-seniors-ask-more-questions-than-juniors",
    "href": "qmd/3_question_asking_per_age.html#do-seniors-ask-more-questions-than-juniors",
    "title": "4  Gender disparity or demographic inertia?",
    "section": "4.2 Do seniors ask more questions than juniors?",
    "text": "4.2 Do seniors ask more questions than juniors?\nThe model that we’re testing here looks like follows:\nglmer(age_questioner_senior ~ questioner_gender + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_senior_prop))\nFirst, we calculate the proportion of the audience that was a senior based on the registration data.\n\nreg_prop_junior = reg$n_age[which(reg$age==\"junior\")][1] / nrow(all_reg)\nreg_prop_senior = 1-reg_prop_junior\n\n## reformat the data to indicate the seniority of the questioner based on age (senior = age class 3 = age)\n\ndata_control &lt;- data_control %&gt;% \n  mutate(audience_junior_prop = (audience_total * reg_prop_junior) / audience_total,\n         audience_senior_prop = (audience_total * reg_prop_senior) / audience_total,\n         age_questioner_junior = case_when(questioner_age == 1 | questioner_age == 2 ~1, questioner_age == 3 ~ 0),\n         age_questioner_senior = case_when(questioner_age == 1 | questioner_age == 2 ~0, questioner_age == 3 ~ 1))\n\nage_questioner_senior &lt;- glmer(age_questioner_senior ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_senior_prop), data = data_control)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(age_questioner_senior)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: age_questioner_senior ~ 1 + (1 | session_id/talk_id)\n   Data: data_control\n Offset: boot::logit(audience_senior_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   156.8    167.4    -75.4    150.8      252 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.9850 -0.3076 -0.1924 -0.1707  4.3849 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev. \n talk_id:session_id (Intercept) 7.825e-10 2.797e-05\n session_id         (Intercept) 1.720e+00 1.312e+00\nNumber of obs: 255, groups:  talk_id:session_id, 105; session_id, 23\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.8796     0.4712  -1.867   0.0619 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nSo, although the intercept is marginally significant (p = 0.06), it seems like there is a trend for the lower probability of seniors asking a question compared to juniors."
  },
  {
    "objectID": "qmd/3_question_asking_per_age.html#is-the-gender-disparity-in-question-asking-dependent-on-seniority",
    "href": "qmd/3_question_asking_per_age.html#is-the-gender-disparity-in-question-asking-dependent-on-seniority",
    "title": "4  Gender disparity or demographic inertia?",
    "section": "4.3 Is the gender disparity in question-asking dependent on seniority?",
    "text": "4.3 Is the gender disparity in question-asking dependent on seniority?\nFor this question, we split up the dataset between juniors and seniors. To do so, we correct the observed number of perceived women in the audience by the number of junior and senior women taken from the registration. This is a complex conversion which is outlined in detail below with mock data.\n\n# calculate proportions required for the correction based on registration\n\n# get proportions of junior women, junior men, senior women, senior men based on registration\nprop_junior_women_registration_women = reg$prop_gender[which(reg$gender==\"Female\" & reg$age == \"junior\")]\n\nprop_senior_women_registration_women = reg$prop_gender[which(reg$gender==\"Female\" & reg$age == \"senior\")]\n\nprop_junior_men_registration_men = reg$prop_gender[which(reg$gender==\"Male\" & reg$age == \"junior\")]\n\nprop_senior_men_registration_men = reg$prop_gender[which(reg$gender==\"Male\" & reg$age == \"senior\")]\n\n# try with hypothetical talk data to ensure the correction is done correctly\ntotal_audience_no = 100\naudience_prop_women = 0.6\naudience_prop_men = 1 - audience_prop_women\n\n# for the below: we first calculate the number of women/men in the audience and multiply that by the proportion of junior/senior women/men of the registration, and then divide that by the total audience number again\n\nprop_junior_women_talk = ((audience_prop_women * total_audience_no) * prop_junior_women_registration_women)/total_audience_no \n\nprop_senior_women_talk = ((audience_prop_women * total_audience_no) * prop_senior_women_registration_women)/total_audience_no\n\nprop_junior_men_talk = ((audience_prop_men * total_audience_no) * prop_junior_men_registration_men)/total_audience_no\n\nprop_senior_men_talk = ((audience_prop_men * total_audience_no) * prop_senior_men_registration_men)/total_audience_no\n\n# should add up to 1\nprop_junior_women_talk + prop_senior_women_talk + prop_junior_men_talk + prop_senior_men_talk\n\n[1] 1\n\n# then get gender proportions by age, which is what we need to do the correction\nprop_junior_women_talk_junior = prop_junior_women_talk*total_audience_no / (prop_junior_women_talk*total_audience_no + prop_junior_men_talk*total_audience_no)\n\nprop_junior_women_talk_junior\n\n[1] 0.6078261\n\nprop_junior_women_talk_senior = prop_senior_women_talk*total_audience_no / (prop_senior_women_talk*total_audience_no + prop_senior_men_talk*total_audience_no)\n\nprop_junior_women_talk_senior\n\n[1] 0.5471018\n\n# if we want to put that in one simplified formula:\n\n(audience_prop_women*prop_junior_women_registration_women) / ((audience_prop_women*prop_junior_women_registration_women) + (audience_prop_men * prop_junior_men_registration_men))\n\n[1] 0.6078261\n\n(audience_prop_women*prop_senior_women_registration_women) / ((audience_prop_women*prop_senior_women_registration_women) + (audience_prop_men * prop_senior_men_registration_men))\n\n[1] 0.5471018\n\n\nThen we can model the junior and senior data separately:\n\n# how many questions were asked by women (1) and men (0) per age class? \n# age class  1 = &lt; 35 years, 2 = 35-50, 3 = &gt; 50\n\ntable(data_control$questioner_age, data_control$gender_questioner_female) %&gt;% kbl() %&gt;%  kable_classic_2()\n\n\n\n\n0\n1\n\n\n\n\n53\n76\n\n\n65\n38\n\n\n19\n10\n\n\n\n\n\n\n# make two dataframes: junior and senior data\njunior &lt;- subset(data_control, (questioner_age == 1 | questioner_age == 2))\nsenior &lt;- subset(data_control, questioner_age == 3)\n\n# add column with corrected gender proportion\njunior$audience_women_prop_junior &lt;- (junior$audience_women_prop*prop_junior_women_registration_women) /\n  ((junior$audience_women_prop*prop_junior_women_registration_women) +\n     (junior$audience_men_prop * prop_junior_men_registration_men))\n\nsenior$audience_women_prop_senior &lt;- (senior$audience_women_prop*prop_senior_women_registration_women) /\n  ((senior$audience_women_prop*prop_senior_women_registration_women) +\n     (senior$audience_men_prop * prop_senior_men_registration_men))\n\n# build model for junior scientists\n\nm_qa_junior &lt;- glmer(gender_questioner_female ~ 1 + (1|session_id/talk_id), \n                     family = \"binomial\",offset=boot::logit(audience_women_prop_junior), \n                     data = junior)\n\n# build model for senior scientists\n\nm_qa_senior &lt;- glmer(gender_questioner_female ~ 1 + (1|session_id/talk_id), \n                     family = \"binomial\", offset=boot::logit(audience_women_prop_senior), \n                     data = senior)\n\n# model output\nsummary(m_qa_junior)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ 1 + (1 | session_id/talk_id)\n   Data: junior\n Offset: boot::logit(audience_women_prop_junior)\n\n     AIC      BIC   logLik deviance df.resid \n   312.3    322.6   -153.1    306.3      226 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4497 -0.9291 -0.6876  1.0189  1.4086 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 229, groups:  talk_id:session_id, 100; session_id, 23\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.6763     0.1344  -5.034 4.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nsummary(m_qa_senior)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ 1 + (1 | session_id/talk_id)\n   Data: senior\n Offset: boot::logit(audience_women_prop_senior)\n\n     AIC      BIC   logLik deviance df.resid \n    39.6     43.4    -16.8     33.6       23 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.8578 -0.7517 -0.6574  1.2507  1.7378 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 26, groups:  talk_id:session_id, 21; session_id, 11\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.7833     0.4143  -1.891   0.0586 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nIt therefore appears that the gender disparity is apparent in both junior and senior attendees, but it is stronger in seniors although with less significance.\n\n\n\n\nHinsley, Amy, William J. Sutherland, and Alison Johnston. 2017. “Men Ask More Questions Than Women at a Scientific Conference.” Edited by Marina A. Pavlova. PLOS ONE 12 (10): e0185534. https://doi.org/10.1371/journal.pone.0185534."
  },
  {
    "objectID": "qmd/4_question_asking_how.html#do-women-raise-their-hands-less-often",
    "href": "qmd/4_question_asking_how.html#do-women-raise-their-hands-less-often",
    "title": "5  Do women raise their hands less or get chosen less often?",
    "section": "5.1 Do women raise their hands less often?",
    "text": "5.1 Do women raise their hands less often?\nFirst, we asked whether women raise their hands less than men do, by fitting a binomial GLMM where the dependent variables were the number of women and men that raised their hands, while accounting for the gender proportion of the audience and the nonindependence of talks within a session, similar to above (see Methods for details).\n\n# subset data\n\ndata_control &lt;- subset(data_analysis, treatment == \"Control\" &\n                     is.na(followup) & is.na(jumper)&\n                     is.na(host_asks) & \n                    !grepl(\"speaker|questioner\", allocator_question))\n\ndata_control &lt;- droplevels(data_control)\n\n# second dataset to test robustness to data with uncertainty\n\ndata_conserved &lt;- subset(data_control, uncertainty_count_audience == 0 & uncertainty_count_hands == 0)\n\n# exclude situations where the hand count was incorrect and there were on paper more hands raised by men only then in total\n\ndata_hands &lt;- subset(data_control, hands_prop_men &lt;= 1)\n\nm_qa_hands &lt;- glmer(cbind(hands_women, hands_men) ~ 1 + (1|session_id/talk_id), family = \"binomial\", \n                  offset=boot::logit(audience_women_prop), \n                  data = data_hands)\n\n\n# model output\nsummary(m_qa_hands)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(hands_women, hands_men) ~ 1 + (1 | session_id/talk_id)\n   Data: data_hands\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   606.4    618.0   -300.2    600.4      346 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.51074 -0.94505 -0.00957  0.93434  1.81749 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.11019  0.3320  \n session_id         (Intercept) 0.05309  0.2304  \nNumber of obs: 349, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.5838     0.1071  -5.448 5.08e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# use helper function to collect model output in data frame\nm_qa_hands_out &lt;- collect_out(model = m_qa_hands, null = NA, \n                              name = \"QA_hands\", n_factors = 0, type = \"qa\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_hands_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_hands\n\n\nAIC\n606.409\n\n\nn_obs\n349\n\n\nlrt_pval\nNA\n\n\nlrt_chisq\nNA\n\n\nintercept_estimate\n-0.584\n\n\nintercept_estimate_prop\n0.358\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-0.813\n\n\nintercept_ci_higher\n-0.375\n\n\nn_factors\n0"
  },
  {
    "objectID": "qmd/4_question_asking_how.html#sec-hands",
    "href": "qmd/4_question_asking_how.html#sec-hands",
    "title": "5  Do women raise their hands less or get chosen less often?",
    "section": "5.2 Do women get chosen less to ask their question?",
    "text": "5.2 Do women get chosen less to ask their question?\n\n5.2.1 Observational data\nNext, we asked whether women get chosen less to ask their question, by fitting another binomial GLMM using the gender of the questioner as the dependent variable, but this time correcting for the gender proportion of the people that raised their hands instead.\n\n# exclude situations where the host did not make a choice between genders (i.e. only men or only women raised hands)\n\nm_qa_chosen &lt;- glmer(gender_questioner_female ~ 1 + (1|talk_id), family = \"binomial\", offset=boot::logit(hands_prop_women), data = subset(data_hands, hands_prop_women !=0 & hands_prop_women != 1))\n\n# can't get the CI's when including (1|session_id/talk_id)! \n\n# model output\nsummary(m_qa_chosen)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ 1 + (1 | talk_id)\n   Data: subset(data_hands, hands_prop_women != 0 & hands_prop_women !=      1)\n Offset: boot::logit(hands_prop_women)\n\n     AIC      BIC   logLik deviance df.resid \n   139.1    144.3    -67.6    135.1       97 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8742 -0.9371 -0.6626  1.0671  1.5092 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n talk_id (Intercept) 0        0       \nNumber of obs: 99, groups:  talk_id, 67\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -0.1300     0.2065  -0.629    0.529\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n# use helper function to collect model output in data frame\nm_qa_chosen_out &lt;- collect_out(model = m_qa_chosen, null = NA, name = \"QA_chosen\", n_factors = 0, type = \"qa\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_chosen_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_chosen\n\n\nAIC\n139.103\n\n\nn_obs\n99\n\n\nlrt_pval\nNA\n\n\nlrt_chisq\nNA\n\n\nintercept_estimate\n-0.13\n\n\nintercept_estimate_prop\n0.468\n\n\nintercept_pval\n0.529\n\n\nintercept_ci_lower\n-0.537\n\n\nintercept_ci_higher\n0.275\n\n\nn_factors\n0\n\n\n\n\n\n\n\nThus, we show that women ask less questions at the congress because women raised their hands less often than men do (GLMM intercept = -0.584, p-value = 0), not because they were chosen less often by session hosts to ask their question compared to men (GLMM intercept = -0.13, p-value = 0.529).\n\n\n5.2.2 Survey data\nWe can answer whether there is a gender disparity in the probability that a person did not ask a question because they were not chosen to do so using the post-congress survey data, too. More specifically, we asked the question in the survey if a person did not ask a question because they were not chosen to do so. We model if the response to this (applies = 1, doesn’t apply = 0) is affected by gender.\n\n# reorder factors\nsurvey$gender &lt;- factor(survey$gender, levels = c(\"Male\", \"Female\", \"Non-binary\"))\n\n# explore data\ntable(survey$reason_noquestion_notchosen, survey$gender) %&gt;% kbl() %&gt;% kable_classic_2()\n\n\n\n\n\nMale\nFemale\nNon-binary\n\n\n\n\n0\n98\n216\n5\n\n\n1\n14\n40\n2\n\n\n\n\n\n\n# model and null model\nm_survey_chosen &lt;- glm(reason_noquestion_notchosen ~ gender, data = subset(survey, !is.na(gender)), family = \"binomial\")\n\nsummary(m_survey_chosen)\n\n\nCall:\nglm(formula = reason_noquestion_notchosen ~ gender, family = \"binomial\", \n    data = subset(survey, !is.na(gender)))\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.9459     0.2857  -6.811 9.71e-12 ***\ngenderFemale       0.2595     0.3336   0.778    0.437    \ngenderNon-binary   1.0296     0.8841   1.165    0.244    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 316.16  on 374  degrees of freedom\nResidual deviance: 314.67  on 372  degrees of freedom\n  (5 observations deleted due to missingness)\nAIC: 320.67\n\nNumber of Fisher Scoring iterations: 4\n\nm_survey_chosen_null &lt;- glm(reason_noquestion_notchosen ~ 1, data = subset(survey, !is.na(gender)), family = \"binomial\")\n\n# helper function to get model output\n\nm_survey_chosen_out &lt;- collect_out(model = m_survey_chosen, null = m_survey_chosen_null, name=\"survey_qa_chosen\", n_factors = 2, type=\"survey\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_survey_chosen_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nsurvey_qa_chosen\n\n\nAIC\n320.673\n\n\nn_obs\n375\n\n\nlrt_pval\n0.475\n\n\nlrt_chisq\n1.491\n\n\nintercept_estimate\n-1.946\n\n\nintercept_estimate_prop\n0.125\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-2.506\n\n\nintercept_ci_higher\n-1.386\n\n\nn_factors\n2\n\n\nest_genderFemale\n0.26\n\n\nlowerCI_genderFemale\n-0.394\n\n\nhigherCI_genderFemale\n0.913\n\n\npval_genderFemale\n0.437\n\n\nzval_genderFemale\n0.778\n\n\nest_genderNon-binary\n1.03\n\n\nlowerCI_genderNon-binary\n-0.703\n\n\nhigherCI_genderNon-binary\n2.762\n\n\npval_genderNon-binary\n0.244\n\n\nzval_genderNon-binary\n1.165\n\n\n\n\n\n\n\nIn line with the observational data, gender did not contribute to the probability that a person did not ask a question because they were not chosen to do so based on the survey data (LRT p-value = 0.475)\nThe gender disparity in question asking therefore likely arises due to internal factors that cause women to raise their hands less often to ask a question compared to men."
  },
  {
    "objectID": "qmd/4_question_asking_how.html#plot-data",
    "href": "qmd/4_question_asking_how.html#plot-data",
    "title": "5  Do women raise their hands less or get chosen less often?",
    "section": "5.3 Plot data",
    "text": "5.3 Plot data\nNext, we can plot the results to visualize the reason why women ask less questions than men do.\n\n# combine output from all sessions and plenaries \nm_why_general &lt;- rbind(m_qa_chosen_out, m_qa_hands_out, m_qa_general_out)\nm_why_general$name &lt;- c(\"Getting chosen to ask\", \"Raising hands\", \"Asking questions\") \nm_why_general$name &lt;- factor(m_why_general$name, levels=c(\"Getting chosen to ask\", \"Raising hands\", \"Asking questions\"))\n\nggplot(m_why_general) +\n  geom_point(aes(x = intercept_estimate, y = name), size = 5) + \n  geom_segment(aes(x = intercept_ci_lower, xend = intercept_ci_higher, y = name),\n               linewidth=0.5) +\n  geom_vline(xintercept = 0, col = \"red\", linetype = \"dotted\") +\n  labs(x = \"Intercept and 95% CI\", y = \"Model\") + \n  geom_text(aes(label = \"Male bias\", y = 3.5, x = -0.5, size = 4))+\n  geom_text(aes(label = \"Female bias\", y = 3.5, x = 0.5, size = 4)) +\n  geom_segment(aes(x = -0.2, xend = -0.8, y = 3.3),\n               col = \"black\", arrow = arrow(length=unit(0.2, \"cm\")))+\n  geom_segment(aes(x = 0.2, xend = 0.8, y = 3.3),\n               col = \"black\", arrow = arrow(length=unit(0.2, \"cm\"))) +  theme(legend.position = \"none\") -&gt; plot_1_qa_why\n\n### also add survey\n\n# convert to include both female and non-binary\n\nm_survey_chosen_out$name &lt;- c(\"Didn't ask because I wasn't chosen to\") \n\nm_survey_chosen_out_long &lt;- data.frame(factor = c(\"Female\", \"Non-binary\"),\n                                estimate = c(m_survey_chosen_out$est_genderFemale, m_survey_chosen_out$`est_genderNon-binary`),\n                                lower = c(m_survey_chosen_out$lowerCI_genderFemale, m_survey_chosen_out$`lowerCI_genderNon-binary`),\n                                upper = c(m_survey_chosen_out$higherCI_genderFemale, m_survey_chosen_out$`higherCI_genderNon-binary`))\n\nggplot(m_survey_chosen_out_long) +\n  geom_point(aes(x = estimate, y = factor), size = 5) + \n  geom_segment(aes(x = lower, xend = upper, y = factor),\n               linewidth=1) +\n  geom_vline(xintercept = 0, col = \"red\", linetype = \"dotted\") +\n  labs(x = \"Gender effect estimate and 95% CI\", y = \"Model\") + \n  xlim(-2, 3.5) +\n  geom_text(aes(label = \"Male bias\", y = 2.5, x = -1, size = 4))+\n  geom_segment(aes(x = -0.3, xend = -1.8, y = 2.4),\n               col = \"black\", arrow = arrow(length=unit(0.2, \"cm\")))+\n  theme(legend.position = \"none\") -&gt; plot_2_survey_chosen\n\ncowplot::plot_grid(plot_1_qa_why, plot_2_survey_chosen, ncol = 1,\n                   align = \"hv\", axis = \"lb\", labels = c(\"a) Behavioural data\", \"b) Survey data\"))"
  },
  {
    "objectID": "qmd/4_question_asking_how.html#supplementary-rerun-models-with-conservative-data",
    "href": "qmd/4_question_asking_how.html#supplementary-rerun-models-with-conservative-data",
    "title": "5  Do women raise their hands less or get chosen less often?",
    "section": "5.4 Supplementary: rerun models with conservative data",
    "text": "5.4 Supplementary: rerun models with conservative data\nAs mentioned, we repeated the analysis on the observational data with the ‘conservative’ data set, which excluded any questions that some level of uncertainty in the collection.\n\n# hands raised\nsummary(glmer(cbind(hands_women, hands_men) ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_conserved, hands_prop_men &lt;= 1)))\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(hands_women, hands_men) ~ 1 + (1 | session_id/talk_id)\n   Data: subset(data_conserved, hands_prop_men &lt;= 1)\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   592.2    603.7   -293.1    586.2      338 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.56540 -0.93434 -0.04708  0.94179  1.85563 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.124229 0.35246 \n session_id         (Intercept) 0.009993 0.09996 \nNumber of obs: 341, groups:  talk_id:session_id, 124; session_id, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.58374    0.09919  -5.885 3.97e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# getting chosen\nsummary(glmer(gender_questioner_female ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(hands_prop_women), data = subset(data_conserved, hands_prop_women !=0 & hands_prop_women != 1 & hands_prop_men &lt;= 1)))\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ 1 + (1 | session_id/talk_id)\n   Data: subset(data_conserved, hands_prop_women != 0 & hands_prop_women !=  \n    1 & hands_prop_men &lt;= 1)\n Offset: boot::logit(hands_prop_women)\n\n     AIC      BIC   logLik deviance df.resid \n   137.5    145.2    -65.8    131.5       93 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8759 -0.9152 -0.5806  0.9833  1.5094 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.0000   0.0000  \n session_id         (Intercept) 0.1191   0.3451  \nNumber of obs: 96, groups:  talk_id:session_id, 65; session_id, 23\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -0.1094     0.2269  -0.482     0.63\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe model results seem virtually identical!"
  },
  {
    "objectID": "qmd/5_question_asking_why.html#survey-data",
    "href": "qmd/5_question_asking_why.html#survey-data",
    "title": "6  Factors that affect the gender disparity",
    "section": "6.1 Survey data",
    "text": "6.1 Survey data\n\n6.1.1 Conditions\nMore specifically, we asked in the survey to indicate on a 7-point Likert scale to what extent people agree with five statements. The statements were phrased as follows: I feel more comfortable asking a question if: 1) the presenter is of my own gender, 2) there is representation of my gender in the audience, 3) the audience size is smaller, 4) I know the speaker, or 5) the host is of their my own gender.\n\n\n6.1.2 Motivations and hestitations\nThe post-congress survey additionally included questions on what aspect(s) motivate(s) people from asking questions at the Behaviour 2023 and in general (hereafter referred to as “motivations”), and what aspect(s) make(s) people more hesitant towards asking a question (hereafter referred to as “hesitations”). We can therefore get a deeper understanding into the internal factors associated with question asking behaviour that affect women more than men. The motivations to ask a question that were most commonly selected by respondents included “Interest in the topic” (87.6%), “Gaining deeper understanding” (72.5%), and “Relevance for my own research” (50%) (see Methods for all motivations included in the survey). The most commonly selected hesitations included “I would rather ask my question after the session, 1-1 with the speaker” (45.3%), “Afraid I misunderstood the content of the presentation” (40.4%) and “I did not think my question was relevant/important” (39.6%).\n\n\n6.1.3 Analysis of gender effects\nWe built five ordinal GLMs, with the Likert-scale response to each of the five conditions as the dependent variable (1: strongly disagree, 7: strongly agree) and self-reported gender identity as the independent variable, while correcting for variation in career stages. We also built 19 binomial GLMs, one per motivation/hesitation, with the binomial response whether the motivation or hesitation was ticked (1) or not (0) as the dependent variable, and self-reported gender as the independent variable, while controlling for career stage. Statistical significance was inferred through an LRT.\nWe first analyzed which conditions, motivations and hesitations are more often indicated to be of importance by women compared to men, and subsequently which factors best predict the probability that a person asked a question during the congress.\n\n# rearrange genders\nsurvey$gender &lt;- factor(survey$gender, levels = c(\"Male\", \"Female\", \"Non-binary\"))\n\n# first ask: gender bias in feeling uncomfortable in asking a question?\nsurvey$comfort_asking_rating &lt;- as.factor(survey$comfort_asking_rating)\n\ncomfort_gender &lt;- MASS::polr(comfort_asking_rating ~ gender + career_3cat, data = subset(survey, !is.na(gender))) \n\ncoeftest(comfort_gender)\n\n\nt test of coefficients:\n\n                       Estimate Std. Error t value  Pr(&gt;|t|)    \ngenderFemale           -1.26188    0.21061 -5.9917 5.006e-09 ***\ngenderNon-binary       -1.60412    0.67979 -2.3597   0.01882 *  \ncareer_3catLate career  2.47523    0.32986  7.5038 4.829e-13 ***\ncareer_3catMid career   1.04273    0.20670  5.0447 7.192e-07 ***\ncareer_3catOther        0.56917    0.47113  1.2081   0.22780    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncomfort_gender_null &lt;- MASS::polr(comfort_asking_rating ~ career_3cat, data = subset(survey, !is.na(gender))) \n\nm_comfortout &lt;- collect_out(model = comfort_gender, null = comfort_gender_null, name = \"comfort_ask_question_gender\", n_factors = 4, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_comfortout %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\ncomfort_ask_question_gender\n\n\nAIC\n1360.136\n\n\nn_obs\n374\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n38.188\n\n\nintercept_12\n-2.774\n\n\nintercept_23\n-1.616\n\n\nintercept_34\n-0.875\n\n\nintercept_45\n-0.074\n\n\nintercept_56\n0.945\n\n\nintercept_67\n2.295\n\n\nn_factors\n4\n\n\nest_genderFemale\n-1.262\n\n\nlowerCI_genderFemale\n-1.676\n\n\nhigherCI_genderFemale\n-0.848\n\n\nse_genderFemale\n0.211\n\n\ntval_genderFemale\n-5.992\n\n\npval_genderFemale\n0\n\n\nest_genderNon-binary\n-1.604\n\n\nlowerCI_genderNon-binary\n-2.941\n\n\nhigherCI_genderNon-binary\n-0.267\n\n\nse_genderNon-binary\n0.68\n\n\ntval_genderNon-binary\n-2.36\n\n\npval_genderNon-binary\n0.019\n\n\nest_career_3catLate career\n2.475\n\n\nlowerCI_career_3catLate career\n1.827\n\n\nhigherCI_career_3catLate career\n3.124\n\n\nse_career_3catLate career\n0.33\n\n\ntval_career_3catLate career\n7.504\n\n\npval_career_3catLate career\n0\n\n\nest_career_3catMid career\n1.043\n\n\nlowerCI_career_3catMid career\n0.636\n\n\nhigherCI_career_3catMid career\n1.449\n\n\nse_career_3catMid career\n0.207\n\n\ntval_career_3catMid career\n5.045\n\n\npval_career_3catMid career\n0\n\n\n\n\n\n\n# if you're less comfortable asking a question, is it less likely you ask one?\nsurvey$comfort_asking_rating &lt;- as.numeric(as.character(survey$comfort_asking_rating))\n\nask_comfort &lt;- glm(ask_questions ~ comfort_asking_rating + gender + career_3cat,\n                   family = binomial, data = subset(survey, !is.na(gender) & !is.na(career_3cat) & !is.na(comfort_asking_rating))) \n\nask_comfort_null &lt;- glm(ask_questions ~ gender + career_3cat, family = binomial,\n                        data = subset(survey, !is.na(gender) & !is.na(career_3cat) & !is.na(comfort_asking_rating))) \n\nm_comfort_ask_out &lt;- collect_out(model = ask_comfort, null = ask_comfort_null, name = \"comfort_ask_question\", n_factors = 5 , type=\"survey\", save = \"yes\",  dir = \"../results/survey\") \n\nm_comfort_ask_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\ncomfort_ask_question\n\n\nAIC\n371.653\n\n\nn_obs\n372\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n86.582\n\n\nintercept_estimate\n-2.781\n\n\nintercept_estimate_prop\n0.058\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-3.695\n\n\nintercept_ci_higher\n-1.866\n\n\nn_factors\n5\n\n\nest_comfort_asking_rating\n0.733\n\n\nlowerCI_comfort_asking_rating\n0.555\n\n\nhigherCI_comfort_asking_rating\n0.911\n\n\npval_comfort_asking_rating\n0\n\n\nzval_comfort_asking_rating\n8.07\n\n\nest_genderFemale\n0.259\n\n\nlowerCI_genderFemale\n-0.341\n\n\nhigherCI_genderFemale\n0.86\n\n\npval_genderFemale\n0.397\n\n\nzval_genderFemale\n0.846\n\n\nest_genderNon-binary\n2.686\n\n\nlowerCI_genderNon-binary\n0.372\n\n\nhigherCI_genderNon-binary\n4.999\n\n\npval_genderNon-binary\n0.023\n\n\nzval_genderNon-binary\n2.275\n\n\nest_career_3catLate career\n1.732\n\n\nlowerCI_career_3catLate career\n0.414\n\n\nhigherCI_career_3catLate career\n3.05\n\n\npval_career_3catLate career\n0.01\n\n\nzval_career_3catLate career\n2.575\n\n\nest_career_3catMid career\n0.336\n\n\nlowerCI_career_3catMid career\n-0.22\n\n\nhigherCI_career_3catMid career\n0.892\n\n\npval_career_3catMid career\n0.237\n\n\nzval_career_3catMid career\n1.183\n\n\n\n\n\n\n# indeed due to gender bias in being comfortable asking questions\n\n### isolate reasons\n# take out reasons that were ticked less than 15 times\nn_ticked &lt;- data.frame()\n\nfor (i in 1:ncol(survey)){\n  if (grepl(\"reason_question|reason_noquestion\", colnames(survey)[i]) == TRUE) {\n    sub &lt;- survey[,i]\n    sum &lt;- as.data.frame(summary(as.factor(sub)))\n    ticked &lt;- sum[\"1\",\"summary(as.factor(sub))\"]\n    row &lt;- data.frame(i = i,\n                      reason = colnames(survey)[i],\n                      ticked = ticked)\n    n_ticked &lt;- rbind(n_ticked, row)\n  }\n}\n\nticked_enough &lt;- subset(n_ticked, ticked &gt;= 15)\n\n# isolate questions about conditions, motivations and hesitations\nconditions &lt;- grep(\"more_comfort\", colnames(survey))\nreasons &lt;- c(ticked_enough$i)\nreasons_conditions &lt;- c(conditions, reasons)\n\n# take out 'other' reasons\nother &lt;- grep(\"_other\", colnames(survey)[reasons_conditions]) \nreasons_conditions &lt;- reasons_conditions[-other] #exclude 'other' categories\n\n# take out not chosen\nnotchosen &lt;- grep(\"reason_noquestion_notchosen\", colnames(survey)[reasons_conditions]) \nreasons_conditions &lt;- reasons_conditions[-notchosen] #exclude \n\noutput_reasons &lt;- data.frame()\n\nfor (i in reasons_conditions){\n  ## split conditions between motivation/hestitations \n  if (grepl(\"more_comfort\", colnames(survey)[i]) == TRUE) {\n    # null model\n    formula_null &lt;- as.formula(paste0(colnames(survey)[i], \"~ career_3cat\"))\n    null &lt;- MASS::polr(formula_null, data = subset(survey, !is.na(gender))) #polr for ordinal GLMs\n    \n    # alternative model with gender\n    formula_alt &lt;- formula(paste0(colnames(survey)[i], \"~ gender + career_3cat\"))  \n    model &lt;- MASS::polr(formula_alt, data = subset(survey, !is.na(gender)))\n    \n    sum &lt;- as.data.frame(coeftest(model)[,])\n    confint &lt;- as.data.frame(confint(model))\n    \n    # lrt\n    lrt &lt;- anova(null, model, test = \"Chisq\")\n    lrt_pval &lt;- lrt$`Pr(Chi)`[2]\n    lrt_chisq &lt;- lrt$`LR stat.`[2]\n    \n    out &lt;- data.frame(response = colnames(survey)[i],\n                      lrt_chisq = lrt_chisq,\n                      lrt_pval = lrt_pval,\n                      coef_female = sum[\"genderFemale\",\"Estimate\"],\n                      pval_female = sum[\"genderFemale\",\"Pr(&gt;|t|)\"],\n                      lower_female = confint[\"genderFemale\",\"2.5 %\"],\n                      upper_female = confint[\"genderFemale\",\"97.5 %\"],\n                      se_female = sum[\"genderFemale\", \"Std. Error\"],\n                      zval_tval_female = sum[\"genderFemale\", \"t value\"],\n                      coef_non_binary = sum[\"genderNon-binary\",\"Estimate\"],\n                      pval_non_binary = sum[\"genderNon-binary\",\"Pr(&gt;|t|)\"],\n                      lower_non_binary = confint[\"genderNon-binary\",\"2.5 %\"],\n                      upper_non_binary = confint[\"genderNon-binary\",\"97.5 %\"],\n                      se_non_binary = sum[\"genderNon-binary\", \"Std. Error\"],\n                      zval_tval_non_binary = sum[\"genderNon-binary\", \"t value\"],\n                      coef_mid = sum[\"career_3catMid\",\"Estimate\"],\n                      pval_mid = sum[\"career_3catMid\",\"Pr(&gt;|t|)\"],\n                      se_mid = sum[\"career_3catMid\", \"Std. Error\"],\n                      zval_tval_mid = sum[\"career_3catMid\", \"t value\"],\n                      coef_late = sum[\"career_3catLate\",\"Estimate\"],\n                      pval_late = sum[\"career_3catLate\",\"Pr(&gt;|t|)\"],\n                       se_late = sum[\"career_3catLate\", \"Std. Error\"],\n                      zval_tval_late = sum[\"career_3catLate\", \"t value\"],\n                      coef_other = sum[\"career_3catOther\",\"Estimate\"],\n                      pval_other = sum[\"career_3catOther\",\"Pr(&gt;|t|)\"],\n                       se_other = sum[\"career_3catOther\", \"Std. Error\"],\n                     zval_tval_other = sum[\"career_3catOther\", \"t value\"])\n    \n    \n    output_reasons &lt;- rbind(output_reasons, out)\n    \n    }\n  if (grepl(\"reason\", colnames(survey)[i]) == TRUE) {\n    # null model\n    formula_null &lt;- as.formula(paste0(colnames(survey)[i], \"~ career_3cat\"))\n    null &lt;- glm(formula_null, data = subset(survey, !is.na(gender)), family = \"binomial\")\n    # alternative model with gender\n    formula_alt &lt;- formula(paste0(colnames(survey)[i], \"~ gender + career_3cat\"))  \n    model &lt;- glm(formula_alt, data = survey, family = \"binomial\")\n    sum &lt;- summary(model)$coefficients\n    confint &lt;- (as.data.frame(summ(model, confint=TRUE)$coeftable))\n    \n    # lrt\n    lrt &lt;- anova(null, model, test = \"Chisq\")\n    lrt_pval &lt;- lrt$`Pr(&gt;Chi)`[2]\n    lrt_chisq &lt;- lrt$`Deviance`[2]\n    \n    # collect all output\n    out &lt;- data.frame(response = colnames(survey)[i],\n                      lrt_chisq = lrt_chisq,\n                      lrt_pval = lrt_pval,\n                      coef_female = sum[\"genderFemale\",\"Estimate\"],\n                      pval_female = sum[\"genderFemale\",\"Pr(&gt;|z|)\"],\n                      lower_female = confint[\"genderFemale\",\"2.5%\"],\n                      upper_female = confint[\"genderFemale\",\"97.5%\"],\n                      se_female = sum[\"genderFemale\", \"Std. Error\"],\n                      zval_tval_female = sum[\"genderFemale\", \"z value\"],\n                      coef_non_binary = sum[\"genderNon-binary\",\"Estimate\"],\n                      pval_non_binary = sum[\"genderNon-binary\",\"Pr(&gt;|z|)\"],\n                      lower_non_binary = confint[\"genderNon-binary\",\"2.5%\"],\n                      upper_non_binary = confint[\"genderNon-binary\",\"97.5%\"],\n                      se_non_binary = sum[\"genderNon-binary\", \"Std. Error\"],\n                      zval_tval_non_binary = sum[\"genderNon-binary\", \"z value\"],\n                      coef_mid = sum[\"career_3catMid career\",\"Estimate\"],\n                      se_mid = sum[\"career_3catMid career\",\"Std. Error\"],\n                      zval_tval_mid = sum[\"career_3catMid career\",\"z value\"],\n                      pval_mid = sum[\"career_3catMid career\",\"Pr(&gt;|z|)\"],\n                      coef_late = sum[\"career_3catLate career\",\"Estimate\"],\n                      se_late = sum[\"career_3catLate career\",\"Std. Error\"],\n                      zval_tval_late = sum[\"career_3catLate career\",\"z value\"],\n                      pval_late = sum[\"career_3catLate career\",\"Pr(&gt;|z|)\"],\n                      coef_other = sum[\"career_3catOther\",\"Estimate\"],\n                      se_other = sum[\"career_3catOther\",\"Std. Error\"],\n                      zval_tval_other = sum[\"career_3catOther\",\"z value\"],\n                      pval_other = sum[\"career_3catOther\",\"Pr(&gt;|z|)\"]\n                      )\n    \n    output_reasons &lt;- rbind(output_reasons, out)\n    }}\n  \n\n# adjust p-values for multiple testing\n\noutput_reasons$lrt_qval &lt;- p.adjust(output_reasons$lrt_pval, method = \"fdr\", n = nrow(output_reasons))\n\n# round to 3 digits\noutput_reasons &lt;- output_reasons %&gt;% mutate_if(is.numeric, round, digits=3)\n\n# save output in results\nwrite.csv(output_reasons, file = \"../results/question-asking/question_asking_why_survey_result.csv\", quote=F, row.names = F)\n\noutput_reasons %&gt;% arrange(lrt_qval) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nresponse\nlrt_chisq\nlrt_pval\ncoef_female\npval_female\nlower_female\nupper_female\nse_female\nzval_tval_female\ncoef_non_binary\npval_non_binary\nlower_non_binary\nupper_non_binary\nse_non_binary\nzval_tval_non_binary\ncoef_mid\npval_mid\nse_mid\nzval_tval_mid\ncoef_late\npval_late\nse_late\nzval_tval_late\ncoef_other\npval_other\nse_other\nzval_tval_other\nlrt_qval\n\n\n\n\nmore_comfort_asking_owngender_speaker_rating\n36.304\n0.000\n1.222\n0.000\n0.785\n1.673\n0.226\n5.402\n2.441\n0.000\n1.080\n3.781\n0.681\n3.583\n-0.345\n0.104\n0.211\n-1.631\n-0.617\n0.062\n0.330\n-1.869\n-0.222\n0.657\n0.500\n-0.444\n0.000\n\n\nmore_comfort_asking_owngender_audience_rating\n41.061\n0.000\n1.327\n0.000\n0.903\n1.764\n0.219\n6.054\n1.902\n0.004\n0.585\n3.201\n0.659\n2.888\n-0.114\n0.583\n0.208\n-0.549\n-0.453\n0.175\n0.333\n-1.358\n-0.161\n0.730\n0.466\n-0.345\n0.000\n\n\nmore_comfort_asking_owngender_host_rating\n19.640\n0.000\n0.921\n0.000\n0.487\n1.368\n0.224\n4.109\n1.578\n0.019\n0.255\n2.923\n0.672\n2.349\n-0.215\n0.318\n0.215\n-1.000\n-0.707\n0.033\n0.330\n-2.141\n0.119\n0.818\n0.515\n0.231\n0.000\n\n\nmore_comfort_asking_smallaudience_rating\n15.805\n0.000\n0.791\n0.000\n0.389\n1.196\n0.206\n3.843\n-0.074\n0.913\n-1.419\n1.250\n0.673\n-0.109\n-0.295\n0.145\n0.202\n-1.460\n-0.588\n0.058\n0.309\n-1.901\n-0.370\n0.445\n0.484\n-0.765\n0.002\n\n\nreason_noquestion_phrasing\n11.191\n0.004\n0.900\n0.002\n0.343\n1.458\n0.285\n3.164\n1.026\n0.205\n-0.562\n2.614\n0.810\n1.266\n-0.570\n0.024\n0.253\n-2.251\n-1.588\n0.002\n0.503\n-3.156\n-0.987\n0.141\n0.670\n-1.474\n0.015\n\n\nreason_noquestion_confidence\n7.642\n0.022\n0.784\n0.012\n0.176\n1.392\n0.310\n2.526\n-0.233\n0.835\n-2.422\n1.957\n1.117\n-0.208\n-0.420\n0.119\n0.269\n-1.559\n-2.914\n0.005\n1.026\n-2.840\n-0.590\n0.379\n0.671\n-0.879\n0.073\n\n\nreason_noquestion_intimidated_audience\n6.270\n0.043\n0.760\n0.021\n0.115\n1.405\n0.329\n2.309\n-0.107\n0.925\n-2.326\n2.111\n1.132\n-0.095\n-1.211\n0.000\n0.322\n-3.762\n-1.197\n0.018\n0.505\n-2.369\n-1.075\n0.169\n0.781\n-1.376\n0.124\n\n\nreason_question_voiceheard\n5.946\n0.051\n-0.601\n0.193\n-1.505\n0.304\n0.461\n-1.302\n1.787\n0.057\n-0.052\n3.626\n0.938\n1.904\n0.856\n0.094\n0.511\n1.673\n0.703\n0.334\n0.727\n0.967\n1.564\n0.072\n0.869\n1.800\n0.128\n\n\nreason_noquestion_private\n5.367\n0.068\n0.297\n0.201\n-0.159\n0.753\n0.233\n1.277\n-1.553\n0.158\n-3.707\n0.601\n1.099\n-1.413\n-0.047\n0.837\n0.229\n-0.206\n-1.106\n0.004\n0.379\n-2.918\n0.394\n0.473\n0.550\n0.717\n0.152\n\n\nreason_noquestion_intimidated_setting\n4.871\n0.088\n0.719\n0.126\n-0.203\n1.641\n0.470\n1.528\n2.002\n0.036\n0.129\n3.876\n0.956\n2.094\n-0.974\n0.044\n0.483\n-2.016\n0.293\n0.563\n0.505\n0.579\n0.698\n0.315\n0.694\n1.006\n0.175\n\n\nreason_question_interest\n4.050\n0.132\n-0.529\n0.166\n-1.275\n0.218\n0.381\n-1.387\n14.303\n0.987\n-1753.050\n1781.656\n901.727\n0.016\n-0.529\n0.107\n0.329\n-1.610\n1.731\n0.096\n1.040\n1.665\n0.646\n0.543\n1.064\n0.608\n0.217\n\n\nreason_noquestion_relevance\n3.894\n0.143\n-0.264\n0.255\n-0.719\n0.190\n0.232\n-1.139\n-1.688\n0.125\n-3.844\n0.469\n1.100\n-1.534\n-0.329\n0.160\n0.234\n-1.403\n-0.710\n0.051\n0.363\n-1.955\n0.636\n0.248\n0.550\n1.156\n0.217\n\n\nreason_noquestion_notime\n3.771\n0.152\n-0.403\n0.108\n-0.895\n0.088\n0.251\n-1.608\n0.620\n0.438\n-0.948\n2.188\n0.800\n0.775\n0.554\n0.034\n0.262\n2.116\n1.204\n0.001\n0.353\n3.406\n0.374\n0.543\n0.615\n0.608\n0.217\n\n\nreason_noquestion_introvert\n4.035\n0.133\n0.515\n0.072\n-0.047\n1.077\n0.287\n1.797\n-0.426\n0.702\n-2.604\n1.752\n1.111\n-0.383\n-0.347\n0.192\n0.267\n-1.303\n-0.961\n0.041\n0.471\n-2.042\n-1.024\n0.189\n0.780\n-1.312\n0.217\n\n\nmore_comfort_asking_know_speaker_rating\n2.759\n0.252\n0.304\n0.123\n-0.081\n0.691\n0.197\n1.545\n-0.187\n0.778\n-1.512\n1.138\n0.665\n-0.282\n-0.334\n0.094\n0.199\n-1.679\n-0.836\n0.006\n0.304\n-2.752\n-0.776\n0.119\n0.497\n-1.562\n0.318\n\n\nreason_noquestion_clever\n2.737\n0.254\n0.418\n0.107\n-0.091\n0.928\n0.260\n1.610\n0.516\n0.523\n-1.067\n2.099\n0.808\n0.639\n-0.815\n0.001\n0.250\n-3.258\n-1.789\n0.000\n0.499\n-3.584\n-0.449\n0.432\n0.571\n-0.787\n0.318\n\n\nreason_question_relevance\n1.289\n0.525\n0.072\n0.753\n-0.375\n0.518\n0.228\n0.314\n0.929\n0.280\n-0.757\n2.615\n0.860\n1.080\n0.096\n0.673\n0.228\n0.422\n-0.052\n0.875\n0.334\n-0.157\n-1.029\n0.088\n0.604\n-1.705\n0.618\n\n\nreason_question_appreciate\n0.984\n0.612\n-0.243\n0.365\n-0.769\n0.283\n0.268\n-0.907\n-0.597\n0.589\n-2.763\n1.568\n1.105\n-0.541\n0.144\n0.608\n0.280\n0.514\n0.496\n0.192\n0.380\n1.304\n0.418\n0.497\n0.615\n0.679\n0.679\n\n\nreason_question_understand\n0.841\n0.657\n-0.049\n0.849\n-0.553\n0.455\n0.257\n-0.190\n-0.748\n0.348\n-2.310\n0.814\n0.797\n-0.938\n0.207\n0.426\n0.260\n0.796\n-0.445\n0.204\n0.351\n-1.271\n0.934\n0.230\n0.778\n1.200\n0.691\n\n\nreason_noquestion_misunderstand\n0.363\n0.834\n0.037\n0.879\n-0.435\n0.508\n0.240\n0.152\n0.484\n0.550\n-1.101\n2.069\n0.809\n0.598\n-0.958\n0.000\n0.240\n-3.995\n-1.496\n0.000\n0.402\n-3.718\n0.019\n0.972\n0.540\n0.036\n0.834\n\n\n\n\n\n\n\nIncluding gender as an explanatory variable included the fit of the model for most conditions. In order of highest to lowest effects, women are more comfortable asking questions they are more comfortable asking questions if: women are represented in the audience, the speaker is a woman, the host is a woman, and when the audience is smaller. Women are not more comfortable asking a question if they know the speaker. Despite the low sample size of non-binary respondents, also people with non-binary gender identities (N = 7) are more comfortable asking questions if their own genders identity is represented in the audience, if the speakers is of their own gender identity and if the host is of their own gender identity. Based on self-reports, female and non-binary representation in various contexts at the congress appears to make women and non-binary researchers more comfortable in asking questions, respectively.\nAlthough no motivations were significantly affected by gender, there were two motivations that were (almost) significantly affected by gender after correcting for multiple testing: “Afraid I would not be able to phrase/articulate my question well”, and “I did not have the confidence” (LRT FDR-corrected q-value &lt; 0.1). These two motivations were also affected by career stage, where early career researchers are more likely to tick the phrasing/articulation-related and confidence-related hesitation compared to late career researchers.\n\n\n6.1.4 Analysis of motivations and hesitations that predict question asking\nNext, we fitted 19 more binomial GLMs to test which one of the motivations and hesitations was a statistically significant predictor for the probability of a person asking a question during the congress. The dependent variable in these models was therefore the response to the question “Did you ask one or more questions during Q&A sessions?” (1 for yes, 0 for no) and the response variable the binomial response whether the motivation or hesitation was ticked (1) or not (0), while again controlling for career stage.\n\n# loop over reasons again but without comfort\n\n# take out 'other'\nother &lt;- grep(\"_other\", colnames(survey)[reasons]) \nreasons &lt;- reasons[-other]\n\n# take out not chosen\nnotchosen &lt;- grep(\"reason_noquestion_notchosen\", colnames(survey)[reasons]) \nreasons &lt;- reasons[-notchosen] #exclude \n\noutput_asking &lt;- data.frame()\n\nfor (i in reasons){\n  subset &lt;- survey %&gt;% filter(!is.na(.[[i]]) & !is.na(colnames(survey)[i]) &\n                     !is.na(gender) & !is.na(career_3cat) &\n                     !is.na(ask_questions))\n\n\n  # null model\n  formula_null &lt;- as.formula(paste0(\"ask_questions ~ gender +career_3cat\"))\n  null &lt;- glm(formula_null, data = subset, family = \"binomial\")\n  # alternative model with gender\n  formula_alt &lt;- as.formula(paste0(\"ask_questions ~ \", \n                                   colnames(survey)[i], \"+ gender + career_3cat\")) \n  model &lt;- glm(formula_alt, data = subset, family = \"binomial\")\n  \n  # summary\n  sum &lt;- summary(model)$coefficients\n  confint &lt;- (as.data.frame(summ(model, confint=TRUE)$coeftable))\n  \n  # lrt\n  lrt &lt;- anova(null, model, test = \"Chisq\")\n  lrt_sig &lt;- lrt$`Pr(&gt;Chi)`[2]\n  lrt_chisq &lt;- lrt$`Deviance`[2]\n  \n  out &lt;- data.frame(predictor = colnames(survey)[i],\n                    lrt_chisq = lrt_chisq,\n                    lrt_sig = lrt_sig,\n                    coef_reason = sum[2,\"Estimate\"],\n                    se_reason = sum[2,\"Std. Error\"],\n                    z_reason = sum[2, \"z value\"],\n                    lower_reason = confint[2,\"2.5%\"],\n                    upper_reason = confint[2,\"97.5%\"],\n                    pval_reason = sum[2,\"Pr(&gt;|z|)\"],\n                    coef_female = sum[\"genderFemale\",\"Estimate\"],\n                    pval_female = sum[\"genderFemale\",\"Pr(&gt;|z|)\"],\n                    coef_non_binary = sum[\"genderNon-binary\",\"Estimate\"],\n                    pval_non_binary = sum[\"genderNon-binary\",\"Pr(&gt;|z|)\"],\n                    coef_mid = sum[\"career_3catMid career\",\"Estimate\"],\n                    pval_mid = sum[\"career_3catMid career\",\"Pr(&gt;|z|)\"],\n                    coef_late = sum[\"career_3catLate career\",\"Estimate\"],\n                    pval_late = sum[\"career_3catLate career\",\"Pr(&gt;|z|)\"],\n                    coef_other = sum[\"career_3catOther\",\"Estimate\"],\n                    pval_other = sum[\"career_3catOther\",\"Pr(&gt;|z|)\"]\n                    \n  )\n  \n  output_asking &lt;- rbind(output_asking, out)\n \n}\n\n## add a column if this variable has a significant effect of gender\nsig_gender &lt;- subset(output_reasons, lrt_qval &lt; 0.05)\n\noutput_asking &lt;- output_asking %&gt;% mutate(gender_effect = case_when(\n  output_asking$predictor %in% sig_gender$response ~ \"gender_effect\",\n  TRUE ~ \"no_gender_effect\"\n))\n\n# add a column with the number of people saying this factor was important\nn_responses &lt;- data.frame()\nfor (i in reasons){\n  summary &lt;- as.data.frame(table(survey[,i], survey$gender))\n  response_yes_female &lt;- subset(summary, Var1 == \"1\" & Var2 == \"Female\")$Freq\n  response_yes_male &lt;- subset(summary, Var1 == \"1\" & Var2 == \"Male\")$Freq\n  response_yes_non_binary &lt;- subset(summary, Var1 == \"1\" & Var2 == \"Non-binary\")$Freq\n  \n  predictor &lt;- data.frame(predictor = colnames(survey)[i],\n                          n_yes_female = response_yes_female,\n                          n_yes_male = response_yes_male,\n                          n_yes_non_binary = response_yes_non_binary)\n  n_responses &lt;- rbind(n_responses, predictor)\n}\n\noutput_asking &lt;- left_join(output_asking, n_responses, by = \"predictor\")\n\n# round\noutput_asking &lt;- output_asking %&gt;% mutate_if(is.numeric, round, digits=3)\n\n# subset significant predictors of question asking\n\noutput_asking$lrt_qval &lt;- p.adjust(output_asking$lrt_sig,\n                                   method=\"fdr\",\n                                   n=nrow(output_asking))\n\noutput_asking %&gt;% arrange(lrt_qval) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\npredictor\nlrt_chisq\nlrt_sig\ncoef_reason\nse_reason\nz_reason\nlower_reason\nupper_reason\npval_reason\ncoef_female\npval_female\ncoef_non_binary\npval_non_binary\ncoef_mid\npval_mid\ncoef_late\npval_late\ncoef_other\npval_other\ngender_effect\nn_yes_female\nn_yes_male\nn_yes_non_binary\nlrt_qval\n\n\n\n\nreason_noquestion_notime\n12.415\n0.000\n0.979\n0.290\n3.382\n0.412\n1.547\n0.001\n-0.435\n0.094\n1.195\n0.287\n0.733\n0.003\n2.488\n0\n1.060\n0.085\nno_gender_effect\n65\n38\n3\n0.0000000\n\n\nreason_noquestion_introvert\n23.122\n0.000\n-1.270\n0.269\n-4.715\n-1.798\n-0.742\n0.000\n-0.412\n0.117\n1.168\n0.293\n0.762\n0.002\n2.569\n0\n0.929\n0.136\nno_gender_effect\n68\n20\n1\n0.0000000\n\n\nreason_noquestion_private\n19.842\n0.000\n-1.038\n0.237\n-4.383\n-1.502\n-0.574\n0.000\n-0.434\n0.096\n0.928\n0.403\n0.822\n0.001\n2.474\n0\n1.224\n0.048\nno_gender_effect\n125\n46\n1\n0.0000000\n\n\nreason_question_interest\n10.324\n0.001\n1.073\n0.340\n3.153\n0.406\n1.741\n0.002\n-0.473\n0.069\n1.156\n0.298\n0.902\n0.000\n2.583\n0\n1.052\n0.086\nno_gender_effect\n220\n102\n7\n0.0037500\n\n\nreason_question_voiceheard\n7.029\n0.008\n1.679\n0.763\n2.200\n0.183\n3.175\n0.028\n-0.495\n0.055\n0.971\n0.390\n0.747\n0.002\n2.651\n0\n0.993\n0.107\nno_gender_effect\n12\n9\n2\n0.0171429\n\n\nreason_noquestion_intimidated_audience\n7.328\n0.007\n-0.766\n0.284\n-2.696\n-1.323\n-0.209\n0.007\n-0.428\n0.098\n1.277\n0.253\n0.670\n0.007\n2.546\n0\n0.975\n0.112\nno_gender_effect\n59\n14\n1\n0.0171429\n\n\nreason_noquestion_confidence\n6.979\n0.008\n-0.698\n0.265\n-2.636\n-1.216\n-0.179\n0.008\n-0.422\n0.103\n1.268\n0.257\n0.760\n0.002\n2.470\n0\n1.035\n0.092\nno_gender_effect\n68\n16\n1\n0.0171429\n\n\nreason_noquestion_clever\n5.329\n0.021\n-0.559\n0.242\n-2.311\n-1.033\n-0.085\n0.021\n-0.464\n0.072\n1.377\n0.219\n0.714\n0.004\n2.492\n0\n1.056\n0.085\nno_gender_effect\n89\n29\n3\n0.0393750\n\n\nreason_question_understand\n3.936\n0.047\n0.511\n0.258\n1.983\n0.006\n1.016\n0.047\n-0.515\n0.044\n1.399\n0.212\n0.790\n0.001\n2.746\n0\n1.020\n0.094\nno_gender_effect\n186\n82\n4\n0.0783333\n\n\nreason_question_appreciate\n2.247\n0.134\n0.426\n0.288\n1.479\n-0.138\n0.990\n0.139\n-0.501\n0.050\n1.295\n0.243\n0.791\n0.001\n2.638\n0\n1.059\n0.081\nno_gender_effect\n53\n28\n1\n0.2010000\n\n\nreason_noquestion_phrasing\n1.803\n0.179\n-0.335\n0.249\n-1.346\n-0.822\n0.153\n0.178\n-0.452\n0.081\n1.361\n0.223\n0.759\n0.002\n2.573\n0\n1.028\n0.092\ngender_effect\n88\n20\n3\n0.2440909\n\n\nreason_noquestion_relevance\n0.848\n0.357\n0.215\n0.234\n0.919\n-0.243\n0.673\n0.358\n-0.503\n0.049\n1.331\n0.231\n0.816\n0.001\n2.701\n0\n1.054\n0.083\nno_gender_effect\n100\n50\n1\n0.4462500\n\n\nreason_noquestion_intimidated_setting\n0.184\n0.668\n-0.171\n0.398\n-0.430\n-0.952\n0.609\n0.667\n-0.506\n0.048\n1.311\n0.240\n0.787\n0.001\n2.667\n0\n1.106\n0.069\nno_gender_effect\n27\n6\n2\n0.7707692\n\n\nreason_question_relevance\n0.028\n0.866\n-0.038\n0.229\n-0.168\n-0.486\n0.409\n0.866\n-0.514\n0.044\n1.272\n0.252\n0.798\n0.001\n2.661\n0\n1.079\n0.076\nno_gender_effect\n129\n55\n5\n0.8660000\n\n\nreason_noquestion_misunderstand\n0.042\n0.838\n-0.048\n0.235\n-0.205\n-0.510\n0.413\n0.838\n-0.515\n0.043\n1.271\n0.252\n0.786\n0.001\n2.646\n0\n1.090\n0.073\nno_gender_effect\n105\n44\n4\n0.8660000\n\n\n\n\n\n\nwrite.csv(output_asking, file = \"../results/question-asking/model_qa_conditions_motivations_gender_effect.csv\",\n          quote=F, row.names = F)"
  },
  {
    "objectID": "qmd/5_question_asking_why.html#observational-data",
    "href": "qmd/5_question_asking_why.html#observational-data",
    "title": "6  Factors that affect the gender disparity",
    "section": "6.2 Observational data",
    "text": "6.2 Observational data\nNext, we investigated if we observe this same pattern in the observational data on question asking behaviour. We introduced a series of potential explanatory variables in the model described in the previous section that identified the gender disparity in question asking (Section 3.1). More specifically, we built five binomial GLMMs with one of the following five factors as an independent variable, 1) speaker gender, 2) gender proportion of the audience, 3) total audience size, 4) room size, or 5) host gender. These five factors therefore represent the same five conditions that were investigated above using the survey data, respectively.\nBecause of the result of the previous section, namely that the gender disparity in question asking arises due to women raising their hands less often (Section 5.2), we only modelled the effect of these five factors on the number of hands that were raised as a dependent variable. We again accounted for the gender proportion of the audience and the nonindependence of talks within a session. We assess significance of each factor through a likelihood ratio test.\n\n# speaker gender\ndata_hands$speaker_gender &lt;- factor(data_hands$speaker_gender, levels=c(\"M\", \"F\"))\n\nm_speaker_gender &lt;- glmer(cbind(hands_women, hands_men) ~ speaker_gender + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(speaker_gender)))\n\nsummary(m_speaker_gender)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \ncbind(hands_women, hands_men) ~ speaker_gender + (1 | session_id/talk_id)\n   Data: subset(data_hands, !is.na(speaker_gender))\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   574.9    590.1   -283.4    566.9      327 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.57063 -0.93767 -0.03456  0.91578  1.79208 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.0541   0.2326  \n session_id         (Intercept) 0.1135   0.3368  \nNumber of obs: 331, groups:  talk_id:session_id, 121; session_id, 24\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.568417   0.164963  -3.446  0.00057 ***\nspeaker_genderF -0.003861   0.197053  -0.020  0.98437    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nspekr_gndrF -0.697\n\nm_speaker_gender_null &lt;- glmer(cbind(hands_women, hands_men) ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(speaker_gender)))\n\n## helper function to collect output\nm_speaker_gender_out &lt;- collect_out(model = m_speaker_gender, null = m_speaker_gender_null, name=\"qa_speaker_gender\", n_factors=1, type=\"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nm_speaker_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nqa_speaker_gender\n\n\nAIC\n574.872\n\n\nn_obs\n331\n\n\nlrt_pval\n0.984\n\n\nlrt_chisq\n0\n\n\nintercept_estimate\n-0.568\n\n\nintercept_estimate_prop\n0.362\n\n\nintercept_pval\n0.001\n\n\nintercept_ci_lower\n-0.892\n\n\nintercept_ci_higher\n-0.245\n\n\nn_factors\n1\n\n\nest_speaker_genderF\n-0.004\n\n\nlowerCI_speaker_genderF\n-0.39\n\n\nhigherCI_speaker_genderF\n0.382\n\n\npval_speaker_genderF\n0.984\n\n\nzval_speaker_genderF\n-0.02\n\n\n\n\n\n\n# not significant\n\n# audience gender proportion\n\nm_audience_gender &lt;- glmer(cbind(hands_women, hands_men) ~ audience_women_prop + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(audience_women_prop)))\n\nsummary(m_audience_gender)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \ncbind(hands_women, hands_men) ~ audience_women_prop + (1 | session_id/talk_id)\n   Data: subset(data_hands, !is.na(audience_women_prop))\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   606.4    621.8   -299.2    598.4      345 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.56732 -0.95301 -0.01054  0.94821  1.63888 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.08359  0.2891  \n session_id         (Intercept) 0.05665  0.2380  \nNumber of obs: 349, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)           0.5228     0.7734   0.676    0.499\naudience_women_prop  -1.7424     1.2090  -1.441    0.150\n\nCorrelation of Fixed Effects:\n            (Intr)\nadnc_wmn_pr -0.991\n\nm_audience_gender_null &lt;- glmer(cbind(hands_women, hands_men) ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(audience_women_prop)))\n\n## helper function to collect output\nm_audience_gender_out &lt;- collect_out(model = m_audience_gender, null = m_audience_gender_null, name=\"qa_audience_gender\", n_factors=1, type=\"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nm_audience_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nqa_audience_gender\n\n\nAIC\n606.38\n\n\nn_obs\n349\n\n\nlrt_pval\n0.154\n\n\nlrt_chisq\n2.028\n\n\nintercept_estimate\n0.523\n\n\nintercept_estimate_prop\n0.628\n\n\nintercept_pval\n0.499\n\n\nintercept_ci_lower\n-0.993\n\n\nintercept_ci_higher\n2.039\n\n\nn_factors\n1\n\n\nest_audience_women_prop\n-1.742\n\n\nlowerCI_audience_women_prop\n-4.112\n\n\nhigherCI_audience_women_prop\n0.627\n\n\npval_audience_women_prop\n0.15\n\n\nzval_audience_women_prop\n-1.441\n\n\n\n\n\n\n# almost significant\n\n# host gender\ndata_hands$host_1_gender &lt;- factor(data_hands$host_1_gender, levels=c(\"M\", \"F\"))\n\nm_host_gender &lt;- glmer(cbind(hands_women, hands_men) ~ host_1_gender + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(host_1_gender)))\n\nsummary(m_host_gender)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \ncbind(hands_women, hands_men) ~ host_1_gender + (1 | session_id/talk_id)\n   Data: subset(data_hands, !is.na(host_1_gender))\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   546.4    561.5   -269.2    538.4      312 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6184 -0.9117 -0.1024  0.9602  1.9401 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.03963  0.1991  \n session_id         (Intercept) 0.09504  0.3083  \nNumber of obs: 316, groups:  talk_id:session_id, 115; session_id, 23\n\nFixed effects:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.8204     0.1948  -4.212 2.54e-05 ***\nhost_1_genderF   0.2963     0.2444   1.213    0.225    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nhst_1_gndrF -0.795\n\nm_host_gender_null &lt;- glmer(cbind(hands_women, hands_men) ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(host_1_gender)))\n\n## helper function to collect output\nm_host_gender_out &lt;- collect_out(model = m_host_gender, null = m_host_gender_null, name=\"qa_host_gender\", n_factors=1, type=\"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nm_host_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nqa_host_gender\n\n\nAIC\n546.441\n\n\nn_obs\n316\n\n\nlrt_pval\n0.217\n\n\nlrt_chisq\n1.522\n\n\nintercept_estimate\n-0.82\n\n\nintercept_estimate_prop\n0.306\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-1.202\n\n\nintercept_ci_higher\n-0.439\n\n\nn_factors\n1\n\n\nest_host_1_genderF\n0.296\n\n\nlowerCI_host_1_genderF\n-0.183\n\n\nhigherCI_host_1_genderF\n0.775\n\n\npval_host_1_genderF\n0.225\n\n\nzval_host_1_genderF\n1.213\n\n\n\n\n\n\n# not significant \n\n# audience size\n\nm_audience_size &lt;- glmer(cbind(hands_women, hands_men) ~ audience_total + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(audience_total)))\n\nsummary(m_audience_size)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \ncbind(hands_women, hands_men) ~ audience_total + (1 | session_id/talk_id)\n   Data: subset(data_hands, !is.na(audience_total))\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   608.4    623.8   -300.2    600.4      345 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.50007 -0.94290 -0.00701  0.92802  1.81278 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.10857  0.3295  \n session_id         (Intercept) 0.05458  0.2336  \nNumber of obs: 349, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n                Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)    -0.524947   0.268331  -1.956   0.0504 .\naudience_total -0.001031   0.004318  -0.239   0.8113  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\naudienc_ttl -0.916\n\nm_audience_size_null &lt;- glmer(cbind(hands_women, hands_men) ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(audience_total)))\n\n## helper function to collect output\nm_audience_size_out &lt;- collect_out(model = m_audience_size, null = m_audience_size_null, name=\"qa_audience_size\", n_factors=1, type=\"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nm_audience_size_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nqa_audience_size\n\n\nAIC\n608.351\n\n\nn_obs\n349\n\n\nlrt_pval\n0.811\n\n\nlrt_chisq\n0.057\n\n\nintercept_estimate\n-0.525\n\n\nintercept_estimate_prop\n0.372\n\n\nintercept_pval\n0.05\n\n\nintercept_ci_lower\n-1.051\n\n\nintercept_ci_higher\n0.001\n\n\nn_factors\n1\n\n\nest_audience_total\n-0.001\n\n\nlowerCI_audience_total\n-0.009\n\n\nhigherCI_audience_total\n0.007\n\n\npval_audience_total\n0.811\n\n\nzval_audience_total\n-0.239\n\n\n\n\n\n\n# not significant \n\n# room size\ndata_hands$room_size &lt;- factor(data_hands$room_size, levels = c( \"Large\",\"Small\", \"Medium\"))\n                               \nm_room_size &lt;- glmer(cbind(hands_women, hands_men) ~ room_size + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(room_size)))\n\nsummary(m_room_size)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(hands_women, hands_men) ~ room_size + (1 | session_id/talk_id)\n   Data: subset(data_hands, !is.na(room_size))\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   609.0    628.2   -299.5    599.0      344 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5545 -0.9263 -0.0012  0.9380  1.7043 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.11381  0.3374  \n session_id         (Intercept) 0.02748  0.1658  \nNumber of obs: 349, groups:  talk_id:session_id, 127; session_id, 24\n\nFixed effects:\n                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)      -0.5521     0.1725  -3.201  0.00137 **\nroom_sizeSmall   -0.1511     0.2313  -0.653  0.51356   \nroom_sizeMedium   0.1678     0.2680   0.626  0.53117   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) rm_szS\nroom_szSmll -0.745       \nroom_sizMdm -0.626  0.471\n\nm_room_size_null &lt;- glmer(cbind(hands_women, hands_men) ~ 1 + (1|session_id/talk_id), family = \"binomial\", offset=boot::logit(audience_women_prop), data = subset(data_hands, !is.na(room_size)))\n\n## helper function to collect output\nm_room_size_out &lt;- collect_out(model = m_room_size, null = m_room_size_null, name=\"qa_room_size\", n_factors=2, type=\"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nm_room_size_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2\n\n\n\n\nmodel_name\nqa_room_size\n\n\nAIC\n608.974\n\n\nn_obs\n349\n\n\nlrt_pval\n0.488\n\n\nlrt_chisq\n1.434\n\n\nintercept_estimate\n-0.552\n\n\nintercept_estimate_prop\n0.365\n\n\nintercept_pval\n0.001\n\n\nintercept_ci_lower\n-0.89\n\n\nintercept_ci_higher\n-0.214\n\n\nn_factors\n2\n\n\nest_room_sizeSmall\n-0.151\n\n\nlowerCI_room_sizeSmall\n-0.605\n\n\nhigherCI_room_sizeSmall\n0.302\n\n\npval_room_sizeSmall\n0.514\n\n\nzval_room_sizeSmall\n-0.653\n\n\nest_room_sizeMedium\n0.168\n\n\nlowerCI_room_sizeMedium\n-0.357\n\n\nhigherCI_room_sizeMedium\n0.693\n\n\npval_room_sizeMedium\n0.531\n\n\nzval_room_sizeMedium\n0.626\n\n\n\n\n\n\n# not significant \n\nThe results based on the observational data contrast our findings from the survey. None of the five factors significantly improved the fit of the models, indicating that they do not significantly affect the probability that a woman raises her hand (LRT p-values &gt; 0.05). The only variable that got close to being significant was the gender proportion of the audience, with an LRT p-value of 0.154, where a higher number of women in the audience was associated with less women raising their hand (estimate = -1.742), opposite of expectations.\nTherefore, although self-reports indicate that women feel more comfortable asking questions in certain situations, observational data collected at the congress indicate that in practice, these conditions are in fact not associated with the probability that a woman asks a question."
  },
  {
    "objectID": "qmd/6_question_experiment.html#manipulated-data",
    "href": "qmd/6_question_experiment.html#manipulated-data",
    "title": "7  Question asking experiment",
    "section": "7.1 Manipulated data",
    "text": "7.1 Manipulated data\nFirst, we focus on manipulated sessions only, where we exclude situations where the host could not make a choice or if the gender proportion in the audience or the gender of the questioner was unknown (or disagreed upon between observers).\nWe assess the significance of the condition using an LRT as well as a Wald test.\n\n### Asking questions\n\nman_ask_null &lt;- glmer(gender_questioner_female ~ (1|session_id/talk_id), data = data_analysis_tr, family = binomial, offset = boot::logit(audience_women_prop))\n\nman_ask &lt;- glmer(gender_questioner_female ~ -1 + condition + (1|session_id/talk_id), data = data_analysis_tr, family = binomial, offset = boot::logit(audience_women_prop))\n\n# LRT\ndrop1(test=\"Chisq\",man_ask) \n\nSingle term deletions\n\nModel:\ngender_questioner_female ~ -1 + condition + (1 | session_id/talk_id)\n          npar    AIC    LRT Pr(Chi)\n&lt;none&gt;         294.73               \ncondition    1 294.87 2.1355  0.1439\n\nsummary(man_ask)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + condition + (1 | session_id/talk_id)\n   Data: data_analysis_tr\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   294.7    308.3   -143.4    286.7      216 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5555 -0.9521  0.6339  0.9046  1.4779 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 220, groups:  talk_id:session_id, 90; session_id, 38\n\nFixed effects:\n           Estimate Std. Error z value Pr(&gt;|z|)   \nconditionF  -0.6561     0.2039  -3.219  0.00129 **\nconditionM  -0.2514     0.1880  -1.337  0.18112   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           cndtnF\nconditionM 0.000 \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_man_ask &lt;- collect_out(model = man_ask, null = man_ask_null,  name = \"QA_mani_asking\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_man_ask %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_mani_asking\n\n\nAIC\n294.732\n\n\nn_obs\n220\n\n\nlrt_pval\n0.144\n\n\nlrt_chisq\n2.135\n\n\nn_factors\n2\n\n\nest_conditionF\n-0.656\n\n\nest_probabitily_conditionF\n0.342\n\n\nlowerCI_conditionF\n-1.056\n\n\nhigherCI_conditionF\n-0.257\n\n\npval_conditionF\n0.001\n\n\nzval_conditionF\n-3.219\n\n\nest_conditionM\n-0.251\n\n\nest_probabitily_conditionM\n0.437\n\n\nlowerCI_conditionM\n-0.62\n\n\nhigherCI_conditionM\n0.117\n\n\npval_conditionM\n0.181\n\n\nzval_conditionM\n-1.337\n\n\n\n\n\n\n### Raising hands\n\n# remove when no hands when were raised, only men or only women\ndata_analysis_tr_hands &lt;- subset(data_analysis_tr, hands_total &gt; 0 & !is.na(hands_women) & !is.na(hands_men) & \n  !is.na(hands_total))\n\nman_hands_null &lt;- glmer(cbind(hands_women, hands_men) ~  (1|session_id/talk_id), data = data_analysis_tr_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nman_hands &lt;- glmer(cbind(hands_women, hands_men) ~ -1 + condition + (1|session_id/talk_id), data = data_analysis_tr_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nsummary(man_hands)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \ncbind(hands_women, hands_men) ~ -1 + condition + (1 | session_id/talk_id)\n   Data: data_analysis_tr_hands\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   330.6    343.9   -161.3    322.6      200 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6465 -0.7914 -0.1532  0.9407  1.6499 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.11951  0.3457  \n session_id         (Intercept) 0.01973  0.1405  \nNumber of obs: 204, groups:  talk_id:session_id, 85; session_id, 38\n\nFixed effects:\n           Estimate Std. Error z value Pr(&gt;|z|)    \nconditionF  -0.9191     0.2038  -4.510 6.48e-06 ***\nconditionM  -0.6171     0.1752  -3.523 0.000427 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           cndtnF\nconditionM 0.000 \n\nm_qa_man_hands &lt;- collect_out(model = man_hands, null = man_hands_null,  name = \"QA_mani_hands\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_man_hands %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_mani_hands\n\n\nAIC\n330.649\n\n\nn_obs\n204\n\n\nlrt_pval\n0.251\n\n\nlrt_chisq\n1.32\n\n\nn_factors\n2\n\n\nest_conditionF\n-0.919\n\n\nest_probabitily_conditionF\n0.285\n\n\nlowerCI_conditionF\n-1.319\n\n\nhigherCI_conditionF\n-0.52\n\n\npval_conditionF\n0\n\n\nzval_conditionF\n-4.51\n\n\nest_conditionM\n-0.617\n\n\nest_probabitily_conditionM\n0.35\n\n\nlowerCI_conditionM\n-0.96\n\n\nhigherCI_conditionM\n-0.274\n\n\npval_conditionM\n0\n\n\nzval_conditionM\n-3.523\n\n\n\n\n\n\n## Getting chosen\n\n# exclude cases where the host could not choose\ndata_analysis_tr_chosen &lt;- subset(data_analysis_tr, hands_prop_women &gt; 0 & hands_prop_women &lt; 1) \n\nnrow(data_analysis_tr_chosen) #49\n\n[1] 49\n\nman_chosen_null &lt;- glmer(gender_questioner_female ~ (1|session_id/talk_id), data_analysis_tr_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nman_chosen &lt;- glmer(gender_questioner_female ~ -1 + condition + (1|session_id/talk_id), data_analysis_tr_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nsummary(man_chosen)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + condition + (1 | session_id/talk_id)\n   Data: data_analysis_tr_chosen\n Offset: boot::logit(hands_prop_women)\n\n     AIC      BIC   logLik deviance df.resid \n    72.2     79.8    -32.1     64.2       45 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.3525 -1.3582  0.7113  0.7362  1.0412 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev. \n talk_id:session_id (Intercept) 2.434e-15 4.934e-08\n session_id         (Intercept) 0.000e+00 0.000e+00\nNumber of obs: 49, groups:  talk_id:session_id, 32; session_id, 20\n\nFixed effects:\n           Estimate Std. Error z value Pr(&gt;|z|)\nconditionF   0.6124     0.4481   1.366    0.172\nconditionM   0.6815     0.4181   1.630    0.103\n\nCorrelation of Fixed Effects:\n           cndtnF\nconditionM 0.000 \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_man_chosen &lt;- collect_out(model = man_chosen, null = man_chosen_null,  name = \"QA_mani_chosen\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_man_chosen %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_mani_chosen\n\n\nAIC\n72.233\n\n\nn_obs\n49\n\n\nlrt_pval\n0.91\n\n\nlrt_chisq\n0.013\n\n\nn_factors\n2\n\n\nest_conditionF\n0.612\n\n\nest_probabitily_conditionF\n0.648\n\n\nlowerCI_conditionF\n-0.266\n\n\nhigherCI_conditionF\n1.491\n\n\npval_conditionF\n0.172\n\n\nzval_conditionF\n1.366\n\n\nest_conditionM\n0.681\n\n\nest_probabitily_conditionM\n0.664\n\n\nlowerCI_conditionM\n-0.138\n\n\nhigherCI_conditionM\n1.501\n\n\npval_conditionM\n0.103\n\n\nzval_conditionM\n1.63\n\n\n\n\n\n\n\n\n7.1.1 Manipulated data only second question\nAdditionally, we address the same three questions as above but only focus on the probability that a woman asks the second question as opposed to the entire rest of the Q&A.\n\n# select only the second question\n\ndata_analysis_tr2 &lt;- subset(data_analysis_tr, question_nr == 2)\n\ndata_analysis_tr2_hands &lt;- subset(data_analysis_tr_hands, question_nr == 2)\n\ndata_analysis_tr2_chosen &lt;- subset(data_analysis_tr_chosen, question_nr == 2)\n\n### Asking questions\nman_ask_q2_null &lt;- glmer(gender_questioner_female ~ -1  + (1|session_id/talk_id), data = data_analysis_tr2, family = binomial, offset = boot::logit(audience_women_prop))\n\nman_ask_q2 &lt;- glmer(gender_questioner_female ~ -1 + condition + (1|session_id/talk_id), data = data_analysis_tr2, family = binomial, offset = boot::logit(audience_women_prop))\n\nsummary(man_ask_q2) \n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + condition + (1 | session_id/talk_id)\n   Data: data_analysis_tr2\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   101.7    111.0    -46.9     93.7       71 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2089 -0.8164  0.4417  0.7247  1.2677 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev.\n talk_id:session_id (Intercept) 4.303e-07 0.000656\n session_id         (Intercept) 5.447e-01 0.738062\nNumber of obs: 75, groups:  talk_id:session_id, 75; session_id, 36\n\nFixed effects:\n           Estimate Std. Error z value Pr(&gt;|z|)  \nconditionF  -0.8545     0.3959  -2.158   0.0309 *\nconditionM   0.5663     0.4733   1.197   0.2315  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           cndtnF\nconditionM -0.041\n\nm_qa_man_ask_q2 &lt;- collect_out(model = man_ask_q2, null = man_ask_q2_null,  name = \"QA_mani_asking_Q2\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_man_ask_q2 %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() #same conclusion as above\n\n\n\n\nmodel_name\nQA_mani_asking_Q2\n\n\nAIC\n101.746\n\n\nn_obs\n75\n\n\nlrt_pval\n0.028\n\n\nlrt_chisq\n7.162\n\n\nn_factors\n2\n\n\nest_conditionF\n-0.855\n\n\nest_probabitily_conditionF\n0.298\n\n\nlowerCI_conditionF\n-1.631\n\n\nhigherCI_conditionF\n-0.079\n\n\npval_conditionF\n0.031\n\n\nzval_conditionF\n-2.158\n\n\nest_conditionM\n0.566\n\n\nest_probabitily_conditionM\n0.638\n\n\nlowerCI_conditionM\n-0.361\n\n\nhigherCI_conditionM\n1.494\n\n\npval_conditionM\n0.231\n\n\nzval_conditionM\n1.197\n\n\n\n\n\n\n## Raising hands\n\nman_hands_q2_null &lt;- glmer(cbind(hands_women, hands_men) ~ (1|session_id/talk_id), data = data_analysis_tr2_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nman_hands_q2 &lt;- glmer(cbind(hands_women, hands_men) ~ -1 + condition + (1|session_id/talk_id), data = data_analysis_tr2_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nsummary(man_hands_q2) \n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \ncbind(hands_women, hands_men) ~ -1 + condition + (1 | session_id/talk_id)\n   Data: data_analysis_tr2_hands\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   127.0    136.1    -59.5    119.0       67 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.92315 -0.74876  0.07772  0.88962  1.55521 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev.\n talk_id:session_id (Intercept) 1.001e-10 0.00001 \n session_id         (Intercept) 1.476e-01 0.38423 \nNumber of obs: 71, groups:  talk_id:session_id, 71; session_id, 36\n\nFixed effects:\n           Estimate Std. Error z value Pr(&gt;|z|)   \nconditionF  -0.9311     0.3103  -3.001  0.00269 **\nconditionM  -0.3193     0.2936  -1.088  0.27667   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           cndtnF\nconditionM -0.044\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_man_hands_q2 &lt;- collect_out(model = man_hands_q2, null = man_hands_q2_null,  name = \"QA_mani_hands_Q2\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_man_hands_q2 %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() #same conclusion as above\n\n\n\n\nmodel_name\nQA_mani_hands_Q2\n\n\nAIC\n127.042\n\n\nn_obs\n71\n\n\nlrt_pval\n0.137\n\n\nlrt_chisq\n2.214\n\n\nn_factors\n2\n\n\nest_conditionF\n-0.931\n\n\nest_probabitily_conditionF\n0.283\n\n\nlowerCI_conditionF\n-1.539\n\n\nhigherCI_conditionF\n-0.323\n\n\npval_conditionF\n0.003\n\n\nzval_conditionF\n-3.001\n\n\nest_conditionM\n-0.319\n\n\nest_probabitily_conditionM\n0.421\n\n\nlowerCI_conditionM\n-0.895\n\n\nhigherCI_conditionM\n0.256\n\n\npval_conditionM\n0.277\n\n\nzval_conditionM\n-1.088\n\n\n\n\n\n\n## Raising hands\n\nnrow(data_analysis_tr2_chosen) #20\n\n[1] 20\n\nman_chosen_q2_null &lt;- glmer(gender_questioner_female ~ (1|session_id/talk_id), data_analysis_tr2_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nman_chosen_q2 &lt;- glmer(gender_questioner_female ~ -1 + condition + (1|session_id/talk_id), data_analysis_tr2_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nsummary(man_chosen_q2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + condition + (1 | session_id/talk_id)\n   Data: data_analysis_tr2_chosen\n Offset: boot::logit(hands_prop_women)\n\n     AIC      BIC   logLik deviance df.resid \n    32.0     35.9    -12.0     24.0       16 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.0187 -0.9544  0.4348  0.6058  1.2771 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev. \n talk_id:session_id (Intercept) 2.711e-01 5.207e-01\n session_id         (Intercept) 9.488e-09 9.741e-05\nNumber of obs: 20, groups:  talk_id:session_id, 20; session_id, 13\n\nFixed effects:\n           Estimate Std. Error z value Pr(&gt;|z|)\nconditionF  0.03595    0.68784   0.052    0.958\nconditionM  1.62264    1.05474   1.538    0.124\n\nCorrelation of Fixed Effects:\n           cndtnF\nconditionM 0.003 \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_man_chosen_q2 &lt;- collect_out(model = man_chosen_q2, null = man_chosen_q2_null,  name = \"QA_mani_chosen_Q2\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_man_chosen_q2 %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_mani_chosen_Q2\n\n\nAIC\n31.963\n\n\nn_obs\n20\n\n\nlrt_pval\n0.127\n\n\nlrt_chisq\n2.327\n\n\nn_factors\n2\n\n\nest_conditionF\n0.036\n\n\nest_probabitily_conditionF\n0.509\n\n\nlowerCI_conditionF\n-1.312\n\n\nhigherCI_conditionF\n1.384\n\n\npval_conditionF\n0.958\n\n\nzval_conditionF\n0.052\n\n\nest_conditionM\n1.623\n\n\nest_probabitily_conditionM\n0.835\n\n\nlowerCI_conditionM\n-0.445\n\n\nhigherCI_conditionM\n3.69\n\n\npval_conditionM\n0.124\n\n\nzval_conditionM\n1.538"
  },
  {
    "objectID": "qmd/6_question_experiment.html#unmanipulated-data",
    "href": "qmd/6_question_experiment.html#unmanipulated-data",
    "title": "7  Question asking experiment",
    "section": "7.2 Unmanipulated data",
    "text": "7.2 Unmanipulated data\nWe ask the same question but using the unmanipulated data only, where session hosts were not given any instructions.\n\n### Asking questions\n\nunman_ask_null &lt;- glmer(gender_questioner_female ~ (1|session_id/talk_id), data = data_analysis_c, family = binomial, offset = boot::logit(audience_women_prop))\n\nunman_ask &lt;- glmer(gender_questioner_female ~ -1 + FIRST_questioner_gender + (1|session_id/talk_id), data = data_analysis_c, family = binomial, offset = boot::logit(audience_women_prop))\n\n# LRT\ndrop1(test=\"Chisq\",unman_ask) \n\nSingle term deletions\n\nModel:\ngender_questioner_female ~ -1 + FIRST_questioner_gender + (1 | \n    session_id/talk_id)\n                        npar    AIC    LRT Pr(Chi)  \n&lt;none&gt;                       288.13                 \nFIRST_questioner_gender    1 292.47 6.3415 0.01179 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(unman_ask)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + FIRST_questioner_gender + (1 |  \n    session_id/talk_id)\n   Data: data_analysis_c\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   288.1    301.6   -140.1    280.1      208 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5886 -0.8727 -0.6687  1.0096  1.7396 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 212, groups:  talk_id:session_id, 96; session_id, 24\n\nFixed effects:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \nFIRST_questioner_genderF  -1.0367     0.1926  -5.384  7.3e-08 ***\nFIRST_questioner_genderM  -0.3270     0.2078  -1.574    0.116    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            FIRST__F\nFIRST_qst_M 0.000   \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_unman_ask &lt;- collect_out(model = unman_ask, null = unman_ask_null,  name = \"QA_unmani_asking\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_unman_ask %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_unmani_asking\n\n\nAIC\n288.129\n\n\nn_obs\n212\n\n\nlrt_pval\n0.012\n\n\nlrt_chisq\n6.341\n\n\nn_factors\n2\n\n\nest_FIRST_questioner_genderF\n-1.037\n\n\nest_probabitily_FIRST_questioner_genderF\n0.262\n\n\nlowerCI_FIRST_questioner_genderF\n-1.414\n\n\nhigherCI_FIRST_questioner_genderF\n-0.659\n\n\npval_FIRST_questioner_genderF\n0\n\n\nzval_FIRST_questioner_genderF\n-5.384\n\n\nest_FIRST_questioner_genderM\n-0.327\n\n\nest_probabitily_FIRST_questioner_genderM\n0.419\n\n\nlowerCI_FIRST_questioner_genderM\n-0.734\n\n\nhigherCI_FIRST_questioner_genderM\n0.08\n\n\npval_FIRST_questioner_genderM\n0.116\n\n\nzval_FIRST_questioner_genderM\n-1.574\n\n\n\n\n\n\n### Raising hands\n\n# remove when no hands when were raised, only men or only women\ndata_analysis_c_hands &lt;- subset(data_analysis_c, hands_total &gt; 0 & !is.na(hands_women) & !is.na(hands_men) & \n  !is.na(hands_total))\n\nunman_hands_null &lt;- glmer(cbind(hands_women, hands_men) ~  (1|session_id/talk_id), data = data_analysis_c_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nunman_hands &lt;- glmer(cbind(hands_women, hands_men) ~ -1 + FIRST_questioner_gender + (1|session_id/talk_id), data = data_analysis_c_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nsummary(unman_hands)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(hands_women, hands_men) ~ -1 + FIRST_questioner_gender +  \n    (1 | session_id/talk_id)\n   Data: data_analysis_c_hands\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   359.7    373.0   -175.8    351.7      202 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.57510 -0.86492 -0.03985  0.80123  1.89468 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.1891   0.4349  \n session_id         (Intercept) 0.1578   0.3972  \nNumber of obs: 206, groups:  talk_id:session_id, 96; session_id, 24\n\nFixed effects:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \nFIRST_questioner_genderF  -0.9023     0.1951  -4.625 3.75e-06 ***\nFIRST_questioner_genderM  -0.3108     0.2195  -1.416    0.157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            FIRST__F\nFIRST_qst_M 0.163   \n\nm_qa_unman_hands &lt;- collect_out(model = unman_hands, null = unman_hands_null,  name = \"QA_unmani_hands\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_unman_hands %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_unmani_hands\n\n\nAIC\n359.675\n\n\nn_obs\n206\n\n\nlrt_pval\n0.027\n\n\nlrt_chisq\n4.902\n\n\nn_factors\n2\n\n\nest_FIRST_questioner_genderF\n-0.902\n\n\nest_probabitily_FIRST_questioner_genderF\n0.289\n\n\nlowerCI_FIRST_questioner_genderF\n-1.285\n\n\nhigherCI_FIRST_questioner_genderF\n-0.52\n\n\npval_FIRST_questioner_genderF\n0\n\n\nzval_FIRST_questioner_genderF\n-4.625\n\n\nest_FIRST_questioner_genderM\n-0.311\n\n\nest_probabitily_FIRST_questioner_genderM\n0.423\n\n\nlowerCI_FIRST_questioner_genderM\n-0.741\n\n\nhigherCI_FIRST_questioner_genderM\n0.12\n\n\npval_FIRST_questioner_genderM\n0.157\n\n\nzval_FIRST_questioner_genderM\n-1.416\n\n\n\n\n\n\n## Getting chosen\n\n# exclude cases where the host could not choose\ndata_analysis_c_chosen &lt;- subset(data_analysis_c, hands_prop_women &gt; 0 & hands_prop_women &lt; 1) \n\nnrow(data_analysis_c_chosen) #51\n\n[1] 51\n\nunman_chosen_null &lt;- glmer(gender_questioner_female ~ (1|session_id/talk_id), data_analysis_c_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nunman_chosen &lt;- glmer(gender_questioner_female ~ -1 + FIRST_questioner_gender + (1|session_id/talk_id), data_analysis_c_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nsummary(unman_chosen)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + FIRST_questioner_gender + (1 |  \n    session_id/talk_id)\n   Data: data_analysis_c_chosen\n Offset: boot::logit(hands_prop_women)\n\n     AIC      BIC   logLik deviance df.resid \n    76.4     84.2    -34.2     68.4       47 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6208 -0.9358 -0.5403  1.0686  1.5113 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.0e+00  0e+00   \n session_id         (Intercept) 2.5e-15  5e-08   \nNumber of obs: 51, groups:  talk_id:session_id, 37; session_id, 21\n\nFixed effects:\n                         Estimate Std. Error z value Pr(&gt;|z|)\nFIRST_questioner_genderF  -0.1327     0.3641  -0.365    0.715\nFIRST_questioner_genderM  -0.3320     0.4707  -0.705    0.481\n\nCorrelation of Fixed Effects:\n            FIRST__F\nFIRST_qst_M 0.000   \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_unman_chosen &lt;- collect_out(model = unman_chosen, null = unman_chosen_null,  name = \"QA_unmani_chosen\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_unman_chosen %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_unmani_chosen\n\n\nAIC\n76.428\n\n\nn_obs\n51\n\n\nlrt_pval\n0.738\n\n\nlrt_chisq\n0.112\n\n\nn_factors\n2\n\n\nest_FIRST_questioner_genderF\n-0.133\n\n\nest_probabitily_FIRST_questioner_genderF\n0.467\n\n\nlowerCI_FIRST_questioner_genderF\n-0.846\n\n\nhigherCI_FIRST_questioner_genderF\n0.581\n\n\npval_FIRST_questioner_genderF\n0.715\n\n\nzval_FIRST_questioner_genderF\n-0.365\n\n\nest_FIRST_questioner_genderM\n-0.332\n\n\nest_probabitily_FIRST_questioner_genderM\n0.418\n\n\nlowerCI_FIRST_questioner_genderM\n-1.254\n\n\nhigherCI_FIRST_questioner_genderM\n0.59\n\n\npval_FIRST_questioner_genderM\n0.481\n\n\nzval_FIRST_questioner_genderM\n-0.705\n\n\n\n\n\n\n\n\n7.2.1 Unmanipulated data only second question\nAdditionally, we address the same three questions as above but only focus on the probability that a woman asks the second question as opposed to the entire rest of the Q&A.\n\n# select only the second question\n\ndata_analysis_c2 &lt;- subset(data_analysis_c, question_nr == 2)\n\ndata_analysis_c2_hands &lt;- subset(data_analysis_c_hands, question_nr == 2)\n\ndata_analysis_c2_chosen &lt;- subset(data_analysis_c_chosen, question_nr == 2)\n\n### Asking questions\nunman_ask_q2_null &lt;- glmer(gender_questioner_female ~ -1  + (1|session_id/talk_id), data = data_analysis_c2, family = binomial, offset = boot::logit(audience_women_prop))\n\nunman_ask_q2 &lt;- glmer(gender_questioner_female ~ -1 + FIRST_questioner_gender + (1|session_id/talk_id), data = data_analysis_c2, family = binomial, offset = boot::logit(audience_women_prop))\n\nsummary(unman_ask_q2) \n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + FIRST_questioner_gender + (1 |  \n    session_id/talk_id)\n   Data: data_analysis_c2\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   107.1    116.4    -49.5     99.1       72 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7007 -0.8280 -0.5853  0.8800  1.8124 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.05513  0.2348  \n session_id         (Intercept) 0.00000  0.0000  \nNumber of obs: 76, groups:  talk_id:session_id, 76; session_id, 24\n\nFixed effects:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \nFIRST_questioner_genderF  -1.2957     0.3612  -3.587 0.000335 ***\nFIRST_questioner_genderM  -0.1497     0.3505  -0.427 0.669395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            FIRST__F\nFIRST_qst_M -0.056  \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_unman_ask_q2 &lt;- collect_out(model = unman_ask_q2, null = unman_ask_q2_null,  name = \"QA_unmani_asking_Q2\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_unman_ask_q2 %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() #same conclusion as above\n\n\n\n\nmodel_name\nQA_unmani_asking_Q2\n\n\nAIC\n107.051\n\n\nn_obs\n76\n\n\nlrt_pval\n0.001\n\n\nlrt_chisq\n13.323\n\n\nn_factors\n2\n\n\nest_FIRST_questioner_genderF\n-1.296\n\n\nest_probabitily_FIRST_questioner_genderF\n0.215\n\n\nlowerCI_FIRST_questioner_genderF\n-2.004\n\n\nhigherCI_FIRST_questioner_genderF\n-0.588\n\n\npval_FIRST_questioner_genderF\n0\n\n\nzval_FIRST_questioner_genderF\n-3.587\n\n\nest_FIRST_questioner_genderM\n-0.15\n\n\nest_probabitily_FIRST_questioner_genderM\n0.463\n\n\nlowerCI_FIRST_questioner_genderM\n-0.837\n\n\nhigherCI_FIRST_questioner_genderM\n0.537\n\n\npval_FIRST_questioner_genderM\n0.669\n\n\nzval_FIRST_questioner_genderM\n-0.427\n\n\n\n\n\n\n## Raising hands\n\nunman_hands_q2_null &lt;- glmer(cbind(hands_women, hands_men) ~ (1|session_id/talk_id), data = data_analysis_c2_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nunman_hands_q2 &lt;- glmer(cbind(hands_women, hands_men) ~ -1 + FIRST_questioner_gender + (1|session_id/talk_id), data = data_analysis_c2_hands, family = binomial, offset = boot::logit(audience_women_prop))\n\nsummary(unman_hands_q2) \n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(hands_women, hands_men) ~ -1 + FIRST_questioner_gender +  \n    (1 | session_id/talk_id)\n   Data: data_analysis_c2_hands\n Offset: boot::logit(audience_women_prop)\n\n     AIC      BIC   logLik deviance df.resid \n   137.8    147.1    -64.9    129.8       71 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7399 -0.9241 -0.0471  0.7580  2.6671 \n\nRandom effects:\n Groups             Name        Variance  Std.Dev. \n talk_id:session_id (Intercept) 1.091e-10 1.044e-05\n session_id         (Intercept) 1.140e-02 1.068e-01\nNumber of obs: 75, groups:  talk_id:session_id, 75; session_id, 24\n\nFixed effects:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \nFIRST_questioner_genderF  -1.0803     0.2258  -4.783 1.73e-06 ***\nFIRST_questioner_genderM  -0.1449     0.2789  -0.520    0.603    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            FIRST__F\nFIRST_qst_M 0.024   \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_unman_hands_q2 &lt;- collect_out(model = unman_hands_q2, null = unman_hands_q2_null,  name = \"QA_unmani_hands_Q2\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_unman_hands_q2 %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() #same conclusion as above\n\n\n\n\nmodel_name\nQA_unmani_hands_Q2\n\n\nAIC\n137.786\n\n\nn_obs\n75\n\n\nlrt_pval\n0.008\n\n\nlrt_chisq\n7.012\n\n\nn_factors\n2\n\n\nest_FIRST_questioner_genderF\n-1.08\n\n\nest_probabitily_FIRST_questioner_genderF\n0.253\n\n\nlowerCI_FIRST_questioner_genderF\n-1.523\n\n\nhigherCI_FIRST_questioner_genderF\n-0.638\n\n\npval_FIRST_questioner_genderF\n0\n\n\nzval_FIRST_questioner_genderF\n-4.783\n\n\nest_FIRST_questioner_genderM\n-0.145\n\n\nest_probabitily_FIRST_questioner_genderM\n0.464\n\n\nlowerCI_FIRST_questioner_genderM\n-0.692\n\n\nhigherCI_FIRST_questioner_genderM\n0.402\n\n\npval_FIRST_questioner_genderM\n0.603\n\n\nzval_FIRST_questioner_genderM\n-0.52\n\n\n\n\n\n\n## Raising hands\n\nnrow(data_analysis_c2_chosen) #26\n\n[1] 26\n\nunman_chosen_q2_null &lt;- glmer(gender_questioner_female ~ (1|session_id/talk_id), data_analysis_c2_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nunman_chosen_q2 &lt;- glmer(gender_questioner_female ~ -1 + FIRST_questioner_gender + (1|session_id/talk_id), data_analysis_c2_chosen, family = \"binomial\", offset = boot::logit(hands_prop_women))\n\nsummary(unman_chosen)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gender_questioner_female ~ -1 + FIRST_questioner_gender + (1 |  \n    session_id/talk_id)\n   Data: data_analysis_c_chosen\n Offset: boot::logit(hands_prop_women)\n\n     AIC      BIC   logLik deviance df.resid \n    76.4     84.2    -34.2     68.4       47 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6208 -0.9358 -0.5403  1.0686  1.5113 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0.0e+00  0e+00   \n session_id         (Intercept) 2.5e-15  5e-08   \nNumber of obs: 51, groups:  talk_id:session_id, 37; session_id, 21\n\nFixed effects:\n                         Estimate Std. Error z value Pr(&gt;|z|)\nFIRST_questioner_genderF  -0.1327     0.3641  -0.365    0.715\nFIRST_questioner_genderM  -0.3320     0.4707  -0.705    0.481\n\nCorrelation of Fixed Effects:\n            FIRST__F\nFIRST_qst_M 0.000   \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nm_qa_unman_chosen_q2 &lt;- collect_out(model = unman_chosen_q2, null = unman_chosen_q2_null,  name = \"QA_unmani_chosen_Q2\", n_factors = 2, type = \"exp\", save=\"yes\", dir=\"../results/question-asking/\")\n\nm_qa_unman_chosen_q2 %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nQA_unmani_chosen_Q2\n\n\nAIC\n42.294\n\n\nn_obs\n26\n\n\nlrt_pval\n0.898\n\n\nlrt_chisq\n0.016\n\n\nn_factors\n2\n\n\nest_FIRST_questioner_genderF\n-0.256\n\n\nest_probabitily_FIRST_questioner_genderF\n0.436\n\n\nlowerCI_FIRST_questioner_genderF\n-1.357\n\n\nhigherCI_FIRST_questioner_genderF\n0.846\n\n\npval_FIRST_questioner_genderF\n0.649\n\n\nzval_FIRST_questioner_genderF\n-0.455\n\n\nest_FIRST_questioner_genderM\n-0.364\n\n\nest_probabitily_FIRST_questioner_genderM\n0.41\n\n\nlowerCI_FIRST_questioner_genderM\n-1.651\n\n\nhigherCI_FIRST_questioner_genderM\n0.922\n\n\npval_FIRST_questioner_genderM\n0.579\n\n\nzval_FIRST_questioner_genderM\n-0.555"
  },
  {
    "objectID": "qmd/6_question_experiment.html#plots",
    "href": "qmd/6_question_experiment.html#plots",
    "title": "7  Question asking experiment",
    "section": "7.3 Plots",
    "text": "7.3 Plots\nLet’s try to play around with ways of showing the raw data of the effect of the gender of the first questioner on the probability that a woman asks a question in the rest of the session.\n\nprop_plot &lt;- data.frame(what = rep(c(\"Asking questions\", \"Raising hands\", \"Getting chosen\"), times=4),\n                        data = rep(c(\"Manipulated\", \"Unmanipulated\"), each = 6),\n                        gender_first = rep(c(rep(c(\"Woman\"), times=3), rep(c(\"Man\"), times=3)), times=2),\n                        est = c(m_qa_man_ask$est_conditionF,\n                                     m_qa_man_hands$est_conditionF,\n                                     m_qa_man_chosen$est_conditionF,\n                                     m_qa_man_ask$est_conditionM,\n                                     m_qa_man_hands$est_conditionM,\n                                     m_qa_man_chosen$est_conditionM,\n                                     m_qa_unman_ask$est_FIRST_questioner_genderF,\n                                     m_qa_unman_hands$est_FIRST_questioner_genderF,\n                                     m_qa_unman_chosen$est_FIRST_questioner_genderF,\n                                     m_qa_unman_ask$est_FIRST_questioner_genderM,\n                                     m_qa_unman_hands$est_FIRST_questioner_genderM,\n                                     m_qa_unman_chosen$est_FIRST_questioner_genderM),\n                        lower = c(m_qa_man_ask$lowerCI_conditionF,\n                                     m_qa_man_hands$lowerCI_conditionF,\n                                     m_qa_man_chosen$lowerCI_conditionF,\n                                     m_qa_man_ask$lowerCI_conditionM,\n                                     m_qa_man_hands$lowerCI_conditionM,\n                                     m_qa_man_chosen$lowerCI_conditionM,\n                                m_qa_unman_ask$lowerCI_FIRST_questioner_genderF,\n                                     m_qa_unman_hands$lowerCI_FIRST_questioner_genderF,\n                                     m_qa_unman_chosen$lowerCI_FIRST_questioner_genderF,\n                                     m_qa_unman_ask$lowerCI_FIRST_questioner_genderM,\n                                     m_qa_unman_hands$lowerCI_FIRST_questioner_genderM,\n                                     m_qa_unman_chosen$lowerCI_FIRST_questioner_genderM),\n                        higher = c(m_qa_man_ask$higherCI_conditionF,\n                                     m_qa_man_hands$higherCI_conditionF,\n                                     m_qa_man_chosen$higherCI_conditionF,\n                                     m_qa_man_ask$higherCI_conditionM,\n                                     m_qa_man_hands$higherCI_conditionM,\n                                     m_qa_man_chosen$higherCI_conditionM,\n                                     m_qa_unman_ask$higherCI_FIRST_questioner_genderF,\n                                     m_qa_unman_hands$higherCI_FIRST_questioner_genderF,\n                                     m_qa_unman_chosen$higherCI_FIRST_questioner_genderF,\n                                     m_qa_unman_ask$higherCI_FIRST_questioner_genderM,\n                                     m_qa_unman_hands$higherCI_FIRST_questioner_genderM,\n                                     m_qa_unman_chosen$higherCI_FIRST_questioner_genderM),\n                         pval = c(m_qa_man_ask$pval_conditionF,\n                                     m_qa_man_hands$pval_conditionF,\n                                     m_qa_man_chosen$pval_conditionF,\n                                     m_qa_man_ask$pval_conditionM,\n                                     m_qa_man_hands$pval_conditionM,\n                                     m_qa_man_chosen$pval_conditionM,\n                                     m_qa_unman_ask$pval_FIRST_questioner_genderF,\n                                     m_qa_unman_hands$pval_FIRST_questioner_genderF,\n                                     m_qa_unman_chosen$pval_FIRST_questioner_genderF,\n                                     m_qa_unman_ask$pval_FIRST_questioner_genderM,\n                                     m_qa_unman_hands$pval_FIRST_questioner_genderM,\n                                     m_qa_unman_chosen$pval_FIRST_questioner_genderM))\n\n\ncol_sig &lt;- clrs[c(10,2)] %&gt;%\n  color() %&gt;% \n  set_names(nm = c(\"sig\", \"nonsig\"))\n\nprop_plot &lt;- prop_plot %&gt;% mutate(est_prob = plogis(est),\n                                  lower_prob = plogis(lower),\n                                  higher_prob = plogis(higher),\n                                  sig = case_when(pval &lt; 0.05 ~ \"sig\", pval &gt;= 0.05 ~ \"nonsig\"))\n\n\nprop_plot$what &lt;- factor(prop_plot$what, levels = c(\"Asking questions\", \"Raising hands\", \"Getting chosen\"))\nprop_plot$data &lt;- factor(prop_plot$data, levels = c(\"Unmanipulated\", \"Manipulated\"))\n\nggplot(prop_plot) +\n  geom_point(aes(y = est_prob, x = gender_first, col = sig), size = 6) + \n  geom_segment(aes(y = lower_prob, yend = higher_prob, x = gender_first),\n               linewidth=1) +\n  geom_hline(yintercept = 0.5, col = \"red\", linetype = \"dotted\") +\n  \n  labs(y = \"Female probability and 95% CI\", x = \"Gender first questioner\") + \n  facet_grid(data~what, scales=\"free_x\")+\n  scale_color_manual(values = col_sig) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16),\n        panel.border = element_rect(color = \"black\", fill = NA, size = 1)) -&gt; plot_exp\n\nplot_exp\n\n\n\nggsave(plot_exp, file = \"../plots/main/fig4_qa_exp.png\", width=10, height=8)"
  },
  {
    "objectID": "qmd/7_congress_experience.html#discrimination-and-harassment",
    "href": "qmd/7_congress_experience.html#discrimination-and-harassment",
    "title": "8  Congress experience",
    "section": "8.1 Discrimination and harassment",
    "text": "8.1 Discrimination and harassment\nRespondents of the post-congress survey were asked if they themselves experienced discrimination and/or harassment (of any sort) at the congress and whether they reported it to the Awareness team, or if they witnessed someone else experiencing this.\nA total of 11 respondents experienced some form of discrimination or harassment, of which 2 cases were reported to the Awareness team. A total of 3 respondents witnessed somebody else receiving some form of discrimination or harassment, of which 1 case was reported to the Awareness team. Reasons of not reporting the cases to the Awareness team included that the case was unrelated to EDI issues. Even though only a few cases were reported to the Awareness team, the qualitative feedback given in the survey highlighted that the presence of this team made some people feel safe.\nDue to the low number of reports of discrimination or harassment, we cannot statistically test if some social identities experienced more discrimination/harassment than others. Nevertheless, these are the summary statistics per social identity for the answer to whether respondents experienced discrimination/harassment themselves:\n\ndis &lt;- subset(survey, discrimination == \"Yes\")\n\ntable(dis$gender, dis$lgbtq, dis$nationality_subcontinent)\n\n, ,  = West Europe\n\n            \n             No Yes\n  Female      3   3\n  Male        1   0\n  Non-binary  0   0\n\n, ,  = North Europe\n\n            \n             No Yes\n  Female      0   1\n  Male        0   0\n  Non-binary  0   0\n\n, ,  = South Europe\n\n            \n             No Yes\n  Female      0   0\n  Male        0   0\n  Non-binary  0   0\n\n, ,  = East Europe\n\n            \n             No Yes\n  Female      0   0\n  Male        0   0\n  Non-binary  0   0\n\n, ,  = North America\n\n            \n             No Yes\n  Female      1   0\n  Male        0   0\n  Non-binary  0   1\n\n, ,  = Oceania\n\n            \n             No Yes\n  Female      0   0\n  Male        0   0\n  Non-binary  0   0\n\n, ,  = South American\n\n            \n             No Yes\n  Female      0   0\n  Male        0   0\n  Non-binary  0   0\n\n, ,  = Asia\n\n            \n             No Yes\n  Female      0   0\n  Male        0   0\n  Non-binary  0   0\n\n, ,  = Dual-nationality\n\n            \n             No Yes\n  Female      0   0\n  Male        0   0\n  Non-binary  0   0\n\n\nThis shows that most reports came from women, and from West Europe."
  },
  {
    "objectID": "qmd/7_congress_experience.html#congress-experience",
    "href": "qmd/7_congress_experience.html#congress-experience",
    "title": "8  Congress experience",
    "section": "8.2 Congress experience",
    "text": "8.2 Congress experience\nIn the post-congress survey, we asked respondents to answer on a 7-point Likert scale (1: Strongly disagree, 7: Strongly agree) how much they agree with the following three statements: 1) “I felt heard during the conversations I had, both during Q&A sessions and social activities”; 2) “I felt comfortable being myself”; 3) “Attending the Behaviour 2023 congress helped me feel like I belong in my research field”. For each of the three statements, we fitted ordinal GLMs to identify which social identity variables (gender, LGBTQIA+, nationality, affiliation, expat status) were significantly associated with the Likert-scale response to the statement. Additionally, we controlled for the level of comfort a person had speaking English as well as their self-reported level of expertise. Only the variables that were significant were used in the final model.\nWe have information on three variables that roughly measure the same thing: age is indicative of career stage, career stage is indicative of expertise, and self-reported expertise rating directly measures this expertise. Since age and career stage are indicator variables of expertise, we therefore only investigate expertise rating as this will give us the signal we are interested in.\n\n8.2.1 Feeling heard\n\n# first, we explore the distribution of answers\n\n# feeling heard\nggplot(survey, aes(feeling_heard_rating)) + \n  geom_histogram(stat=\"count\", aes(y=stat(count/sum(count)*100)), fill = clrs[11]) + \n  geom_text(aes(label = paste0(\"N = \", ..count..), y = stat(count/sum(count)*100)), \n            stat=\"count\", vjust=-1) +\n  ylim(0, 100) + \n  labs(x = \"I felt heard\", y = \"% of responses\") \n\n\n\n# next: explore the relationship between age and expertise, and career stage and expertise rating\n\ncoeftest(polr(as.factor(expertise_rating) ~ age, data = survey))\n\n\nt test of coefficients:\n\n         Estimate Std. Error t value  Pr(&gt;|t|)    \nage35-50  2.02180    0.22656  8.9239 &lt; 2.2e-16 ***\nage&gt; 50   3.43812    0.38098  9.0244 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(polr(as.factor(expertise_rating) ~ career_3cat, data = survey))\n\n\nt test of coefficients:\n\n                 Estimate Std. Error t value  Pr(&gt;|t|)    \ncareer_3catMid    2.20513    0.23164  9.5196 &lt; 2.2e-16 ***\ncareer_3catLate   4.16000    0.36819 11.2985 &lt; 2.2e-16 ***\ncareer_3catOther  1.81962    0.53727  3.3868 0.0007817 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# as expected!\n\n# another check: is expertise rating affected by gender?\n\ncoeftest(polr(as.factor(expertise_rating) ~ gender, data = survey))\n\n\nt test of coefficients:\n\n                  Estimate Std. Error t value Pr(&gt;|t|)\ngenderMale        0.074149   0.203076  0.3651   0.7152\ngenderNon-binary -0.208320   0.759624 -0.2742   0.7841\n\n# answer = no\n\n# then we test the effect of each of the social identity variables\n\n# gender\nsurvey$gender &lt;- factor(survey$gender, levels = c(\"Male\", \"Female\", \"Non-binary\"))\n\nm_heard_gender_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(gender)))\n\nm_heard_gender &lt;- polr(feeling_heard_rating ~ gender,\n                   data=survey) \n\nm_heard_gender_out &lt;- collect_out(model = m_heard_gender, null = m_heard_gender_null, name = \"feeling_heard_gender\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_heard_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nfeeling_heard_gender\n\n\nAIC\n1086.637\n\n\nn_obs\n373\n\n\nlrt_pval\n0.112\n\n\nlrt_chisq\n4.378\n\n\nintercept_12\n-3.623\n\n\nintercept_23\n-3.302\n\n\nintercept_34\n-2.95\n\n\nintercept_45\n-1.997\n\n\nintercept_56\n-1.001\n\n\nintercept_67\n0.355\n\n\nn_factors\n2\n\n\nest_genderFemale\n-0.113\n\n\nlowerCI_genderFemale\n-0.525\n\n\nhigherCI_genderFemale\n0.3\n\n\nse_genderFemale\n0.21\n\n\ntval_genderFemale\n-0.537\n\n\npval_genderFemale\n0.591\n\n\nest_genderNon-binary\n-1.348\n\n\nlowerCI_genderNon-binary\n-2.576\n\n\nhigherCI_genderNon-binary\n-0.12\n\n\nse_genderNon-binary\n0.624\n\n\ntval_genderNon-binary\n-2.158\n\n\npval_genderNon-binary\n0.032\n\n\n\n\n\n\n# lgbtqia \n\nm_heard_lgbtq_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(lgbtq)))\n\nm_heard_lgbtq &lt;- polr(feeling_heard_rating ~ lgbtq,\n                   data=subset(survey, !is.na(lgbtq))) \n\nm_heard_lgbtq_out &lt;- collect_out(model = m_heard_lgbtq, null = m_heard_lgbtq_null, name = \"feeling_heard_lgbtq\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_heard_lgbtq_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # almost significant, note that the baseline is answer \"No\" to lgbtq identity\n\n\n\n\nmodel_name\nfeeling_heard_lgbtq\n\n\nAIC\n1049.062\n\n\nn_obs\n360\n\n\nlrt_pval\n0.059\n\n\nlrt_chisq\n3.567\n\n\nintercept_12\n-3.658\n\n\nintercept_23\n-3.31\n\n\nintercept_34\n-2.935\n\n\nintercept_45\n-1.968\n\n\nintercept_56\n-0.946\n\n\nintercept_67\n0.353\n\n\nn_factors\n1\n\n\nest_lgbtqYes\n-0.487\n\n\nlowerCI_lgbtqYes\n-0.992\n\n\nhigherCI_lgbtqYes\n0.018\n\n\nse_lgbtqYes\n0.257\n\n\ntval_lgbtqYes\n-1.895\n\n\npval_lgbtqYes\n0.059\n\n\n\n\n\n\n# nationality\nsurvey$nationality_continent &lt;- factor(survey$nationality_continent, levels = c(\"Europe\", \"Asia\", \"North America\", \"Oceania\", \"South America\"))\n\nm_heard_nat_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(nationality_continent)))\n\nm_heard_nat &lt;- polr(feeling_heard_rating ~ nationality_continent,\n                   data=survey) \n\nm_heard_nat_out &lt;- collect_out(model = m_heard_nat, null = m_heard_nat_null, name = \"feeling_heard_nat\", n_factors = 4, type = \"likert\", save = \"yes\", \n            dir = \"../results/survey\") \n\nm_heard_nat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # almost significant, but Asia is borderline sig with lower values\n\n\n\n\nmodel_name\nfeeling_heard_nat\n\n\nAIC\n1082.951\n\n\nn_obs\n374\n\n\nlrt_pval\n0.055\n\n\nlrt_chisq\n9.247\n\n\nintercept_12\n-3.58\n\n\nintercept_23\n-3.258\n\n\nintercept_34\n-2.901\n\n\nintercept_45\n-1.953\n\n\nintercept_56\n-0.961\n\n\nintercept_67\n0.408\n\n\nn_factors\n4\n\n\nest_nationality_continentAsia\n-0.729\n\n\nlowerCI_nationality_continentAsia\n-1.393\n\n\nhigherCI_nationality_continentAsia\n-0.065\n\n\nse_nationality_continentAsia\n0.338\n\n\ntval_nationality_continentAsia\n-2.159\n\n\npval_nationality_continentAsia\n0.032\n\n\nest_nationality_continentNorth America\n0.435\n\n\nlowerCI_nationality_continentNorth America\n-0.267\n\n\nhigherCI_nationality_continentNorth America\n1.136\n\n\nse_nationality_continentNorth America\n0.357\n\n\ntval_nationality_continentNorth America\n1.218\n\n\npval_nationality_continentNorth America\n0.224\n\n\nest_nationality_continentOceania\n-0.93\n\n\nlowerCI_nationality_continentOceania\n-2.234\n\n\nhigherCI_nationality_continentOceania\n0.374\n\n\nse_nationality_continentOceania\n0.663\n\n\ntval_nationality_continentOceania\n-1.402\n\n\npval_nationality_continentOceania\n0.162\n\n\nest_nationality_continentSouth America\n0.602\n\n\nlowerCI_nationality_continentSouth America\n-0.905\n\n\nhigherCI_nationality_continentSouth America\n2.109\n\n\nse_nationality_continentSouth America\n0.766\n\n\ntval_nationality_continentSouth America\n0.785\n\n\npval_nationality_continentSouth America\n0.433\n\n\n\n\n\n\n# affiliation\nsurvey$affiliation_continent &lt;- factor(survey$affiliation_continent, levels = c(\"Europe\", \"Asia\", \"Africa\", \"North America\", \"Oceania\", \"South America\"))\n\nm_heard_aff_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(affiliation_continent)))\n\nm_heard_aff &lt;- polr(feeling_heard_rating ~ affiliation_continent,\n                   data=subset(survey, !is.na(affiliation_continent))) \n\nm_heard_aff_out &lt;- collect_out(model = m_heard_aff, null = m_heard_aff_null, name = \"feeling_heard_aff\", n_factors = 5, type = \"likert\", save = \"yes\", dir = \"../results/survey\") \n\nm_heard_aff_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nfeeling_heard_aff\n\n\nAIC\n1094.802\n\n\nn_obs\n377\n\n\nlrt_pval\n0.122\n\n\nlrt_chisq\n8.695\n\n\nintercept_12\n-3.528\n\n\nintercept_23\n-3.206\n\n\nintercept_34\n-2.85\n\n\nintercept_45\n-1.886\n\n\nintercept_56\n-0.913\n\n\nintercept_67\n0.449\n\n\nn_factors\n5\n\n\nest_affiliation_continentAsia\n-0.667\n\n\nlowerCI_affiliation_continentAsia\n-1.686\n\n\nhigherCI_affiliation_continentAsia\n0.351\n\n\nse_affiliation_continentAsia\n0.518\n\n\ntval_affiliation_continentAsia\n-1.288\n\n\npval_affiliation_continentAsia\n0.199\n\n\nest_affiliation_continentAfrica\n15.592\n\n\nlowerCI_affiliation_continentAfrica\n15.592\n\n\nhigherCI_affiliation_continentAfrica\n15.592\n\n\nse_affiliation_continentAfrica\n0\n\n\ntval_affiliation_continentAfrica\n6117895554\n\n\npval_affiliation_continentAfrica\n0\n\n\nest_affiliation_continentNorth America\n0.438\n\n\nlowerCI_affiliation_continentNorth America\n-0.454\n\n\nhigherCI_affiliation_continentNorth America\n1.329\n\n\nse_affiliation_continentNorth America\n0.453\n\n\ntval_affiliation_continentNorth America\n0.965\n\n\npval_affiliation_continentNorth America\n0.335\n\n\nest_affiliation_continentOceania\n-0.369\n\n\nlowerCI_affiliation_continentOceania\n-1.606\n\n\nhigherCI_affiliation_continentOceania\n0.867\n\n\nse_affiliation_continentOceania\n0.629\n\n\ntval_affiliation_continentOceania\n-0.587\n\n\npval_affiliation_continentOceania\n0.557\n\n\nest_affiliation_continentSouth America\n15.592\n\n\nlowerCI_affiliation_continentSouth America\n15.592\n\n\nhigherCI_affiliation_continentSouth America\n15.592\n\n\nse_affiliation_continentSouth America\n0\n\n\ntval_affiliation_continentSouth America\n3058988690\n\n\npval_affiliation_continentSouth America\n0\n\n\n\n\n\n\n# expat\n\nsurvey$expat &lt;- factor(survey$expat, levels = c(\"No expat\", \"Expat\"))\n\nm_heard_expat_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(expat)))\n\nm_heard_expat &lt;- polr(feeling_heard_rating ~ expat,\n                   data=subset(survey, !is.na(expat))) \n\nm_heard_expat_out &lt;- collect_out(model = m_heard_expat, null = m_heard_expat_null, name = \"feeling_heard_expat\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_heard_expat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nfeeling_heard_expat\n\n\nAIC\n1077.053\n\n\nn_obs\n371\n\n\nlrt_pval\n0.197\n\n\nlrt_chisq\n1.661\n\n\nintercept_12\n-3.381\n\n\nintercept_23\n-3.06\n\n\nintercept_34\n-2.706\n\n\nintercept_45\n-1.775\n\n\nintercept_56\n-0.806\n\n\nintercept_67\n0.561\n\n\nn_factors\n1\n\n\nest_expatExpat\n0.245\n\n\nlowerCI_expatExpat\n-0.129\n\n\nhigherCI_expatExpat\n0.619\n\n\nse_expatExpat\n0.19\n\n\ntval_expatExpat\n1.287\n\n\npval_expatExpat\n0.199\n\n\n\n\n\n\n# english level \nsurvey$english_comfort_rating &lt;- as.numeric(as.character(survey$english_comfort_rating))\n\nm_heard_english_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(english_comfort_rating)))\n\nm_heard_english &lt;- polr(feeling_heard_rating ~ english_comfort_rating,\n                   data=subset(survey, !is.na(english_comfort_rating))) \n\nm_heard_english_out &lt;- collect_out(model = m_heard_english, null = m_heard_english_null, name = \"feeling_heard_english\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_heard_english_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nfeeling_heard_english\n\n\nAIC\n1098.731\n\n\nn_obs\n384\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n14.376\n\n\nintercept_12\n-1.169\n\n\nintercept_23\n-0.845\n\n\nintercept_34\n-0.48\n\n\nintercept_45\n0.5\n\n\nintercept_56\n1.517\n\n\nintercept_67\n2.903\n\n\nn_factors\n1\n\n\nest_english_comfort_rating\n0.381\n\n\nlowerCI_english_comfort_rating\n0.186\n\n\nhigherCI_english_comfort_rating\n0.575\n\n\nse_english_comfort_rating\n0.099\n\n\ntval_english_comfort_rating\n3.841\n\n\npval_english_comfort_rating\n0\n\n\n\n\n\n\n# expert rating\nsurvey$expertise_rating &lt;- as.numeric(as.character(survey$expertise_rating))\n\nm_heard_expert_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(expertise_rating)))\n\nm_heard_expert &lt;- polr(feeling_heard_rating ~ expertise_rating,\n                   data=subset(survey, !is.na(expertise_rating))) \n\nm_heard_expert_out &lt;- collect_out(model = m_heard_expert, null = m_heard_expert_null, name = \"feeling_heard_expert\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_heard_expert_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nfeeling_heard_expert\n\n\nAIC\n1091.194\n\n\nn_obs\n384\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n21.914\n\n\nintercept_12\n-2.285\n\n\nintercept_23\n-1.962\n\n\nintercept_34\n-1.605\n\n\nintercept_45\n-0.636\n\n\nintercept_56\n0.383\n\n\nintercept_67\n1.788\n\n\nn_factors\n1\n\n\nest_expertise_rating\n0.274\n\n\nlowerCI_expertise_rating\n0.158\n\n\nhigherCI_expertise_rating\n0.391\n\n\nse_expertise_rating\n0.059\n\n\ntval_expertise_rating\n4.622\n\n\npval_expertise_rating\n0\n\n\n\n\n\n\n### build final model only with significant variables\n\nm_heard_null &lt;- polr(feeling_heard_rating ~ 1, data=subset(survey, !is.na(expertise_rating) & !is.na(english_comfort_rating)))\n\nm_heard &lt;- polr(feeling_heard_rating ~english_comfort_rating + expertise_rating, data=subset(survey, !is.na(expertise_rating) & !is.na(english_comfort_rating)))\n\ndrop1(m_heard, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nfeeling_heard_rating ~ english_comfort_rating + expertise_rating\n                       Df    AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;                    1085.7                     \nenglish_comfort_rating  1 1091.2  7.4904 0.006203 ** \nexpertise_rating        1 1098.7 15.0275 0.000106 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(m_heard)\n\n\nt test of coefficients:\n\n                       Estimate Std. Error t value  Pr(&gt;|t|)    \nenglish_comfort_rating 0.283610   0.102551  2.7656 0.0059628 ** \nexpertise_rating       0.235642   0.061224  3.8488 0.0001394 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm_heard_out &lt;- collect_out(model = m_heard, null = m_heard_null, name = \"feeling_heard_final\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_heard_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\nfeeling_heard_final\n\n\nAIC\n1085.704\n\n\nn_obs\n384\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n29.404\n\n\nintercept_12\n-0.706\n\n\nintercept_23\n-0.38\n\n\nintercept_34\n-0.015\n\n\nintercept_45\n0.972\n\n\nintercept_56\n2.009\n\n\nintercept_67\n3.436\n\n\nn_factors\n2\n\n\nest_english_comfort_rating\n0.284\n\n\nlowerCI_english_comfort_rating\n0.082\n\n\nhigherCI_english_comfort_rating\n0.485\n\n\nse_english_comfort_rating\n0.103\n\n\ntval_english_comfort_rating\n2.766\n\n\npval_english_comfort_rating\n0.006\n\n\nest_expertise_rating\n0.236\n\n\nlowerCI_expertise_rating\n0.115\n\n\nhigherCI_expertise_rating\n0.356\n\n\nse_expertise_rating\n0.061\n\n\ntval_expertise_rating\n3.849\n\n\npval_expertise_rating\n0\n\n\n\n\n\n\n\nThe results indicate that none of the social identity variables (gender, lgbtq, nationality, affiliation (but this model did not converge)) affected whether a person felt heard during the congress. However, people who are more comfortable speaking English felt heard more, and people that rated their expertise in the field higher also felt heard more.\n\n\n8.2.2 Feeling comfortable being yourself\n\n# first, we explore the distribution of answers\n\nggplot(survey, aes(comfort_being_yourself_rating)) + \n  geom_histogram(stat=\"count\", aes(y=stat(count/sum(count)*100)), fill = clrs[11]) + \n  geom_text(aes(label = paste0(\"N = \", ..count..), y = stat(count/sum(count)*100)), \n            stat=\"count\", vjust=-1) +\n  ylim(0, 100) + \n  labs(x = \"I felt comfortable being myself\", y = \"% of responses\") \n\n\n\n# then we test the effect of each of the social identity variables\n\n# gender\nm_yourself_gender_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(gender)))\n\nm_yourself_gender &lt;- polr(comfort_being_yourself_rating ~ gender,\n                   data=survey) \n\nm_yourself_gender_out &lt;- collect_out(model = m_yourself_gender, null = m_yourself_gender_null, name = \"comf_yourself_gender\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_yourself_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ncomf_yourself_gender\n\n\nAIC\n982.799\n\n\nn_obs\n374\n\n\nlrt_pval\n0.001\n\n\nlrt_chisq\n13.295\n\n\nintercept_12\n-4.984\n\n\nintercept_23\n-3.945\n\n\nintercept_34\n-3.486\n\n\nintercept_45\n-2.765\n\n\nintercept_56\n-1.599\n\n\nintercept_67\n-0.289\n\n\nn_factors\n2\n\n\nest_genderFemale\n-0.464\n\n\nlowerCI_genderFemale\n-0.894\n\n\nhigherCI_genderFemale\n-0.034\n\n\nse_genderFemale\n0.219\n\n\ntval_genderFemale\n-2.123\n\n\npval_genderFemale\n0.034\n\n\nest_genderNon-binary\n-2.323\n\n\nlowerCI_genderNon-binary\n-3.628\n\n\nhigherCI_genderNon-binary\n-1.018\n\n\nse_genderNon-binary\n0.664\n\n\ntval_genderNon-binary\n-3.5\n\n\npval_genderNon-binary\n0.001\n\n\n\n\n\n\n# lgbtqia \n\nm_yourself_lgbtq_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(lgbtq)))\n\nm_yourself_lgbtq &lt;- polr(comfort_being_yourself_rating ~ lgbtq,\n                   data=subset(survey, !is.na(lgbtq))) \n\nm_yourself_lgbtq_out &lt;- collect_out(model = m_yourself_lgbtq, null = m_yourself_lgbtq_null, name = \"comf_yourself_lgbtq\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_yourself_lgbtq_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # almost significant\n\n\n\n\nmodel_name\ncomf_yourself_lgbtq\n\n\nAIC\n949.434\n\n\nn_obs\n361\n\n\nlrt_pval\n0.069\n\n\nlrt_chisq\n3.3\n\n\nintercept_12\n-4.88\n\n\nintercept_23\n-3.655\n\n\nintercept_34\n-3.166\n\n\nintercept_45\n-2.422\n\n\nintercept_56\n-1.342\n\n\nintercept_67\n-0.027\n\n\nn_factors\n1\n\n\nest_lgbtqYes\n-0.478\n\n\nlowerCI_lgbtqYes\n-0.993\n\n\nhigherCI_lgbtqYes\n0.037\n\n\nse_lgbtqYes\n0.262\n\n\ntval_lgbtqYes\n-1.826\n\n\npval_lgbtqYes\n0.069\n\n\n\n\n\n\n# nationality\nm_yourself_nat_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(nationality_continent)))\n\nm_yourself_nat &lt;- polr(comfort_being_yourself_rating ~ nationality_continent,\n                   data=survey) \n\nm_yourself_nat_out &lt;- collect_out(model = m_yourself_nat, null = m_yourself_nat_null, name = \"comf_yourself_nat\", n_factors = 4, type = \"likert\", save = \"yes\", \n            dir = \"../results/survey\") \n\nm_yourself_nat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant, but East Europe is sig with higher values\n\n\n\n\nmodel_name\ncomf_yourself_nat\n\n\nAIC\n1001.98\n\n\nn_obs\n375\n\n\nlrt_pval\n0.735\n\n\nlrt_chisq\n2.006\n\n\nintercept_12\n-4.524\n\n\nintercept_23\n-3.494\n\n\nintercept_34\n-3.044\n\n\nintercept_45\n-2.337\n\n\nintercept_56\n-1.193\n\n\nintercept_67\n0.081\n\n\nn_factors\n4\n\n\nest_nationality_continentAsia\n0.071\n\n\nlowerCI_nationality_continentAsia\n-0.574\n\n\nhigherCI_nationality_continentAsia\n0.715\n\n\nse_nationality_continentAsia\n0.328\n\n\ntval_nationality_continentAsia\n0.215\n\n\npval_nationality_continentAsia\n0.83\n\n\nest_nationality_continentNorth America\n-0.209\n\n\nlowerCI_nationality_continentNorth America\n-0.88\n\n\nhigherCI_nationality_continentNorth America\n0.463\n\n\nse_nationality_continentNorth America\n0.342\n\n\ntval_nationality_continentNorth America\n-0.611\n\n\npval_nationality_continentNorth America\n0.541\n\n\nest_nationality_continentOceania\n0.352\n\n\nlowerCI_nationality_continentOceania\n-1.124\n\n\nhigherCI_nationality_continentOceania\n1.828\n\n\nse_nationality_continentOceania\n0.751\n\n\ntval_nationality_continentOceania\n0.469\n\n\npval_nationality_continentOceania\n0.639\n\n\nest_nationality_continentSouth America\n0.91\n\n\nlowerCI_nationality_continentSouth America\n-0.748\n\n\nhigherCI_nationality_continentSouth America\n2.567\n\n\nse_nationality_continentSouth America\n0.843\n\n\ntval_nationality_continentSouth America\n1.079\n\n\npval_nationality_continentSouth America\n0.281\n\n\n\n\n\n\n# affiliation\nm_yourself_aff_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(affiliation_continent)))\n\nm_yourself_aff &lt;- polr(comfort_being_yourself_rating ~ affiliation_continent,\n                   data=subset(survey, !is.na(affiliation_continent))) \n\nm_yourself_aff_out &lt;- collect_out(model = m_yourself_aff, null = m_yourself_aff_null, name = \"comf_yourself_aff\", n_factors = 5, type = \"likert\", save = \"yes\", dir = \"../results/survey\") \n\nm_yourself_aff_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ncomf_yourself_aff\n\n\nAIC\n999.597\n\n\nn_obs\n378\n\n\nlrt_pval\n0.29\n\n\nlrt_chisq\n6.171\n\n\nintercept_12\n-4.515\n\n\nintercept_23\n-3.583\n\n\nintercept_34\n-3.095\n\n\nintercept_45\n-2.355\n\n\nintercept_56\n-1.194\n\n\nintercept_67\n0.102\n\n\nn_factors\n5\n\n\nest_affiliation_continentAsia\n0.588\n\n\nlowerCI_affiliation_continentAsia\n-0.43\n\n\nhigherCI_affiliation_continentAsia\n1.607\n\n\nse_affiliation_continentAsia\n0.518\n\n\ntval_affiliation_continentAsia\n1.136\n\n\npval_affiliation_continentAsia\n0.257\n\n\nest_affiliation_continentAfrica\n15.292\n\n\nlowerCI_affiliation_continentAfrica\n15.292\n\n\nhigherCI_affiliation_continentAfrica\n15.292\n\n\nse_affiliation_continentAfrica\n0\n\n\ntval_affiliation_continentAfrica\n5516237801\n\n\npval_affiliation_continentAfrica\n0\n\n\nest_affiliation_continentNorth America\n0.131\n\n\nlowerCI_affiliation_continentNorth America\n-0.753\n\n\nhigherCI_affiliation_continentNorth America\n1.015\n\n\nse_affiliation_continentNorth America\n0.45\n\n\ntval_affiliation_continentNorth America\n0.291\n\n\npval_affiliation_continentNorth America\n0.771\n\n\nest_affiliation_continentOceania\n-0.349\n\n\nlowerCI_affiliation_continentOceania\n-1.555\n\n\nhigherCI_affiliation_continentOceania\n0.858\n\n\nse_affiliation_continentOceania\n0.614\n\n\ntval_affiliation_continentOceania\n-0.569\n\n\npval_affiliation_continentOceania\n0.57\n\n\nest_affiliation_continentSouth America\n15.292\n\n\nlowerCI_affiliation_continentSouth America\n15.292\n\n\nhigherCI_affiliation_continentSouth America\n15.292\n\n\nse_affiliation_continentSouth America\n0\n\n\ntval_affiliation_continentSouth America\n2758136541\n\n\npval_affiliation_continentSouth America\n0\n\n\n\n\n\n\n# expat\n\nm_yourself_expat_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(expat)))\n\nm_yourself_expat &lt;- polr(comfort_being_yourself_rating ~ expat,\n                   data=subset(survey, !is.na(expat))) \n\nm_yourself_expat_out &lt;- collect_out(model = m_yourself_expat, null = m_yourself_expat_null, name = \"comf_yourself_expat\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_yourself_expat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ncomf_yourself_expat\n\n\nAIC\n986.957\n\n\nn_obs\n372\n\n\nlrt_pval\n0.945\n\n\nlrt_chisq\n0.005\n\n\nintercept_12\n-4.53\n\n\nintercept_23\n-3.595\n\n\nintercept_34\n-3.109\n\n\nintercept_45\n-2.37\n\n\nintercept_56\n-1.208\n\n\nintercept_67\n0.069\n\n\nn_factors\n1\n\n\nest_expatExpat\n-0.013\n\n\nlowerCI_expatExpat\n-0.394\n\n\nhigherCI_expatExpat\n0.368\n\n\nse_expatExpat\n0.194\n\n\ntval_expatExpat\n-0.069\n\n\npval_expatExpat\n0.945\n\n\n\n\n\n\n# english level \nsurvey$english_comfort_rating &lt;- as.numeric(as.character(survey$english_comfort_rating))\n\nm_yourself_english_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(english_comfort_rating)))\n\nm_yourself_english &lt;- polr(comfort_being_yourself_rating ~ english_comfort_rating,\n                   data=subset(survey, !is.na(english_comfort_rating))) \n\nm_yourself_english_out &lt;- collect_out(model = m_yourself_english, null = m_yourself_english_null, name = \"comf_yourself_english\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_yourself_english_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ncomf_yourself_english\n\n\nAIC\n1008.542\n\n\nn_obs\n385\n\n\nlrt_pval\n0.001\n\n\nlrt_chisq\n10.598\n\n\nintercept_12\n-2.451\n\n\nintercept_23\n-1.413\n\n\nintercept_34\n-0.95\n\n\nintercept_45\n-0.227\n\n\nintercept_56\n0.944\n\n\nintercept_67\n2.253\n\n\nn_factors\n1\n\n\nest_english_comfort_rating\n0.339\n\n\nlowerCI_english_comfort_rating\n0.135\n\n\nhigherCI_english_comfort_rating\n0.542\n\n\nse_english_comfort_rating\n0.104\n\n\ntval_english_comfort_rating\n3.269\n\n\npval_english_comfort_rating\n0.001\n\n\n\n\n\n\n# expert rating\nsurvey$expertise_rating &lt;- as.numeric(as.character(survey$expertise_rating))\n\nm_yourself_expert_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(expertise_rating)))\n\nm_yourself_expert &lt;- polr(comfort_being_yourself_rating ~ expertise_rating,\n                   data=subset(survey, !is.na(expertise_rating))) \n\nm_yourself_expert_out &lt;- collect_out(model = m_yourself_expert, null = m_yourself_expert_null, name = \"comf_yourself_expert\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_yourself_expert_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ncomf_yourself_expert\n\n\nAIC\n1001.373\n\n\nn_obs\n385\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n17.767\n\n\nintercept_12\n-3.436\n\n\nintercept_23\n-2.399\n\n\nintercept_34\n-1.942\n\n\nintercept_45\n-1.228\n\n\nintercept_56\n-0.061\n\n\nintercept_67\n1.268\n\n\nn_factors\n1\n\n\nest_expertise_rating\n0.248\n\n\nlowerCI_expertise_rating\n0.131\n\n\nhigherCI_expertise_rating\n0.365\n\n\nse_expertise_rating\n0.059\n\n\ntval_expertise_rating\n4.169\n\n\npval_expertise_rating\n0\n\n\n\n\n\n\n### build final model only with significant variables\n\nm_yourself_null &lt;- polr(comfort_being_yourself_rating ~ 1, data=subset(survey, !is.na(gender) & !is.na(expertise_rating) & !is.na(english_comfort_rating)))\n\nm_yourself &lt;- polr(comfort_being_yourself_rating ~ gender + english_comfort_rating + expertise_rating, data=subset(survey, !is.na(gender) & !is.na(expertise_rating) & !is.na(english_comfort_rating)))\n\ndrop1(m_yourself, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\ncomfort_being_yourself_rating ~ gender + english_comfort_rating + \n    expertise_rating\n                       Df    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;                    962.68                      \ngender                  2 971.35 12.6677 0.0017752 ** \nenglish_comfort_rating  1 967.20  6.5161 0.0106902 *  \nexpertise_rating        1 973.32 12.6387 0.0003778 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm_yourself_out &lt;- collect_out(model = m_yourself, null = m_yourself_null, name = \"comf_yourself_final\", n_factors = 4, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_yourself_out %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\nAIC\nn_obs\nlrt_pval\nlrt_chisq\nintercept_12\nintercept_23\nintercept_34\nintercept_45\nintercept_56\nintercept_67\nn_factors\nest_genderFemale\nlowerCI_genderFemale\nhigherCI_genderFemale\nse_genderFemale\ntval_genderFemale\npval_genderFemale\nest_genderNon-binary\nlowerCI_genderNon-binary\nhigherCI_genderNon-binary\nse_genderNon-binary\ntval_genderNon-binary\npval_genderNon-binary\nest_english_comfort_rating\nlowerCI_english_comfort_rating\nhigherCI_english_comfort_rating\nse_english_comfort_rating\ntval_english_comfort_rating\npval_english_comfort_rating\nest_expertise_rating\nlowerCI_expertise_rating\nhigherCI_expertise_rating\nse_expertise_rating\ntval_expertise_rating\npval_expertise_rating\n\n\n\n\ncomf_yourself_final\n962.684\n374\n0\n37.41\n-2.277\n-1.218\n-0.74\n0.002\n1.2\n2.572\n4\n-0.475\n-0.911\n-0.039\n0.222\n-2.141\n0.033\n-2.261\n-3.588\n-0.934\n0.675\n-3.351\n0.001\n0.281\n0.067\n0.496\n0.109\n2.579\n0.01\n0.218\n0.097\n0.339\n0.062\n3.534\n0\n\n\n\n\n\n\n\nThe results indicate that lgbtq and nationality did not affect whether a person felt comfortable being themselves during the congress (but lgbtq came close to significance). However, men feel more comfortable than women and non-binary people feel less comfortable being themselves compared to women. People who are more comfortable speaking English felt more comfortable being themselves, and people that rated their expertise in the field higher also felt more comfortable being themselves.\n\n\n8.2.3 Sense of belonging\n\n# first, we explore the distribution of answers\n\nggplot(survey, aes(sense_of_belonging_rating)) + \n  geom_histogram(stat=\"count\", aes(y=stat(count/sum(count)*100)), fill = clrs[11]) + \n  geom_text(aes(label = paste0(\"N = \", ..count..), y = stat(count/sum(count)*100)), \n            stat=\"count\", vjust=-1) +\n  ylim(0, 100) + \n  labs(x = \"Attending the Behaviour 2023 congress helped me \nfeel like I belong in my research field\", y = \"% of responses\") \n\n\n\n# then we test the effect of each of the social identity variables\n\n# gender\nm_sob_gender_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(gender)))\n\nm_sob_gender &lt;- polr(sense_of_belonging_rating ~ gender,\n                   data=survey) \n\nm_sob_gender_out &lt;- collect_out(model = m_sob_gender, null = m_sob_gender_null, name = \"belonging_gender\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_sob_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nbelonging_gender\n\n\nAIC\n1137.815\n\n\nn_obs\n375\n\n\nlrt_pval\n0.107\n\n\nlrt_chisq\n4.475\n\n\nintercept_12\n-4.216\n\n\nintercept_23\n-3.662\n\n\nintercept_34\n-2.89\n\n\nintercept_45\n-1.684\n\n\nintercept_56\n-0.84\n\n\nintercept_67\n0.072\n\n\nn_factors\n2\n\n\nest_genderFemale\n-0.316\n\n\nlowerCI_genderFemale\n-0.724\n\n\nhigherCI_genderFemale\n0.092\n\n\nse_genderFemale\n0.207\n\n\ntval_genderFemale\n-1.525\n\n\npval_genderFemale\n0.128\n\n\nest_genderNon-binary\n-1.181\n\n\nlowerCI_genderNon-binary\n-2.505\n\n\nhigherCI_genderNon-binary\n0.144\n\n\nse_genderNon-binary\n0.674\n\n\ntval_genderNon-binary\n-1.753\n\n\npval_genderNon-binary\n0.081\n\n\n\n\n\n\n# lgbtqia \n\nm_sob_lgbtq_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(lgbtq)))\n\nm_sob_lgbtq &lt;- polr(sense_of_belonging_rating ~ lgbtq,\n                   data=subset(survey, !is.na(lgbtq))) \n\nm_sob_lgbtq_out &lt;- collect_out(model = m_sob_lgbtq, null = m_sob_lgbtq_null, name = \"belonging_lgbtq\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_sob_lgbtq_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nbelonging_lgbtq\n\n\nAIC\n1100.466\n\n\nn_obs\n362\n\n\nlrt_pval\n0.365\n\n\nlrt_chisq\n0.82\n\n\nintercept_12\n-3.969\n\n\nintercept_23\n-3.505\n\n\nintercept_34\n-2.687\n\n\nintercept_45\n-1.469\n\n\nintercept_56\n-0.618\n\n\nintercept_67\n0.274\n\n\nn_factors\n1\n\n\nest_lgbtqYes\n-0.23\n\n\nlowerCI_lgbtqYes\n-0.729\n\n\nhigherCI_lgbtqYes\n0.269\n\n\nse_lgbtqYes\n0.254\n\n\ntval_lgbtqYes\n-0.908\n\n\npval_lgbtqYes\n0.365\n\n\n\n\n\n\n# nationality\nm_sob_nat_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(nationality_continent)))\n\nm_sob_nat &lt;- polr(sense_of_belonging_rating ~ nationality_continent,\n                   data=survey) \n\nm_sob_nat_out &lt;- collect_out(model = m_sob_nat, null = m_sob_nat_null, name = \"belonging_nat\", n_factors = 4, type = \"likert\", save = \"yes\", \n            dir = \"../results/survey\") \n\nm_sob_nat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant, but North America is borderline sig with higher values\n\n\n\n\nmodel_name\nbelonging_nat\n\n\nAIC\n1137.541\n\n\nn_obs\n376\n\n\nlrt_pval\n0.112\n\n\nlrt_chisq\n7.504\n\n\nintercept_12\n-3.895\n\n\nintercept_23\n-3.343\n\n\nintercept_34\n-2.532\n\n\nintercept_45\n-1.366\n\n\nintercept_56\n-0.567\n\n\nintercept_67\n0.377\n\n\nn_factors\n4\n\n\nest_nationality_continentAsia\n0.314\n\n\nlowerCI_nationality_continentAsia\n-0.357\n\n\nhigherCI_nationality_continentAsia\n0.985\n\n\nse_nationality_continentAsia\n0.341\n\n\ntval_nationality_continentAsia\n0.919\n\n\npval_nationality_continentAsia\n0.358\n\n\nest_nationality_continentNorth America\n0.647\n\n\nlowerCI_nationality_continentNorth America\n-0.012\n\n\nhigherCI_nationality_continentNorth America\n1.306\n\n\nse_nationality_continentNorth America\n0.335\n\n\ntval_nationality_continentNorth America\n1.93\n\n\npval_nationality_continentNorth America\n0.054\n\n\nest_nationality_continentOceania\n-0.645\n\n\nlowerCI_nationality_continentOceania\n-1.905\n\n\nhigherCI_nationality_continentOceania\n0.616\n\n\nse_nationality_continentOceania\n0.641\n\n\ntval_nationality_continentOceania\n-1.006\n\n\npval_nationality_continentOceania\n0.315\n\n\nest_nationality_continentSouth America\n1.159\n\n\nlowerCI_nationality_continentSouth America\n-0.517\n\n\nhigherCI_nationality_continentSouth America\n2.834\n\n\nse_nationality_continentSouth America\n0.852\n\n\ntval_nationality_continentSouth America\n1.36\n\n\npval_nationality_continentSouth America\n0.175\n\n\n\n\n\n\n# affiliation\nm_sob_aff_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(affiliation_continent)))\n\nm_sob_aff &lt;- polr(sense_of_belonging_rating ~ affiliation_continent,\n                   data=subset(survey, !is.na(affiliation_continent))) \n\nm_sob_aff_out &lt;- collect_out(model = m_sob_aff, null = m_sob_aff_null, name = \"belonging_aff\", n_factors = 5, type = \"likert\", save = \"yes\", \n             dir = \"../results/survey\") \n\nm_sob_aff_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nbelonging_aff\n\n\nAIC\n1141.766\n\n\nn_obs\n379\n\n\nlrt_pval\n0.013\n\n\nlrt_chisq\n14.457\n\n\nintercept_12\n-3.899\n\n\nintercept_23\n-3.346\n\n\nintercept_34\n-2.533\n\n\nintercept_45\n-1.381\n\n\nintercept_56\n-0.551\n\n\nintercept_67\n0.415\n\n\nn_factors\n5\n\n\nest_affiliation_continentAsia\n0.734\n\n\nlowerCI_affiliation_continentAsia\n-0.227\n\n\nhigherCI_affiliation_continentAsia\n1.694\n\n\nse_affiliation_continentAsia\n0.488\n\n\ntval_affiliation_continentAsia\n1.502\n\n\npval_affiliation_continentAsia\n0.134\n\n\nest_affiliation_continentAfrica\n-0.063\n\n\nlowerCI_affiliation_continentAfrica\n-2.932\n\n\nhigherCI_affiliation_continentAfrica\n2.806\n\n\nse_affiliation_continentAfrica\n1.459\n\n\ntval_affiliation_continentAfrica\n-0.043\n\n\npval_affiliation_continentAfrica\n0.966\n\n\nest_affiliation_continentNorth America\n1.439\n\n\nlowerCI_affiliation_continentNorth America\n0.409\n\n\nhigherCI_affiliation_continentNorth America\n2.469\n\n\nse_affiliation_continentNorth America\n0.524\n\n\ntval_affiliation_continentNorth America\n2.747\n\n\npval_affiliation_continentNorth America\n0.006\n\n\nest_affiliation_continentOceania\n0.214\n\n\nlowerCI_affiliation_continentOceania\n-0.827\n\n\nhigherCI_affiliation_continentOceania\n1.255\n\n\nse_affiliation_continentOceania\n0.529\n\n\ntval_affiliation_continentOceania\n0.405\n\n\npval_affiliation_continentOceania\n0.686\n\n\nest_affiliation_continentSouth America\n16.065\n\n\nlowerCI_affiliation_continentSouth America\n16.065\n\n\nhigherCI_affiliation_continentSouth America\n16.065\n\n\nse_affiliation_continentSouth America\n0\n\n\ntval_affiliation_continentSouth America\n2685232830\n\n\npval_affiliation_continentSouth America\n0\n\n\n\n\n\n\n# expat\n\nm_sob_expat_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(expat)))\n\nm_sob_expat &lt;- polr(sense_of_belonging_rating ~ expat,\n                   data=subset(survey, !is.na(expat))) \n\nm_sob_expat_out &lt;- collect_out(model = m_sob_expat, null = m_sob_expat_null, name = \"belonging_expat\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_sob_expat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nbelonging_expat\n\n\nAIC\n1130.934\n\n\nn_obs\n373\n\n\nlrt_pval\n0.471\n\n\nlrt_chisq\n0.519\n\n\nintercept_12\n-4.023\n\n\nintercept_23\n-3.47\n\n\nintercept_34\n-2.657\n\n\nintercept_45\n-1.514\n\n\nintercept_56\n-0.714\n\n\nintercept_67\n0.223\n\n\nn_factors\n1\n\n\nest_expatExpat\n-0.136\n\n\nlowerCI_expatExpat\n-0.508\n\n\nhigherCI_expatExpat\n0.236\n\n\nse_expatExpat\n0.189\n\n\ntval_expatExpat\n-0.72\n\n\npval_expatExpat\n0.472\n\n\n\n\n\n\n# english level \nsurvey$english_comfort_rating &lt;- as.numeric(as.character(survey$english_comfort_rating))\n\nm_sob_english_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(english_comfort_rating)))\n\nm_sob_english &lt;- polr(sense_of_belonging_rating ~ english_comfort_rating,\n                   data=subset(survey, !is.na(english_comfort_rating))) \n\nm_sob_english_out &lt;- collect_out(model = m_sob_english, null = m_sob_english_null, name = \"belonging_english\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_sob_english_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nbelonging_english\n\n\nAIC\n1151.747\n\n\nn_obs\n386\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n19.426\n\n\nintercept_12\n-1.316\n\n\nintercept_23\n-0.759\n\n\nintercept_34\n0.063\n\n\nintercept_45\n1.27\n\n\nintercept_56\n2.144\n\n\nintercept_67\n3.105\n\n\nn_factors\n1\n\n\nest_english_comfort_rating\n0.433\n\n\nlowerCI_english_comfort_rating\n0.241\n\n\nhigherCI_english_comfort_rating\n0.626\n\n\nse_english_comfort_rating\n0.098\n\n\ntval_english_comfort_rating\n4.421\n\n\npval_english_comfort_rating\n0\n\n\n\n\n\n\n# expert rating\nsurvey$expertise_rating &lt;- as.numeric(as.character(survey$expertise_rating))\n\nm_sob_expert_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(expertise_rating)))\n\nm_sob_expert &lt;- polr(sense_of_belonging_rating ~ expertise_rating,\n                   data=subset(survey, !is.na(expertise_rating))) \n\nm_sob_expert_out &lt;- collect_out(model = m_sob_expert, null = m_sob_expert_null, name = \"belonging_expert\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_sob_expert_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nbelonging_expert\n\n\nAIC\n1124.873\n\n\nn_obs\n386\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n46.3\n\n\nintercept_12\n-2.269\n\n\nintercept_23\n-1.71\n\n\nintercept_34\n-0.886\n\n\nintercept_45\n0.339\n\n\nintercept_56\n1.257\n\n\nintercept_67\n2.281\n\n\nn_factors\n1\n\n\nest_expertise_rating\n0.4\n\n\nlowerCI_expertise_rating\n0.282\n\n\nhigherCI_expertise_rating\n0.518\n\n\nse_expertise_rating\n0.06\n\n\ntval_expertise_rating\n6.661\n\n\npval_expertise_rating\n0\n\n\n\n\n\n\n### build final model only with significant variables\n\nm_sob_null &lt;- polr(sense_of_belonging_rating ~ 1, data=subset(survey, !is.na(expertise_rating) & !is.na(english_comfort_rating) & !is.na(affiliation_continent)))\n\nm_sob &lt;- polr(sense_of_belonging_rating ~ affiliation_continent + english_comfort_rating + expertise_rating, data=subset(survey, !is.na(expertise_rating) & !is.na(english_comfort_rating)& !is.na(affiliation_continent)))\n\ndrop1(m_sob, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nsense_of_belonging_rating ~ affiliation_continent + english_comfort_rating + \n    expertise_rating\n                       Df    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;                    1097.0                     \naffiliation_continent   5 1098.7 11.733  0.038629 *  \nenglish_comfort_rating  1 1104.0  9.022  0.002667 ** \nexpertise_rating        1 1126.7 31.693 1.806e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(m_sob)\n\n\nt test of coefficients:\n\n                                      Estimate  Std. Error     t value\naffiliation_continentAsia           8.8114e-01  5.1664e-01  1.7055e+00\naffiliation_continentAfrica        -1.0164e+00  1.4763e+00 -6.8850e-01\naffiliation_continentNorth America  1.1562e+00  5.2882e-01  2.1864e+00\naffiliation_continentOceania        6.4092e-02  5.2423e-01  1.2230e-01\naffiliation_continentSouth America  1.5758e+01  2.7271e-08  5.7783e+08\nenglish_comfort_rating              3.1388e-01  1.0360e-01  3.0298e+00\nexpertise_rating                    3.5048e-01  6.3142e-02  5.5507e+00\n                                    Pr(&gt;|t|)    \naffiliation_continentAsia           0.088949 .  \naffiliation_continentAfrica         0.491597    \naffiliation_continentNorth America  0.029421 *  \naffiliation_continentOceania        0.902761    \naffiliation_continentSouth America &lt; 2.2e-16 ***\nenglish_comfort_rating              0.002621 ** \nexpertise_rating                   5.472e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm_sob_out &lt;- collect_out(model = m_sob, null = m_sob_null, name = \"belonging_final\", n_factors = 7, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_sob_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\nbelonging_final\n\n\nAIC\n1096.961\n\n\nn_obs\n379\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n63.263\n\n\nintercept_12\n-0.446\n\n\nintercept_23\n0.118\n\n\nintercept_34\n0.95\n\n\nintercept_45\n2.172\n\n\nintercept_56\n3.096\n\n\nintercept_67\n4.171\n\n\nn_factors\n7\n\n\nest_affiliation_continentAsia\n0.881\n\n\nlowerCI_affiliation_continentAsia\n-0.135\n\n\nhigherCI_affiliation_continentAsia\n1.897\n\n\nse_affiliation_continentAsia\n0.517\n\n\ntval_affiliation_continentAsia\n1.706\n\n\npval_affiliation_continentAsia\n0.089\n\n\nest_affiliation_continentAfrica\n-1.016\n\n\nlowerCI_affiliation_continentAfrica\n-3.919\n\n\nhigherCI_affiliation_continentAfrica\n1.887\n\n\nse_affiliation_continentAfrica\n1.476\n\n\ntval_affiliation_continentAfrica\n-0.688\n\n\npval_affiliation_continentAfrica\n0.492\n\n\nest_affiliation_continentNorth America\n1.156\n\n\nlowerCI_affiliation_continentNorth America\n0.116\n\n\nhigherCI_affiliation_continentNorth America\n2.196\n\n\nse_affiliation_continentNorth America\n0.529\n\n\ntval_affiliation_continentNorth America\n2.186\n\n\npval_affiliation_continentNorth America\n0.029\n\n\nest_affiliation_continentOceania\n0.064\n\n\nlowerCI_affiliation_continentOceania\n-0.967\n\n\nhigherCI_affiliation_continentOceania\n1.095\n\n\nse_affiliation_continentOceania\n0.524\n\n\ntval_affiliation_continentOceania\n0.122\n\n\npval_affiliation_continentOceania\n0.903\n\n\nest_affiliation_continentSouth America\n15.758\n\n\nlowerCI_affiliation_continentSouth America\n15.758\n\n\nhigherCI_affiliation_continentSouth America\n15.758\n\n\nse_affiliation_continentSouth America\n0\n\n\ntval_affiliation_continentSouth America\n577826929\n\n\npval_affiliation_continentSouth America\n0\n\n\nest_english_comfort_rating\n0.314\n\n\nlowerCI_english_comfort_rating\n0.11\n\n\nhigherCI_english_comfort_rating\n0.518\n\n\nse_english_comfort_rating\n0.104\n\n\ntval_english_comfort_rating\n3.03\n\n\npval_english_comfort_rating\n0.003\n\n\nest_expertise_rating\n0.35\n\n\nlowerCI_expertise_rating\n0.226\n\n\nhigherCI_expertise_rating\n0.475\n\n\nse_expertise_rating\n0.063\n\n\ntval_expertise_rating\n5.551\n\n\npval_expertise_rating\n0\n\n\n\n\n\n\n\nThe results are very similar to the “Feeling heard” part: none of the social identity variables (gender, lgbtq, nationality) affected whether a person felt that attending the congress increased their Sense of Belonging. However, people who are more comfortable speaking English felt more like it did, and people that rated their expertise in the field higher also felt like attending the congress increased their feeling like they belong in the field."
  },
  {
    "objectID": "qmd/8_perception_issues.html#diversity-representation",
    "href": "qmd/8_perception_issues.html#diversity-representation",
    "title": "9  Perception of EDI issues",
    "section": "9.1 Diversity representation",
    "text": "9.1 Diversity representation\n\n# first, we explore the distribution of answers\n\n# diversity\nggplot(survey, aes(assess_diversity_congress_rating)) + \n  geom_histogram(stat=\"count\", aes(y=stat(count/sum(count)*100)), fill = clrs[11]) + \n  geom_text(aes(label = paste0(\"N = \", ..count..), y = stat(count/sum(count)*100)), \n            stat=\"count\", vjust=-1) +\n  labs(x = \"I think the Congress attendees represented the \ndiversity of researchers in our field\", y = \"% of responses\") \n\n\n\n# then we test the effect of each of the social identity variables\n\n# gender\nsurvey$gender &lt;- factor(survey$gender, levels = c(\"Male\", \"Female\", \"Non-binary\"))\n\nm_diversity_gender_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(gender)))\n\nm_diversity_gender &lt;- polr(assess_diversity_congress_rating ~ gender,\n                   data=survey) \n\nm_diversity_gender_out &lt;- collect_out(model = m_diversity_gender, null = m_diversity_gender_null, name = \"diversity_gender\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_diversity_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ndiversity_gender\n\n\nAIC\n1274.876\n\n\nn_obs\n373\n\n\nlrt_pval\n0.011\n\n\nlrt_chisq\n9.049\n\n\nintercept_12\n-4.517\n\n\nintercept_23\n-3.167\n\n\nintercept_34\n-2.192\n\n\nintercept_45\n-1.194\n\n\nintercept_56\n-0.19\n\n\nintercept_67\n1.326\n\n\nn_factors\n2\n\n\nest_genderFemale\n-0.506\n\n\nlowerCI_genderFemale\n-0.905\n\n\nhigherCI_genderFemale\n-0.108\n\n\nse_genderFemale\n0.203\n\n\ntval_genderFemale\n-2.498\n\n\npval_genderFemale\n0.013\n\n\nest_genderNon-binary\n-1.379\n\n\nlowerCI_genderNon-binary\n-2.632\n\n\nhigherCI_genderNon-binary\n-0.127\n\n\nse_genderNon-binary\n0.637\n\n\ntval_genderNon-binary\n-2.165\n\n\npval_genderNon-binary\n0.031\n\n\n\n\n\n\n# lgbtqia \n\nm_diversity_lgbtq_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(lgbtq)))\n\nm_diversity_lgbtq &lt;- polr(assess_diversity_congress_rating ~ lgbtq,\n                   data=subset(survey, !is.na(lgbtq))) \n\nm_diversity_lgbtq_out &lt;- collect_out(model = m_diversity_lgbtq, null = m_diversity_lgbtq_null, name = \"diversity_lgbtq\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_diversity_lgbtq_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ndiversity_lgbtq\n\n\nAIC\n1239.239\n\n\nn_obs\n360\n\n\nlrt_pval\n0.008\n\n\nlrt_chisq\n6.947\n\n\nintercept_12\n-4.219\n\n\nintercept_23\n-2.866\n\n\nintercept_34\n-1.881\n\n\nintercept_45\n-0.875\n\n\nintercept_56\n0.115\n\n\nintercept_67\n1.58\n\n\nn_factors\n1\n\n\nest_lgbtqYes\n-0.664\n\n\nlowerCI_lgbtqYes\n-1.161\n\n\nhigherCI_lgbtqYes\n-0.167\n\n\nse_lgbtqYes\n0.253\n\n\ntval_lgbtqYes\n-2.629\n\n\npval_lgbtqYes\n0.009\n\n\n\n\n\n\n# nationality\nsurvey$nationality_continent &lt;- factor(survey$nationality_continent, levels = c(\"Europe\", \"Asia\", \"North America\", \"Oceania\", \"South America\"))\n\nm_diversity_nat_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(nationality_continent)))\n\nm_diversity_nat &lt;- polr(assess_diversity_congress_rating ~ nationality_continent,\n                   data=survey) \n\nm_diversity_nat_out &lt;- collect_out(model = m_diversity_nat, null = m_diversity_nat_null, name = \"diversity_nat\", n_factors = 4, type = \"likert\", save = \"yes\", \n            dir = \"../results/survey\") \n\nm_diversity_nat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant, but Asia is borderline sig with higher values\n\n\n\n\nmodel_name\ndiversity_nat\n\n\nAIC\n1281.564\n\n\nn_obs\n374\n\n\nlrt_pval\n0.403\n\n\nlrt_chisq\n4.021\n\n\nintercept_12\n-4.093\n\n\nintercept_23\n-2.8\n\n\nintercept_34\n-1.825\n\n\nintercept_45\n-0.789\n\n\nintercept_56\n0.187\n\n\nintercept_67\n1.718\n\n\nn_factors\n4\n\n\nest_nationality_continentAsia\n0.593\n\n\nlowerCI_nationality_continentAsia\n-0.081\n\n\nhigherCI_nationality_continentAsia\n1.267\n\n\nse_nationality_continentAsia\n0.343\n\n\ntval_nationality_continentAsia\n1.73\n\n\npval_nationality_continentAsia\n0.084\n\n\nest_nationality_continentNorth America\n-0.263\n\n\nlowerCI_nationality_continentNorth America\n-0.907\n\n\nhigherCI_nationality_continentNorth America\n0.381\n\n\nse_nationality_continentNorth America\n0.327\n\n\ntval_nationality_continentNorth America\n-0.803\n\n\npval_nationality_continentNorth America\n0.422\n\n\nest_nationality_continentOceania\n-0.016\n\n\nlowerCI_nationality_continentOceania\n-1.232\n\n\nhigherCI_nationality_continentOceania\n1.2\n\n\nse_nationality_continentOceania\n0.618\n\n\ntval_nationality_continentOceania\n-0.026\n\n\npval_nationality_continentOceania\n0.979\n\n\nest_nationality_continentSouth America\n0.192\n\n\nlowerCI_nationality_continentSouth America\n-1.124\n\n\nhigherCI_nationality_continentSouth America\n1.508\n\n\nse_nationality_continentSouth America\n0.669\n\n\ntval_nationality_continentSouth America\n0.287\n\n\npval_nationality_continentSouth America\n0.774\n\n\n\n\n\n\n# affiliation\nsurvey$affiliation_continent &lt;- factor(survey$affiliation_continent, levels = c(\"Europe\", \"Asia\", \"Africa\", \"North America\", \"Oceania\", \"South America\"))\n\nm_diversity_aff_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(affiliation_continent)))\n\nm_diversity_aff &lt;- polr(assess_diversity_congress_rating ~ affiliation_continent,\n                   data=subset(survey, !is.na(affiliation_continent))) \n\nm_diversity_aff_out &lt;- collect_out(model = m_diversity_aff, null = m_diversity_aff_null, name = \"diversity_aff\", n_factors = 5, type = \"likert\", save = \"yes\", dir = \"../results/survey\") \n\nm_diversity_aff_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ndiversity_aff\n\n\nAIC\n1296.609\n\n\nn_obs\n377\n\n\nlrt_pval\n0.555\n\n\nlrt_chisq\n3.96\n\n\nintercept_12\n-4.128\n\n\nintercept_23\n-2.783\n\n\nintercept_34\n-1.81\n\n\nintercept_45\n-0.79\n\n\nintercept_56\n0.192\n\n\nintercept_67\n1.728\n\n\nn_factors\n5\n\n\nest_affiliation_continentAsia\n0.016\n\n\nlowerCI_affiliation_continentAsia\n-0.936\n\n\nhigherCI_affiliation_continentAsia\n0.967\n\n\nse_affiliation_continentAsia\n0.484\n\n\ntval_affiliation_continentAsia\n0.033\n\n\npval_affiliation_continentAsia\n0.974\n\n\nest_affiliation_continentAfrica\n-1.301\n\n\nlowerCI_affiliation_continentAfrica\n-4.183\n\n\nhigherCI_affiliation_continentAfrica\n1.581\n\n\nse_affiliation_continentAfrica\n1.465\n\n\ntval_affiliation_continentAfrica\n-0.888\n\n\npval_affiliation_continentAfrica\n0.375\n\n\nest_affiliation_continentNorth America\n-0.253\n\n\nlowerCI_affiliation_continentNorth America\n-1.179\n\n\nhigherCI_affiliation_continentNorth America\n0.673\n\n\nse_affiliation_continentNorth America\n0.471\n\n\ntval_affiliation_continentNorth America\n-0.537\n\n\npval_affiliation_continentNorth America\n0.591\n\n\nest_affiliation_continentOceania\n0.894\n\n\nlowerCI_affiliation_continentOceania\n-0.156\n\n\nhigherCI_affiliation_continentOceania\n1.945\n\n\nse_affiliation_continentOceania\n0.534\n\n\ntval_affiliation_continentOceania\n1.674\n\n\npval_affiliation_continentOceania\n0.095\n\n\nest_affiliation_continentSouth America\n0.163\n\n\nlowerCI_affiliation_continentSouth America\n-3.45\n\n\nhigherCI_affiliation_continentSouth America\n3.775\n\n\nse_affiliation_continentSouth America\n1.837\n\n\ntval_affiliation_continentSouth America\n0.088\n\n\npval_affiliation_continentSouth America\n0.93\n\n\n\n\n\n\n# expat\nsurvey$expat &lt;- factor(levels=c(\"No expat\", \"Expat\"), survey$expat)\n\nm_diversity_expat_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(expat)))\n\nm_diversity_expat &lt;- polr(assess_diversity_congress_rating ~ expat,\n                   data=subset(survey, !is.na(expat))) \n\nm_diversity_expat_out &lt;- collect_out(model = m_diversity_expat, null = m_diversity_expat_null, name = \"diversity_expat\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_diversity_expat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ndiversity_expat\n\n\nAIC\n1268.707\n\n\nn_obs\n371\n\n\nlrt_pval\n0.252\n\n\nlrt_chisq\n1.312\n\n\nintercept_12\n-4.213\n\n\nintercept_23\n-2.918\n\n\nintercept_34\n-1.942\n\n\nintercept_45\n-0.906\n\n\nintercept_56\n0.053\n\n\nintercept_67\n1.587\n\n\nn_factors\n1\n\n\nest_expatExpat\n-0.212\n\n\nlowerCI_expatExpat\n-0.575\n\n\nhigherCI_expatExpat\n0.152\n\n\nse_expatExpat\n0.185\n\n\ntval_expatExpat\n-1.145\n\n\npval_expatExpat\n0.253\n\n\n\n\n\n\n# english level \nsurvey$english_comfort_rating &lt;- as.numeric(as.character(survey$english_comfort_rating))\n\nm_diversity_english_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(english_comfort_rating)))\n\nm_diversity_english &lt;- polr(assess_diversity_congress_rating ~ english_comfort_rating,\n                   data=subset(survey, !is.na(english_comfort_rating))) \n\nm_diversity_english_out &lt;- collect_out(model = m_diversity_english, null = m_diversity_english_null, name = \"diversity_english\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_diversity_english_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ndiversity_english\n\n\nAIC\n1313.893\n\n\nn_obs\n384\n\n\nlrt_pval\n0.381\n\n\nlrt_chisq\n0.767\n\n\nintercept_12\n-4.679\n\n\nintercept_23\n-3.336\n\n\nintercept_34\n-2.365\n\n\nintercept_45\n-1.343\n\n\nintercept_56\n-0.353\n\n\nintercept_67\n1.155\n\n\nn_factors\n1\n\n\nest_english_comfort_rating\n-0.083\n\n\nlowerCI_english_comfort_rating\n-0.269\n\n\nhigherCI_english_comfort_rating\n0.104\n\n\nse_english_comfort_rating\n0.095\n\n\ntval_english_comfort_rating\n-0.873\n\n\npval_english_comfort_rating\n0.383\n\n\n\n\n\n\n# age\n\nm_diversity_age_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(age)))\n\nm_diversity_age &lt;- polr(assess_diversity_congress_rating ~ age,\n                   data=subset(survey, !is.na(age))) \n\nm_diversity_age_out &lt;- collect_out(model = m_diversity_age, null = m_diversity_age_null, name = \"diversity_age\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_diversity_age_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ndiversity_age\n\n\nAIC\n1315.435\n\n\nn_obs\n384\n\n\nlrt_pval\n0.542\n\n\nlrt_chisq\n1.225\n\n\nintercept_12\n-4.133\n\n\nintercept_23\n-2.789\n\n\nintercept_34\n-1.819\n\n\nintercept_45\n-0.799\n\n\nintercept_56\n0.192\n\n\nintercept_67\n1.704\n\n\nn_factors\n2\n\n\nest_age35-50\n-0.048\n\n\nlowerCI_age35-50\n-0.443\n\n\nhigherCI_age35-50\n0.348\n\n\nse_age35-50\n0.201\n\n\ntval_age35-50\n-0.237\n\n\npval_age35-50\n0.813\n\n\nest_age&gt; 50\n0.329\n\n\nlowerCI_age&gt; 50\n-0.314\n\n\nhigherCI_age&gt; 50\n0.972\n\n\nse_age&gt; 50\n0.327\n\n\ntval_age&gt; 50\n1.006\n\n\npval_age&gt; 50\n0.315\n\n\n\n\n\n\n### build final model only with significant variables\n\nm_diversity_null &lt;- polr(assess_diversity_congress_rating ~ 1, data=subset(survey, !is.na(gender) & !is.na(lgbtq)))\n\nm_diversity &lt;- polr(assess_diversity_congress_rating ~ gender + lgbtq, data=subset(survey, !is.na(gender) & !is.na(lgbtq)))\n\ndrop1(m_diversity, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\nassess_diversity_congress_rating ~ gender + lgbtq\n       Df    AIC    LRT Pr(&gt;Chi)  \n&lt;none&gt;    1227.4                  \ngender  2 1230.4 6.9931  0.03030 *\nlgbtq   1 1230.2 4.7748  0.02888 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(m_diversity)\n\n\nt test of coefficients:\n\n                 Estimate Std. Error t value Pr(&gt;|t|)  \ngenderFemale     -0.53345    0.20878 -2.5550  0.01104 *\ngenderNon-binary -0.83271    0.68289 -1.2194  0.22352  \nlgbtqYes         -0.60211    0.27603 -2.1813  0.02983 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# what about an interaction?\nm_diversity_interact &lt;- polr(assess_diversity_congress_rating ~ gender*lgbtq, data=subset(survey, !is.na(gender) & !is.na(lgbtq)))\n\ndrop1(m_diversity_interact, test = \"Chisq\") #NS\n\nSingle term deletions\n\nModel:\nassess_diversity_congress_rating ~ gender * lgbtq\n             Df    AIC     LRT Pr(&gt;Chi)\n&lt;none&gt;          1229.0                 \ngender:lgbtq  1 1227.4 0.41641   0.5187\n\nm_diversity_out &lt;- collect_out(model = m_diversity, null = m_diversity_null, name = \"diversity_final\", n_factors = 3, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_diversity_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\ndiversity_final\n\n\nAIC\n1227.38\n\n\nn_obs\n357\n\n\nlrt_pval\n0.002\n\n\nlrt_chisq\n14.497\n\n\nintercept_12\n-4.605\n\n\nintercept_23\n-3.247\n\n\nintercept_34\n-2.259\n\n\nintercept_45\n-1.242\n\n\nintercept_56\n-0.252\n\n\nintercept_67\n1.225\n\n\nn_factors\n3\n\n\nest_genderFemale\n-0.533\n\n\nlowerCI_genderFemale\n-0.944\n\n\nhigherCI_genderFemale\n-0.123\n\n\nse_genderFemale\n0.209\n\n\ntval_genderFemale\n-2.555\n\n\npval_genderFemale\n0.011\n\n\nest_genderNon-binary\n-0.833\n\n\nlowerCI_genderNon-binary\n-2.176\n\n\nhigherCI_genderNon-binary\n0.51\n\n\nse_genderNon-binary\n0.683\n\n\ntval_genderNon-binary\n-1.219\n\n\npval_genderNon-binary\n0.224\n\n\nest_lgbtqYes\n-0.602\n\n\nlowerCI_lgbtqYes\n-1.145\n\n\nhigherCI_lgbtqYes\n-0.059\n\n\nse_lgbtqYes\n0.276\n\n\ntval_lgbtqYes\n-2.181\n\n\npval_lgbtqYes\n0.03\n\n\n\n\n\n\n\nThe results indicate that gender and LGBTQ+ identity affected whether a person agreed more with the diversity of researchers in our field was represented at the congress. Men had higher agreement compared to women, and LGBTQ+ people had lower agreement compared to non-LGBTQ+ people."
  },
  {
    "objectID": "qmd/8_perception_issues.html#edi-issues",
    "href": "qmd/8_perception_issues.html#edi-issues",
    "title": "9  Perception of EDI issues",
    "section": "9.2 EDI issues",
    "text": "9.2 EDI issues\n\n# first, we explore the distribution of answers\n\n# edi issues\nggplot(survey, aes(assess_edi_issues_rating)) + \n  geom_histogram(stat=\"count\", aes(y=stat(count/sum(count)*100)), fill = clrs[11]) + \n  geom_text(aes(label = paste0(\"N = \", ..count..), y = stat(count/sum(count)*100)), \n            stat=\"count\", vjust=-1) +\n  labs(x = \"Our research field experiences equity, diversity and \ninclusion related issues\", y = \"% of responses\") \n\n\n\n# then we test the effect of each of the social identity variables\n\n# gender\nm_edi_issues_gender_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(gender)))\n\nm_edi_issues_gender &lt;- polr(assess_edi_issues_rating ~ gender,\n                   data=survey) \n\nm_edi_issues_gender_out &lt;- collect_out(model = m_edi_issues_gender, null = m_edi_issues_gender_null, name = \"edi_issues_gender\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_edi_issues_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nedi_issues_gender\n\n\nAIC\n1323.308\n\n\nn_obs\n370\n\n\nlrt_pval\n0.004\n\n\nlrt_chisq\n10.917\n\n\nintercept_12\n-3.185\n\n\nintercept_23\n-1.7\n\n\nintercept_34\n-0.994\n\n\nintercept_45\n0.172\n\n\nintercept_56\n1.232\n\n\nintercept_67\n2.367\n\n\nn_factors\n2\n\n\nest_genderFemale\n0.628\n\n\nlowerCI_genderFemale\n0.225\n\n\nhigherCI_genderFemale\n1.032\n\n\nse_genderFemale\n0.205\n\n\ntval_genderFemale\n3.062\n\n\npval_genderFemale\n0.002\n\n\nest_genderNon-binary\n1.231\n\n\nlowerCI_genderNon-binary\n-0.066\n\n\nhigherCI_genderNon-binary\n2.527\n\n\nse_genderNon-binary\n0.659\n\n\ntval_genderNon-binary\n1.867\n\n\npval_genderNon-binary\n0.063\n\n\n\n\n\n\n# lgbtqia \n\nm_edi_issues_lgbtq_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(lgbtq)))\n\nm_edi_issues_lgbtq &lt;- polr(assess_edi_issues_rating ~ lgbtq,\n                   data=subset(survey, !is.na(lgbtq))) \n\nm_edi_issues_lgbtq_out &lt;- collect_out(model = m_edi_issues_lgbtq, null = m_edi_issues_lgbtq_null, name = \"edi_issues_lgbtq\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_edi_issues_lgbtq_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nedi_issues_lgbtq\n\n\nAIC\n1279.769\n\n\nn_obs\n357\n\n\nlrt_pval\n0.001\n\n\nlrt_chisq\n10.4\n\n\nintercept_12\n-3.45\n\n\nintercept_23\n-1.991\n\n\nintercept_34\n-1.296\n\n\nintercept_45\n-0.143\n\n\nintercept_56\n0.886\n\n\nintercept_67\n2.009\n\n\nn_factors\n1\n\n\nest_lgbtqYes\n0.799\n\n\nlowerCI_lgbtqYes\n0.309\n\n\nhigherCI_lgbtqYes\n1.289\n\n\nse_lgbtqYes\n0.249\n\n\ntval_lgbtqYes\n3.208\n\n\npval_lgbtqYes\n0.001\n\n\n\n\n\n\n# nationality\nm_edi_issues_nat_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(nationality_continent)))\n\nm_edi_issues_nat &lt;- polr(assess_edi_issues_rating ~ nationality_continent,\n                   data=survey) \n\nm_edi_issues_nat_out &lt;- collect_out(model = m_edi_issues_nat, null = m_edi_issues_nat_null, name = \"edi_issues_nat\", n_factors = 4, type = \"likert\", save = \"yes\", dir = \"../results/survey\") \n\nm_edi_issues_nat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant, North America higher\n\n\n\n\nmodel_name\nedi_issues_nat\n\n\nAIC\n1326.398\n\n\nn_obs\n371\n\n\nlrt_pval\n0.015\n\n\nlrt_chisq\n12.385\n\n\nintercept_12\n-3.45\n\n\nintercept_23\n-2.04\n\n\nintercept_34\n-1.405\n\n\nintercept_45\n-0.2\n\n\nintercept_56\n0.853\n\n\nintercept_67\n2.034\n\n\nn_factors\n4\n\n\nest_nationality_continentAsia\n-0.317\n\n\nlowerCI_nationality_continentAsia\n-0.951\n\n\nhigherCI_nationality_continentAsia\n0.316\n\n\nse_nationality_continentAsia\n0.322\n\n\ntval_nationality_continentAsia\n-0.985\n\n\npval_nationality_continentAsia\n0.325\n\n\nest_nationality_continentNorth America\n0.991\n\n\nlowerCI_nationality_continentNorth America\n0.338\n\n\nhigherCI_nationality_continentNorth America\n1.644\n\n\nse_nationality_continentNorth America\n0.332\n\n\ntval_nationality_continentNorth America\n2.985\n\n\npval_nationality_continentNorth America\n0.003\n\n\nest_nationality_continentOceania\n0.636\n\n\nlowerCI_nationality_continentOceania\n-0.577\n\n\nhigherCI_nationality_continentOceania\n1.85\n\n\nse_nationality_continentOceania\n0.617\n\n\ntval_nationality_continentOceania\n1.032\n\n\npval_nationality_continentOceania\n0.303\n\n\nest_nationality_continentSouth America\n0.715\n\n\nlowerCI_nationality_continentSouth America\n-0.581\n\n\nhigherCI_nationality_continentSouth America\n2.01\n\n\nse_nationality_continentSouth America\n0.659\n\n\ntval_nationality_continentSouth America\n1.085\n\n\npval_nationality_continentSouth America\n0.279\n\n\n\n\n\n\n# affiliation\nm_edi_issues_aff_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(affiliation_continent)))\n\nm_edi_issues_aff &lt;- polr(assess_edi_issues_rating ~ affiliation_continent,\n                   data=subset(survey, !is.na(affiliation_continent))) \n\nm_edi_issues_aff_out &lt;- collect_out(model = m_edi_issues_aff, null = m_edi_issues_aff_null, name = \"edi_issues_aff\", n_factors = 5, type = \"likert\", save = \"yes\", dir = \"../results/survey\") \n\nm_edi_issues_aff_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nedi_issues_aff\n\n\nAIC\n1344.82\n\n\nn_obs\n374\n\n\nlrt_pval\n0.238\n\n\nlrt_chisq\n6.775\n\n\nintercept_12\n-3.485\n\n\nintercept_23\n-2.079\n\n\nintercept_34\n-1.451\n\n\nintercept_45\n-0.276\n\n\nintercept_56\n0.773\n\n\nintercept_67\n1.919\n\n\nn_factors\n5\n\n\nest_affiliation_continentAsia\n0.264\n\n\nlowerCI_affiliation_continentAsia\n-0.684\n\n\nhigherCI_affiliation_continentAsia\n1.212\n\n\nse_affiliation_continentAsia\n0.482\n\n\ntval_affiliation_continentAsia\n0.547\n\n\npval_affiliation_continentAsia\n0.584\n\n\nest_affiliation_continentAfrica\n13.018\n\n\nlowerCI_affiliation_continentAfrica\n-492.568\n\n\nhigherCI_affiliation_continentAfrica\n518.604\n\n\nse_affiliation_continentAfrica\n257.097\n\n\ntval_affiliation_continentAfrica\n0.051\n\n\npval_affiliation_continentAfrica\n0.96\n\n\nest_affiliation_continentNorth America\n0.466\n\n\nlowerCI_affiliation_continentNorth America\n-0.342\n\n\nhigherCI_affiliation_continentNorth America\n1.273\n\n\nse_affiliation_continentNorth America\n0.411\n\n\ntval_affiliation_continentNorth America\n1.134\n\n\npval_affiliation_continentNorth America\n0.258\n\n\nest_affiliation_continentOceania\n-0.653\n\n\nlowerCI_affiliation_continentOceania\n-1.918\n\n\nhigherCI_affiliation_continentOceania\n0.612\n\n\nse_affiliation_continentOceania\n0.643\n\n\ntval_affiliation_continentOceania\n-1.015\n\n\npval_affiliation_continentOceania\n0.311\n\n\nest_affiliation_continentSouth America\n0.502\n\n\nlowerCI_affiliation_continentSouth America\n-2.923\n\n\nhigherCI_affiliation_continentSouth America\n3.927\n\n\nse_affiliation_continentSouth America\n1.742\n\n\ntval_affiliation_continentSouth America\n0.288\n\n\npval_affiliation_continentSouth America\n0.773\n\n\n\n\n\n\n# expat\n\nm_edi_issues_expat_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(expat)))\n\nm_edi_issues_expat &lt;- polr(assess_edi_issues_rating ~ expat,\n                   data=subset(survey, !is.na(expat))) \n\nm_edi_issues_expat_out &lt;- collect_out(model = m_edi_issues_expat, null = m_edi_issues_expat_null, name = \"edi_issues_expat\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_edi_issues_expat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\nedi_issues_expat\n\n\nAIC\n1312.628\n\n\nn_obs\n368\n\n\nlrt_pval\n0.003\n\n\nlrt_chisq\n8.881\n\n\nintercept_12\n-3.254\n\n\nintercept_23\n-1.851\n\n\nintercept_34\n-1.236\n\n\nintercept_45\n-0.029\n\n\nintercept_56\n1.03\n\n\nintercept_67\n2.19\n\n\nn_factors\n1\n\n\nest_expatExpat\n0.555\n\n\nlowerCI_expatExpat\n0.187\n\n\nhigherCI_expatExpat\n0.923\n\n\nse_expatExpat\n0.187\n\n\ntval_expatExpat\n2.967\n\n\npval_expatExpat\n0.003\n\n\n\n\n\n\n# english level \nsurvey$english_comfort_rating &lt;- as.numeric(as.character(survey$english_comfort_rating))\n\nm_edi_issues_english_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(english_comfort_rating)))\n\nm_edi_issues_english &lt;- polr(assess_edi_issues_rating ~ english_comfort_rating,\n                   data=subset(survey, !is.na(english_comfort_rating))) \n\nm_edi_issues_english_out &lt;- collect_out(model = m_edi_issues_english, null = m_edi_issues_english_null, name = \"edi_issues_english\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_edi_issues_english_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nedi_issues_english\n\n\nAIC\n1373.653\n\n\nn_obs\n381\n\n\nlrt_pval\n0.583\n\n\nlrt_chisq\n0.301\n\n\nintercept_12\n-3.184\n\n\nintercept_23\n-1.757\n\n\nintercept_34\n-1.091\n\n\nintercept_45\n0.052\n\n\nintercept_56\n1.074\n\n\nintercept_67\n2.201\n\n\nn_factors\n1\n\n\nest_english_comfort_rating\n0.052\n\n\nlowerCI_english_comfort_rating\n-0.134\n\n\nhigherCI_english_comfort_rating\n0.237\n\n\nse_english_comfort_rating\n0.094\n\n\ntval_english_comfort_rating\n0.547\n\n\npval_english_comfort_rating\n0.585\n\n\n\n\n\n\n# age\n\nm_edi_issues_age_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(age)))\n\nm_edi_issues_age &lt;- polr(assess_edi_issues_rating ~ age,\n                   data=subset(survey, !is.na(age))) \n\nm_edi_issues_age_out &lt;- collect_out(model = m_edi_issues_age, null = m_edi_issues_age_null, name = \"edi_issues_age\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_edi_issues_age_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\nedi_issues_age\n\n\nAIC\n1373.432\n\n\nn_obs\n381\n\n\nlrt_pval\n0.283\n\n\nlrt_chisq\n2.521\n\n\nintercept_12\n-3.577\n\n\nintercept_23\n-2.147\n\n\nintercept_34\n-1.477\n\n\nintercept_45\n-0.328\n\n\nintercept_56\n0.698\n\n\nintercept_67\n1.827\n\n\nn_factors\n2\n\n\nest_age35-50\n-0.01\n\n\nlowerCI_age35-50\n-0.406\n\n\nhigherCI_age35-50\n0.386\n\n\nse_age35-50\n0.202\n\n\ntval_age35-50\n-0.05\n\n\npval_age35-50\n0.96\n\n\nest_age&gt; 50\n-0.518\n\n\nlowerCI_age&gt; 50\n-1.168\n\n\nhigherCI_age&gt; 50\n0.132\n\n\nse_age&gt; 50\n0.331\n\n\ntval_age&gt; 50\n-1.567\n\n\npval_age&gt; 50\n0.118\n\n\n\n\n\n\n### build final model only with significant variables\n\nm_edi_issues_null &lt;- polr(assess_edi_issues_rating ~ 1, data=subset(survey, !is.na(gender) & !is.na(lgbtq) & !is.na(nationality_continent) & !is.na(expat)))\n\nm_edi_issues &lt;- polr(assess_edi_issues_rating ~ gender + lgbtq + nationality_continent + expat, data=subset(survey, !is.na(gender) & !is.na(lgbtq) & !is.na(nationality_continent) & !is.na(expat)))\n\ndrop1(m_edi_issues, test = \"Chisq\") #gender not sig anymore\n\nSingle term deletions\n\nModel:\nassess_edi_issues_rating ~ gender + lgbtq + nationality_continent + \n    expat\n                      Df    AIC    LRT Pr(&gt;Chi)   \n&lt;none&gt;                   1210.9                   \ngender                 2 1211.8 4.8976 0.086395 . \nlgbtq                  1 1216.0 7.0187 0.008066 **\nnationality_continent  4 1212.0 9.0084 0.060890 . \nexpat                  1 1216.6 7.6871 0.005562 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm_edi_issues_out &lt;- collect_out(model = m_edi_issues, null = m_edi_issues_null, name = \"edi_issues_final\", n_factors = 8, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_edi_issues_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\nedi_issues_final\n\n\nAIC\n1210.941\n\n\nn_obs\n342\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n34.391\n\n\nintercept_12\n-2.876\n\n\nintercept_23\n-1.43\n\n\nintercept_34\n-0.771\n\n\nintercept_45\n0.488\n\n\nintercept_56\n1.6\n\n\nintercept_67\n2.768\n\n\nn_factors\n8\n\n\nest_genderFemale\n0.481\n\n\nlowerCI_genderFemale\n0.051\n\n\nhigherCI_genderFemale\n0.911\n\n\nse_genderFemale\n0.219\n\n\ntval_genderFemale\n2.2\n\n\npval_genderFemale\n0.029\n\n\nest_genderNon-binary\n0.238\n\n\nlowerCI_genderNon-binary\n-1.123\n\n\nhigherCI_genderNon-binary\n1.599\n\n\nse_genderNon-binary\n0.692\n\n\ntval_genderNon-binary\n0.344\n\n\npval_genderNon-binary\n0.731\n\n\nest_lgbtqYes\n0.729\n\n\nlowerCI_lgbtqYes\n0.185\n\n\nhigherCI_lgbtqYes\n1.273\n\n\nse_lgbtqYes\n0.276\n\n\ntval_lgbtqYes\n2.637\n\n\npval_lgbtqYes\n0.009\n\n\nest_nationality_continentAsia\n-0.335\n\n\nlowerCI_nationality_continentAsia\n-1.009\n\n\nhigherCI_nationality_continentAsia\n0.339\n\n\nse_nationality_continentAsia\n0.343\n\n\ntval_nationality_continentAsia\n-0.978\n\n\npval_nationality_continentAsia\n0.329\n\n\nest_nationality_continentNorth America\n0.767\n\n\nlowerCI_nationality_continentNorth America\n0.087\n\n\nhigherCI_nationality_continentNorth America\n1.446\n\n\nse_nationality_continentNorth America\n0.345\n\n\ntval_nationality_continentNorth America\n2.219\n\n\npval_nationality_continentNorth America\n0.027\n\n\nest_nationality_continentOceania\n0.368\n\n\nlowerCI_nationality_continentOceania\n-0.985\n\n\nhigherCI_nationality_continentOceania\n1.721\n\n\nse_nationality_continentOceania\n0.688\n\n\ntval_nationality_continentOceania\n0.535\n\n\npval_nationality_continentOceania\n0.593\n\n\nest_nationality_continentSouth America\n1.267\n\n\nlowerCI_nationality_continentSouth America\n-0.296\n\n\nhigherCI_nationality_continentSouth America\n2.83\n\n\nse_nationality_continentSouth America\n0.795\n\n\ntval_nationality_continentSouth America\n1.594\n\n\npval_nationality_continentSouth America\n0.112\n\n\nest_expatExpat\n0.548\n\n\nlowerCI_expatExpat\n0.158\n\n\nhigherCI_expatExpat\n0.939\n\n\nse_expatExpat\n0.199\n\n\ntval_expatExpat\n2.762\n\n\npval_expatExpat\n0.006\n\n\n\n\n\n\n\nThe results indicate that even though gender and nationality were significant in the univariate models, when accounting for LGBTQ+ identity and expat status, they are not significant anymore in the final model (although nationality is borderline significant). The final model shows that LGBTQ+ identities agree more with there being EDI issues in our field, and so do expats. Looking at nationality, the result is difficult to interpret due to the unbalanced sample size and needing to drop the nationality on a sub-continent level to zoom out to the continent level. Nevertheless, it appears that North American nationalities agrees more compared to European nationalities."
  },
  {
    "objectID": "qmd/8_perception_issues.html#question-asking-gender-disparity",
    "href": "qmd/8_perception_issues.html#question-asking-gender-disparity",
    "title": "9  Perception of EDI issues",
    "section": "9.3 Question asking gender disparity",
    "text": "9.3 Question asking gender disparity\n\n# first, we explore the distribution of answers\n\n# no qa disparity\nggplot(survey, aes(assess_gender_qa_rating)) + \n  geom_histogram(stat=\"count\", aes(y=stat(count/sum(count)*100)), fill = clrs[11]) + \n  geom_text(aes(label = paste0(\"N = \", ..count..), y = stat(count/sum(count)*100)), \n            stat=\"count\", vjust=-1) +\n  labs(x = \"I think the questions asked after the talks were equally \ndivided across genders\", y = \"% of responses\") \n\n\n\n# then we test the effect of each of the social identity variables\n\n# gender\nm_gender_qa_gender_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(gender)))\n\nm_gender_qa_gender &lt;- polr(assess_gender_qa_rating ~ gender,\n                   data=survey) \n\nm_gender_qa_gender_out &lt;- collect_out(model = m_gender_qa_gender, null = m_gender_qa_gender_null, name = \"gender_qa_gender\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_gender_qa_gender_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ngender_qa_gender\n\n\nAIC\n1345.544\n\n\nn_obs\n373\n\n\nlrt_pval\n0.014\n\n\nlrt_chisq\n8.58\n\n\nintercept_12\n-3.343\n\n\nintercept_23\n-2.565\n\n\nintercept_34\n-1.651\n\n\nintercept_45\n-0.357\n\n\nintercept_56\n0.404\n\n\nintercept_67\n1.655\n\n\nn_factors\n2\n\n\nest_genderFemale\n-0.44\n\n\nlowerCI_genderFemale\n-0.836\n\n\nhigherCI_genderFemale\n-0.043\n\n\nse_genderFemale\n0.202\n\n\ntval_genderFemale\n-2.181\n\n\npval_genderFemale\n0.03\n\n\nest_genderNon-binary\n-1.516\n\n\nlowerCI_genderNon-binary\n-2.761\n\n\nhigherCI_genderNon-binary\n-0.271\n\n\nse_genderNon-binary\n0.633\n\n\ntval_genderNon-binary\n-2.394\n\n\npval_genderNon-binary\n0.017\n\n\n\n\n\n\n# lgbtqia \n\nm_gender_qa_lgbtq_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(lgbtq)))\n\nm_gender_qa_lgbtq &lt;- polr(assess_gender_qa_rating ~ lgbtq,\n                   data=subset(survey, !is.na(lgbtq))) \n\nm_gender_qa_lgbtq_out &lt;- collect_out(model = m_gender_qa_lgbtq, null = m_gender_qa_lgbtq_null, name = \"gender_qa_lgbtq\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_gender_qa_lgbtq_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ngender_qa_lgbtq\n\n\nAIC\n1295.149\n\n\nn_obs\n360\n\n\nlrt_pval\n0.006\n\n\nlrt_chisq\n7.599\n\n\nintercept_12\n-3.088\n\n\nintercept_23\n-2.364\n\n\nintercept_34\n-1.413\n\n\nintercept_45\n-0.105\n\n\nintercept_56\n0.649\n\n\nintercept_67\n1.875\n\n\nn_factors\n1\n\n\nest_lgbtqYes\n-0.692\n\n\nlowerCI_lgbtqYes\n-1.189\n\n\nhigherCI_lgbtqYes\n-0.196\n\n\nse_lgbtqYes\n0.252\n\n\ntval_lgbtqYes\n-2.742\n\n\npval_lgbtqYes\n0.006\n\n\n\n\n\n\n# nationality\nm_gender_qa_nat_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(nationality_continent)))\n\nm_gender_qa_nat &lt;- polr(assess_gender_qa_rating ~ nationality_continent,\n                   data=survey) \n\nm_gender_qa_nat_out &lt;- collect_out(model = m_gender_qa_nat, null = m_gender_qa_nat_null, name = \"gender_qa_nat\", n_factors = 4, type = \"likert\", save = \"yes\", dir = \"../results/survey\") \n\nm_gender_qa_nat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant, Asia sig with higher values\n\n\n\n\nmodel_name\ngender_qa_nat\n\n\nAIC\n1345.941\n\n\nn_obs\n374\n\n\nlrt_pval\n0.011\n\n\nlrt_chisq\n13.085\n\n\nintercept_12\n-2.988\n\n\nintercept_23\n-2.152\n\n\nintercept_34\n-1.24\n\n\nintercept_45\n0.089\n\n\nintercept_56\n0.843\n\n\nintercept_67\n2.077\n\n\nn_factors\n4\n\n\nest_nationality_continentAsia\n1.126\n\n\nlowerCI_nationality_continentAsia\n0.456\n\n\nhigherCI_nationality_continentAsia\n1.796\n\n\nse_nationality_continentAsia\n0.341\n\n\ntval_nationality_continentAsia\n3.304\n\n\npval_nationality_continentAsia\n0.001\n\n\nest_nationality_continentNorth America\n-0.227\n\n\nlowerCI_nationality_continentNorth America\n-0.903\n\n\nhigherCI_nationality_continentNorth America\n0.449\n\n\nse_nationality_continentNorth America\n0.344\n\n\ntval_nationality_continentNorth America\n-0.661\n\n\npval_nationality_continentNorth America\n0.509\n\n\nest_nationality_continentOceania\n-0.081\n\n\nlowerCI_nationality_continentOceania\n-1.326\n\n\nhigherCI_nationality_continentOceania\n1.165\n\n\nse_nationality_continentOceania\n0.633\n\n\ntval_nationality_continentOceania\n-0.127\n\n\npval_nationality_continentOceania\n0.899\n\n\nest_nationality_continentSouth America\n0.936\n\n\nlowerCI_nationality_continentSouth America\n-0.692\n\n\nhigherCI_nationality_continentSouth America\n2.563\n\n\nse_nationality_continentSouth America\n0.828\n\n\ntval_nationality_continentSouth America\n1.131\n\n\npval_nationality_continentSouth America\n0.259\n\n\n\n\n\n\n# affiliation \nm_gender_qa_aff_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(affiliation_continent)))\n\nm_gender_qa_aff &lt;- polr(assess_gender_qa_rating ~ affiliation_continent,\n                   data=subset(survey, !is.na(affiliation_continent))) \n\nm_gender_qa_aff_out &lt;- collect_out(model = m_gender_qa_aff, null = m_gender_qa_aff_null, name = \"gender_qa_aff\", n_factors = 5, type = \"likert\", save = \"yes\", dir = \"../results/survey\") \n\nm_gender_qa_aff_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() #sig, South America almost sig with lower values\n\n\n\n\nmodel_name\ngender_qa_aff\n\n\nAIC\n1356.011\n\n\nn_obs\n377\n\n\nlrt_pval\n0.009\n\n\nlrt_chisq\n15.324\n\n\nintercept_12\n-2.992\n\n\nintercept_23\n-2.209\n\n\nintercept_34\n-1.279\n\n\nintercept_45\n0.044\n\n\nintercept_56\n0.809\n\n\nintercept_67\n2.079\n\n\nn_factors\n5\n\n\nest_affiliation_continentAsia\n1.615\n\n\nlowerCI_affiliation_continentAsia\n0.632\n\n\nhigherCI_affiliation_continentAsia\n2.599\n\n\nse_affiliation_continentAsia\n0.5\n\n\ntval_affiliation_continentAsia\n3.231\n\n\npval_affiliation_continentAsia\n0.001\n\n\nest_affiliation_continentAfrica\n1.444\n\n\nlowerCI_affiliation_continentAfrica\n-1.489\n\n\nhigherCI_affiliation_continentAfrica\n4.377\n\n\nse_affiliation_continentAfrica\n1.491\n\n\ntval_affiliation_continentAfrica\n0.968\n\n\npval_affiliation_continentAfrica\n0.334\n\n\nest_affiliation_continentNorth America\n-0.306\n\n\nlowerCI_affiliation_continentNorth America\n-1.117\n\n\nhigherCI_affiliation_continentNorth America\n0.504\n\n\nse_affiliation_continentNorth America\n0.412\n\n\ntval_affiliation_continentNorth America\n-0.743\n\n\npval_affiliation_continentNorth America\n0.458\n\n\nest_affiliation_continentOceania\n0.393\n\n\nlowerCI_affiliation_continentOceania\n-0.706\n\n\nhigherCI_affiliation_continentOceania\n1.492\n\n\nse_affiliation_continentOceania\n0.559\n\n\ntval_affiliation_continentOceania\n0.703\n\n\npval_affiliation_continentOceania\n0.483\n\n\nest_affiliation_continentSouth America\n-2.338\n\n\nlowerCI_affiliation_continentSouth America\n-5.152\n\n\nhigherCI_affiliation_continentSouth America\n0.476\n\n\nse_affiliation_continentSouth America\n1.431\n\n\ntval_affiliation_continentSouth America\n-1.634\n\n\npval_affiliation_continentSouth America\n0.103\n\n\n\n\n\n\n# expat\n\nm_gender_qa_expat_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(expat)))\n\nm_gender_qa_expat &lt;- polr(assess_gender_qa_rating ~ expat,\n                   data=subset(survey, !is.na(expat))) \n\nm_gender_qa_expat_out &lt;- collect_out(model = m_gender_qa_expat, null = m_gender_qa_expat_null, name = \"gender_qa_expat\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_gender_qa_expat_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ngender_qa_expat\n\n\nAIC\n1340.416\n\n\nn_obs\n371\n\n\nlrt_pval\n0.801\n\n\nlrt_chisq\n0.063\n\n\nintercept_12\n-3.058\n\n\nintercept_23\n-2.253\n\n\nintercept_34\n-1.329\n\n\nintercept_45\n-0.016\n\n\nintercept_56\n0.73\n\n\nintercept_67\n1.934\n\n\nn_factors\n1\n\n\nest_expatExpat\n-0.046\n\n\nlowerCI_expatExpat\n-0.409\n\n\nhigherCI_expatExpat\n0.316\n\n\nse_expatExpat\n0.185\n\n\ntval_expatExpat\n-0.252\n\n\npval_expatExpat\n0.801\n\n\n\n\n\n\n# english level \nsurvey$english_comfort_rating &lt;- as.numeric(as.character(survey$english_comfort_rating))\n\nm_gender_qa_english_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(english_comfort_rating)))\n\nm_gender_qa_english &lt;- polr(assess_gender_qa_rating ~ english_comfort_rating,\n                   data=subset(survey, !is.na(english_comfort_rating))) \n\nm_gender_qa_english_out &lt;- collect_out(model = m_gender_qa_english, null = m_gender_qa_english_null, name = \"gender_qa_english\", n_factors = 1, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_gender_qa_english_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # significant\n\n\n\n\nmodel_name\ngender_qa_english\n\n\nAIC\n1383.542\n\n\nn_obs\n384\n\n\nlrt_pval\n0.016\n\n\nlrt_chisq\n5.795\n\n\nintercept_12\n-4.497\n\n\nintercept_23\n-3.692\n\n\nintercept_34\n-2.781\n\n\nintercept_45\n-1.46\n\n\nintercept_56\n-0.711\n\n\nintercept_67\n0.501\n\n\nn_factors\n1\n\n\nest_english_comfort_rating\n-0.228\n\n\nlowerCI_english_comfort_rating\n-0.414\n\n\nhigherCI_english_comfort_rating\n-0.042\n\n\nse_english_comfort_rating\n0.095\n\n\ntval_english_comfort_rating\n-2.411\n\n\npval_english_comfort_rating\n0.016\n\n\n\n\n\n\n# age\n\nm_gender_qa_age_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(age)))\n\nm_gender_qa_age &lt;- polr(assess_gender_qa_rating ~ age,\n                   data=subset(survey, !is.na(age))) \n\nm_gender_qa_age_out &lt;- collect_out(model = m_gender_qa_age, null = m_gender_qa_age_null, name = \"gender_qa_age\", n_factors = 2, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_gender_qa_age_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() # not significant\n\n\n\n\nmodel_name\ngender_qa_age\n\n\nAIC\n1387.829\n\n\nn_obs\n384\n\n\nlrt_pval\n0.173\n\n\nlrt_chisq\n3.507\n\n\nintercept_12\n-3.16\n\n\nintercept_23\n-2.355\n\n\nintercept_34\n-1.448\n\n\nintercept_45\n-0.139\n\n\nintercept_56\n0.607\n\n\nintercept_67\n1.82\n\n\nn_factors\n2\n\n\nest_age35-50\n-0.316\n\n\nlowerCI_age35-50\n-0.71\n\n\nhigherCI_age35-50\n0.079\n\n\nse_age35-50\n0.201\n\n\ntval_age35-50\n-1.571\n\n\npval_age35-50\n0.117\n\n\nest_age&gt; 50\n-0.419\n\n\nlowerCI_age&gt; 50\n-1.037\n\n\nhigherCI_age&gt; 50\n0.198\n\n\nse_age&gt; 50\n0.314\n\n\ntval_age&gt; 50\n-1.336\n\n\npval_age&gt; 50\n0.182\n\n\n\n\n\n\n### build final model only with significant variables\n\nm_gender_qa_null &lt;- polr(assess_gender_qa_rating ~ 1, data=subset(survey, !is.na(gender) & !is.na(lgbtq) & !is.na(nationality_continent) & !is.na(affiliation_continent) & !is.na(english_comfort_rating)))\n\nm_gender_qa &lt;- polr(assess_gender_qa_rating ~ gender + lgbtq + nationality_continent + affiliation_continent + english_comfort_rating, data=subset(survey, !is.na(gender) & !is.na(lgbtq) & !is.na(nationality_continent) & !is.na(affiliation_continent) & !is.na(english_comfort_rating)))\n\ndrop1(m_gender_qa, test = \"Chisq\") #gender not sig anymore, lgbtq and nationality borderline\n\nSingle term deletions\n\nModel:\nassess_gender_qa_rating ~ gender + lgbtq + nationality_continent + \n    affiliation_continent + english_comfort_rating\n                       Df    AIC     LRT Pr(&gt;Chi)  \n&lt;none&gt;                    1234.9                   \ngender                  2 1235.5  4.5519  0.10270  \nlgbtq                   1 1236.2  3.2644  0.07080 .\nnationality_continent   4 1235.0  8.1397  0.08659 .\naffiliation_continent   5 1236.7 11.7399  0.03853 *\nenglish_comfort_rating  1 1237.8  4.9265  0.02645 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm_gender_qa_out &lt;- collect_out(model = m_gender_qa, null = m_gender_qa_null, name = \"gender_qa_final\", n_factors = 13, type = \"likert\", save = \"yes\",  dir = \"../results/survey\") \n\nm_gender_qa_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2() \n\n\n\n\nmodel_name\ngender_qa_final\n\n\nAIC\n1234.907\n\n\nn_obs\n344\n\n\nlrt_pval\n0.001\n\n\nlrt_chisq\n36.286\n\n\nintercept_12\n-4.856\n\n\nintercept_23\n-4.087\n\n\nintercept_34\n-3.113\n\n\nintercept_45\n-1.768\n\n\nintercept_56\n-0.974\n\n\nintercept_67\n0.363\n\n\nn_factors\n13\n\n\nest_genderFemale\n-0.405\n\n\nlowerCI_genderFemale\n-0.847\n\n\nhigherCI_genderFemale\n0.036\n\n\nse_genderFemale\n0.224\n\n\ntval_genderFemale\n-1.807\n\n\npval_genderFemale\n0.072\n\n\nest_genderNon-binary\n-1.079\n\n\nlowerCI_genderNon-binary\n-2.448\n\n\nhigherCI_genderNon-binary\n0.29\n\n\nse_genderNon-binary\n0.696\n\n\ntval_genderNon-binary\n-1.55\n\n\npval_genderNon-binary\n0.122\n\n\nest_lgbtqYes\n-0.521\n\n\nlowerCI_lgbtqYes\n-1.09\n\n\nhigherCI_lgbtqYes\n0.049\n\n\nse_lgbtqYes\n0.29\n\n\ntval_lgbtqYes\n-1.798\n\n\npval_lgbtqYes\n0.073\n\n\nest_nationality_continentAsia\n0.742\n\n\nlowerCI_nationality_continentAsia\n-0.144\n\n\nhigherCI_nationality_continentAsia\n1.629\n\n\nse_nationality_continentAsia\n0.451\n\n\ntval_nationality_continentAsia\n1.648\n\n\npval_nationality_continentAsia\n0.1\n\n\nest_nationality_continentNorth America\n0.433\n\n\nlowerCI_nationality_continentNorth America\n-0.395\n\n\nhigherCI_nationality_continentNorth America\n1.26\n\n\nse_nationality_continentNorth America\n0.421\n\n\ntval_nationality_continentNorth America\n1.029\n\n\npval_nationality_continentNorth America\n0.304\n\n\nest_nationality_continentOceania\n-0.256\n\n\nlowerCI_nationality_continentOceania\n-1.96\n\n\nhigherCI_nationality_continentOceania\n1.447\n\n\nse_nationality_continentOceania\n0.866\n\n\ntval_nationality_continentOceania\n-0.296\n\n\npval_nationality_continentOceania\n0.768\n\n\nest_nationality_continentSouth America\n2.639\n\n\nlowerCI_nationality_continentSouth America\n0.094\n\n\nhigherCI_nationality_continentSouth America\n5.184\n\n\nse_nationality_continentSouth America\n1.294\n\n\ntval_nationality_continentSouth America\n2.04\n\n\npval_nationality_continentSouth America\n0.042\n\n\nest_affiliation_continentAsia\n0.575\n\n\nlowerCI_affiliation_continentAsia\n-0.786\n\n\nhigherCI_affiliation_continentAsia\n1.936\n\n\nse_affiliation_continentAsia\n0.692\n\n\ntval_affiliation_continentAsia\n0.831\n\n\npval_affiliation_continentAsia\n0.406\n\n\nest_affiliation_continentAfrica\n1.74\n\n\nlowerCI_affiliation_continentAfrica\n-1.22\n\n\nhigherCI_affiliation_continentAfrica\n4.7\n\n\nse_affiliation_continentAfrica\n1.505\n\n\ntval_affiliation_continentAfrica\n1.156\n\n\npval_affiliation_continentAfrica\n0.248\n\n\nest_affiliation_continentNorth America\n-0.448\n\n\nlowerCI_affiliation_continentNorth America\n-1.421\n\n\nhigherCI_affiliation_continentNorth America\n0.526\n\n\nse_affiliation_continentNorth America\n0.495\n\n\ntval_affiliation_continentNorth America\n-0.905\n\n\npval_affiliation_continentNorth America\n0.366\n\n\nest_affiliation_continentOceania\n0.53\n\n\nlowerCI_affiliation_continentOceania\n-1.007\n\n\nhigherCI_affiliation_continentOceania\n2.068\n\n\nse_affiliation_continentOceania\n0.782\n\n\ntval_affiliation_continentOceania\n0.679\n\n\npval_affiliation_continentOceania\n0.498\n\n\nest_affiliation_continentSouth America\n-5.387\n\n\nlowerCI_affiliation_continentSouth America\n-9.229\n\n\nhigherCI_affiliation_continentSouth America\n-1.544\n\n\nse_affiliation_continentSouth America\n1.953\n\n\ntval_affiliation_continentSouth America\n-2.758\n\n\npval_affiliation_continentSouth America\n0.006\n\n\nest_english_comfort_rating\n-0.234\n\n\nlowerCI_english_comfort_rating\n-0.441\n\n\nhigherCI_english_comfort_rating\n-0.027\n\n\nse_english_comfort_rating\n0.105\n\n\ntval_english_comfort_rating\n-2.227\n\n\npval_english_comfort_rating\n0.027\n\n\n\n\n\n\n\nThe results indicate that even though gender, LGBTQ+ and nationality were significant in the univariate models, when accounting for all other significant variables too, they are not significant anymore in the final model (although LGBTQ+ and nationality are borderline significant). The final model shows that Asian countries agree more compared to European nationalities. South American affiliates agree less compared to European affiliations. Lastly, people who are more comfortable speaking English agree less with the statement.\nLGBTQ+ identities agree more with there being EDI issues in our field, and so do expats. Looking at nationality, the result is difficult to interpret due to the unbalanced sample size and needing to drop the nationality on a sub-continent level to zoom out to the continent level. Nevertheless, it appears that North American nationalities agrees more compared to European nationalities."
  },
  {
    "objectID": "qmd/9_other_gender_disparity.html#positive-appraisal",
    "href": "qmd/9_other_gender_disparity.html#positive-appraisal",
    "title": "10  What other gender disparities do we observe in oral sessions?",
    "section": "10.1 Positive appraisal",
    "text": "10.1 Positive appraisal\nPositive appraisal was defined as any positive words towards the speaker, but excluding when simply “Thank you for your talk” was said. However, if this comment included a compliment such as “Thank you for your nice talk”, this was counted as positive appraisal.\nWe used binomial GLMMs to address the effect of the gender and age of the questioner and gender and career stage of the speaker, and corrected for the question number within that Q&A session. We assessed the fit of each variable using a likelihood ratio test and included only the variables that explained significant variation in the final model.\n\n10.1.1 Receiving\n\n# explore the data \ntable(data_analysis$compliment, data_analysis$questioner_gender) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nF\nM\n\n\n\n\nNo\n275\n325\n\n\nYes\n216\n214\n\n\n\n\n\n\ntable(data_analysis$compliment, data_analysis$speaker_gender) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nF\nM\n\n\n\n\nNo\n360\n235\n\n\nYes\n274\n147\n\n\n\n\n\n\n# build initial models\n\n## speaker gender\n\nmodel_compliment_s_gender &lt;- glmer(compliment ~ speaker_gender + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_gender)), family = \"binomial\") \n\nmodel_compliment_s_gender_null &lt;- glmer(compliment ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_gender)), family = \"binomial\") \n\nanova(model_compliment_s_gender, model_compliment_s_gender_null) # not significant\n\nData: subset(data_analysis, !is.na(speaker_gender))\nModels:\nmodel_compliment_s_gender_null: compliment ~ 1 + (1 | session_id/talk_id)\nmodel_compliment_s_gender: compliment ~ speaker_gender + (1 | session_id/talk_id)\n                               npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_compliment_s_gender_null    3 1351.1 1365.8 -672.53   1345.1          \nmodel_compliment_s_gender         4 1351.0 1370.6 -671.47   1343.0 2.1136  1\n                               Pr(&gt;Chisq)\nmodel_compliment_s_gender_null           \nmodel_compliment_s_gender           0.146\n\n## speaker career stage\n\nmodel_compliment_s_career &lt;- glmer(compliment ~ speaker_career_short + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_career_short)), family = \"binomial\") \n\nmodel_compliment_s_career_null &lt;- glmer(compliment ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_career_short)), family = \"binomial\") \n\nanova(model_compliment_s_career, model_compliment_s_career_null) # almost significant\n\nData: subset(data_analysis, !is.na(speaker_career_short))\nModels:\nmodel_compliment_s_career_null: compliment ~ 1 + (1 | session_id/talk_id)\nmodel_compliment_s_career: compliment ~ speaker_career_short + (1 | session_id/talk_id)\n                               npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_compliment_s_career_null    3 1254.6 1269.2 -624.31   1248.6          \nmodel_compliment_s_career         5 1254.0 1278.2 -621.98   1244.0 4.6456  2\n                               Pr(&gt;Chisq)  \nmodel_compliment_s_career_null             \nmodel_compliment_s_career           0.098 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## final model: nothing significant\n\n\n\n10.1.2 Giving\n\n## question nr \n\nmodel_compliment_q_nr &lt;- glmer(compliment ~ question_nr + (1|session_id/talk_id), data = subset(data_analysis, !is.na(question_nr)), family = \"binomial\") \n\nmodel_compliment_q_nr_null &lt;- glmer(compliment ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(question_nr)), family = \"binomial\") \n\nanova(model_compliment_q_nr, model_compliment_q_nr_null) # significant\n\nData: subset(data_analysis, !is.na(question_nr))\nModels:\nmodel_compliment_q_nr_null: compliment ~ 1 + (1 | session_id/talk_id)\nmodel_compliment_q_nr: compliment ~ question_nr + (1 | session_id/talk_id)\n                           npar    AIC    BIC  logLik deviance Chisq Df\nmodel_compliment_q_nr_null    3 1398.7 1413.6 -696.36   1392.7         \nmodel_compliment_q_nr         4 1333.9 1353.8 -662.97   1325.9 66.79  1\n                           Pr(&gt;Chisq)    \nmodel_compliment_q_nr_null               \nmodel_compliment_q_nr       3.021e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## questioner gender\n\nmodel_compliment_q_gender &lt;- glmer(compliment ~ questioner_gender + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_gender)), family = \"binomial\") \n\nmodel_compliment_q_gender_null &lt;- glmer(compliment ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_gender)), family = \"binomial\") \n\nanova(model_compliment_q_gender, model_compliment_q_gender_null) # not significant\n\nData: subset(data_analysis, !is.na(questioner_gender))\nModels:\nmodel_compliment_q_gender_null: compliment ~ 1 + (1 | session_id/talk_id)\nmodel_compliment_q_gender: compliment ~ questioner_gender + (1 | session_id/talk_id)\n                               npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_compliment_q_gender_null    3 1364.9 1379.7 -679.45   1358.9          \nmodel_compliment_q_gender         4 1365.5 1385.2 -678.73   1357.5 1.4431  1\n                               Pr(&gt;Chisq)\nmodel_compliment_q_gender_null           \nmodel_compliment_q_gender          0.2296\n\n## questioner age\n\nmodel_compliment_q_age &lt;- glmer(compliment ~ questioner_age + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_age)), family = \"binomial\") \n\nmodel_compliment_q_age_null &lt;- glmer(compliment ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_age)), family = \"binomial\") \n\nanova(model_compliment_q_age, model_compliment_q_age_null) # significant\n\nData: subset(data_analysis, !is.na(questioner_age))\nModels:\nmodel_compliment_q_age_null: compliment ~ 1 + (1 | session_id/talk_id)\nmodel_compliment_q_age: compliment ~ questioner_age + (1 | session_id/talk_id)\n                            npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_compliment_q_age_null    3 1085.8 1099.9 -539.87   1079.8          \nmodel_compliment_q_age         5 1081.9 1105.5 -535.96   1071.9 7.8264  2\n                            Pr(&gt;Chisq)  \nmodel_compliment_q_age_null             \nmodel_compliment_q_age         0.01998 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# only question number and age questioner significant (LRT p &lt; 0.05)\n\n# build final model\n\nmodel_compliment_null &lt;- glmer(compliment ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(question_nr) & !is.na(questioner_age)), family = \"binomial\") \n\nmodel_compliment &lt;- glmer(compliment ~ question_nr + questioner_age + (1|session_id/talk_id), data = subset(data_analysis, !is.na(question_nr) & !is.na(questioner_age)), family = \"binomial\") \n\n# use helper function to collect output\n\nmodel_compliment_out &lt;- collect_out(model = model_compliment, null = model_compliment_null, n_factors = 3, name = \"m_compliment\", type = \"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nmodel_compliment_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nm_compliment\n\n\nAIC\n1032.187\n\n\nn_obs\n822\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n59.56\n\n\nintercept_estimate\n0.921\n\n\nintercept_estimate_prop\n0.715\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n0.497\n\n\nintercept_ci_higher\n1.344\n\n\nn_factors\n3\n\n\nest_question_nr\n-0.365\n\n\nlowerCI_question_nr\n-0.473\n\n\nhigherCI_question_nr\n-0.257\n\n\npval_question_nr\n0\n\n\nzval_question_nr\n-6.607\n\n\nest_questioner_age2\n-0.607\n\n\nlowerCI_questioner_age2\n-0.976\n\n\nhigherCI_questioner_age2\n-0.238\n\n\npval_questioner_age2\n0.001\n\n\nzval_questioner_age2\n-3.222\n\n\nest_questioner_age3\n-0.493\n\n\nlowerCI_questioner_age3\n-0.99\n\n\nhigherCI_questioner_age3\n0.004\n\n\npval_questioner_age3\n0.052\n\n\nzval_questioner_age3\n-1.945\n\n\n\n\n\n\n\nThese results show is that the likelihood of a questioner giving a words of positive appraisal is higher for questions asked earlier in the Q&A, and that especially younger audience members (age &lt; 35 years) are more likely to give words of positive appraisal compared to older age categories. Gender of either the question asker or speaker were not significant."
  },
  {
    "objectID": "qmd/9_other_gender_disparity.html#jumpers",
    "href": "qmd/9_other_gender_disparity.html#jumpers",
    "title": "10  What other gender disparities do we observe in oral sessions?",
    "section": "10.2 Jumpers",
    "text": "10.2 Jumpers\nNext, we asked whether men or women are more likely to ask a question without being allocated to do so (i.e. chosen by the session host to ask your question). Since this might have to do with the perceived ‘authority’ of the session host, we controlled for the age and gender of the session host as well.\nWe used binomial GLMMs to address the effect of the gender and age of the questioner, and corrected for question number and age and gender of the session host. We assessed the fit of each variable using a likelihood ratio test and included only the variables that explained significant variation in the final model.\n\n# recode NA to 'no jumper' = 0\njumperdata &lt;- data_analysis %&gt;% mutate(jumper = as.factor(case_when(\n  is.na(jumper) ~ \"0\",\n  jumper == \"1\" ~ \"1\" )))\n\n# explore the data \ntable(jumperdata$jumper, jumperdata$questioner_gender) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nF\nM\n\n\n\n\n0\n486\n526\n\n\n1\n5\n13\n\n\n\n\n\n\ntable(jumperdata$jumper, jumperdata$questioner_age) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\n1\n2\n3\n\n\n\n\n0\n391\n302\n119\n\n\n1\n4\n5\n1\n\n\n\n\n\n\n# build initial models\n\n## question nr\n\nmodel_jumper_q_nr &lt;- glmer(jumper ~ question_nr + (1|session_id/talk_id), data = subset(jumperdata, !is.na(question_nr)), family = \"binomial\") \n\nmodel_jumper_q_nr_null &lt;- glmer(jumper ~ 1 + (1|session_id/talk_id), data = subset(jumperdata, !is.na(question_nr)), family = \"binomial\") \n\nanova(model_jumper_q_nr, model_jumper_q_nr_null) # not significant\n\nData: subset(jumperdata, !is.na(question_nr))\nModels:\nmodel_jumper_q_nr_null: jumper ~ 1 + (1 | session_id/talk_id)\nmodel_jumper_q_nr: jumper ~ question_nr + (1 | session_id/talk_id)\n                       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmodel_jumper_q_nr_null    3 188.24 203.13 -91.122   182.24                     \nmodel_jumper_q_nr         4 188.12 207.97 -90.062   180.12 2.1197  1     0.1454\n\n## questioner age\n\nmodel_jumper_q_age &lt;- glmer(jumper ~ questioner_age + (1|session_id/talk_id), data = subset(jumperdata, !is.na(questioner_age)), family = \"binomial\") \n\nmodel_jumper_q_age_null &lt;- glmer(jumper ~ 1 + (1|session_id/talk_id), data = subset(jumperdata, !is.na(questioner_age)), family = \"binomial\") \n\nanova(model_jumper_q_age, model_jumper_q_age_null) # not significant\n\nData: subset(jumperdata, !is.na(questioner_age))\nModels:\nmodel_jumper_q_age_null: jumper ~ 1 + (1 | session_id/talk_id)\nmodel_jumper_q_age: jumper ~ questioner_age + (1 | session_id/talk_id)\n                        npar    AIC    BIC  logLik deviance Chisq Df Pr(&gt;Chisq)\nmodel_jumper_q_age_null    3 114.06 128.20 -54.030   108.06                    \nmodel_jumper_q_age         5 117.36 140.92 -53.679   107.36 0.702  2      0.704\n\n## questioner gender\n\nmodel_jumper_q_gender &lt;- glmer(jumper ~ questioner_gender + (1|session_id/talk_id), data = subset(jumperdata, !is.na(questioner_gender)), family = \"binomial\") \n\nmodel_jumper_q_gender_null &lt;- glmer(jumper ~ 1 + (1|session_id/talk_id), data = subset(jumperdata, !is.na(questioner_gender)), family = \"binomial\") \n\nanova(model_jumper_q_gender, model_jumper_q_gender_null) # almost significant\n\nData: subset(jumperdata, !is.na(questioner_gender))\nModels:\nmodel_jumper_q_gender_null: jumper ~ 1 + (1 | session_id/talk_id)\nmodel_jumper_q_gender: jumper ~ questioner_gender + (1 | session_id/talk_id)\n                           npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_jumper_q_gender_null    3 187.37 202.19 -90.687   181.37          \nmodel_jumper_q_gender         4 186.35 206.10 -89.173   178.35 3.0267  1\n                           Pr(&gt;Chisq)  \nmodel_jumper_q_gender_null             \nmodel_jumper_q_gender          0.0819 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model_jumper_q_gender)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: jumper ~ questioner_gender + (1 | session_id/talk_id)\n   Data: subset(jumperdata, !is.na(questioner_gender))\n\n     AIC      BIC   logLik deviance df.resid \n   186.3    206.1    -89.2    178.3     1026 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.1572 -0.1572 -0.1572 -0.1014  9.8590 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 0        0       \n session_id         (Intercept) 0        0       \nNumber of obs: 1030, groups:  talk_id:session_id, 339; session_id, 64\n\nFixed effects:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -4.5768     0.4495 -10.182   &lt;2e-16 ***\nquestioner_genderM   0.8764     0.5300   1.654   0.0982 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nqstnr_gndrM -0.848\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n## host age\n\nmodel_jumper_h_age &lt;- glmer(jumper ~ host_1_age + (1|session_id/talk_id), data = subset(jumperdata, !is.na(host_1_age)), family = \"binomial\") \n\nmodel_jumper_h_age_null &lt;- glmer(jumper ~ 1 + (1|session_id/talk_id), data = subset(jumperdata, !is.na(host_1_age)), family = \"binomial\") \n\nanova(model_jumper_h_age, model_jumper_h_age_null) # not significant\n\nData: subset(jumperdata, !is.na(host_1_age))\nModels:\nmodel_jumper_h_age_null: jumper ~ 1 + (1 | session_id/talk_id)\nmodel_jumper_h_age: jumper ~ host_1_age + (1 | session_id/talk_id)\n                        npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_jumper_h_age_null    3 129.37 143.27 -61.686   123.37          \nmodel_jumper_h_age         5 130.14 153.30 -60.068   120.14 3.2346  2\n                        Pr(&gt;Chisq)\nmodel_jumper_h_age_null           \nmodel_jumper_h_age          0.1984\n\n## host gender\n \nmodel_jumper_h_gender &lt;- glmer(jumper ~ host_1_gender + (1|session_id/talk_id), data = subset(jumperdata, !is.na(host_1_gender)), family = \"binomial\") \n\nmodel_jumper_h_gender_null &lt;- glmer(jumper ~ 1 + (1|session_id/talk_id), data = subset(jumperdata, !is.na(host_1_gender)), family = \"binomial\") \n\nanova(model_jumper_h_gender, model_jumper_h_gender_null) # not significant\n\nData: subset(jumperdata, !is.na(host_1_gender))\nModels:\nmodel_jumper_h_gender_null: jumper ~ 1 + (1 | session_id/talk_id)\nmodel_jumper_h_gender: jumper ~ host_1_gender + (1 | session_id/talk_id)\n                           npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_jumper_h_gender_null    3 169.48 184.15 -81.741   163.48          \nmodel_jumper_h_gender         4 171.16 190.72 -81.579   163.16 0.3243  1\n                           Pr(&gt;Chisq)\nmodel_jumper_h_gender_null           \nmodel_jumper_h_gender           0.569\n\n# nothing is significant, trend for gender questioner\n\nThese results show is that the likelihood of a person jumping a question not significantly affected by any of the variables. There was however a tendency for men to be more likely to jump a question compared to women. Note that the inference of these models is however limited, since jumpers were rare (N = 18)."
  },
  {
    "objectID": "qmd/9_other_gender_disparity.html#speaker-over-time",
    "href": "qmd/9_other_gender_disparity.html#speaker-over-time",
    "title": "10  What other gender disparities do we observe in oral sessions?",
    "section": "10.3 Speaker over time",
    "text": "10.3 Speaker over time\nNext, we investigated the probability that a speaker talks for longer than their allocated speaking time is affected by speaker gender or career stage. We did not expect that any other confounding variables would explain variation in speaking overtime, since this is something that was prepared by only the speaker.\n\n# have to summarize by talk not per question\ndata_overtime &lt;- data_analysis %&gt;% select(talk_id, session_id, overtime, speaker_gender, speaker_career_short) %&gt;% unique()\n\n# N = \nnrow(data_overtime)\n\n[1] 340\n\n# explore data\ntable(data_overtime$overtime, data_overtime$speaker_gender) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nF\nM\n\n\n\n\nN\n181\n97\n\n\nY\n29\n21\n\n\n\n\n\n\ntable(data_overtime$overtime, data_overtime$speaker_career_short) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nEarly career\nLate career\nMid career\n\n\n\n\nN\n113\n18\n123\n\n\nY\n22\n9\n14\n\n\n\n\n\n\n# build initial models\n\n## speaker gender\nmodel_overtime_s_gender &lt;- glmer(overtime ~ speaker_gender + (1|session_id/talk_id), data = subset(data_overtime, !is.na(speaker_gender)), family = \"binomial\") \n\nmodel_overtime_s_gender_null &lt;- glmer(overtime ~ 1 + (1|session_id/talk_id), data = subset(data_overtime, !is.na(speaker_gender)), family = \"binomial\") \n\nanova(model_overtime_s_gender, model_overtime_s_gender_null) # not significant\n\nData: subset(data_overtime, !is.na(speaker_gender))\nModels:\nmodel_overtime_s_gender_null: overtime ~ 1 + (1 | session_id/talk_id)\nmodel_overtime_s_gender: overtime ~ speaker_gender + (1 | session_id/talk_id)\n                             npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_overtime_s_gender_null    3 160.29 171.67 -77.145   154.29          \nmodel_overtime_s_gender         4 162.21 177.38 -77.105   154.21 0.0795  1\n                             Pr(&gt;Chisq)\nmodel_overtime_s_gender_null           \nmodel_overtime_s_gender           0.778\n\n## speaker career \nmodel_overtime_s_career &lt;- glmer(overtime ~ speaker_career_short + (1|session_id/talk_id), data = subset(data_overtime, !is.na(speaker_career_short)), family = \"binomial\") \n\nmodel_overtime_s_career_null &lt;- glmer(overtime ~ 1 + (1|session_id/talk_id), data = subset(data_overtime, !is.na(speaker_career_short)), family = \"binomial\") \n\nanova(model_overtime_s_career, model_overtime_s_career_null) # significant\n\nData: subset(data_overtime, !is.na(speaker_career_short))\nModels:\nmodel_overtime_s_career_null: overtime ~ 1 + (1 | session_id/talk_id)\nmodel_overtime_s_career: overtime ~ speaker_career_short + (1 | session_id/talk_id)\n                             npar    AIC    BIC   logLik deviance  Chisq Df\nmodel_overtime_s_career_null    3 253.24 264.34 -123.621   247.24          \nmodel_overtime_s_career         5 148.76 167.26  -69.381   138.76 108.48  2\n                             Pr(&gt;Chisq)    \nmodel_overtime_s_career_null               \nmodel_overtime_s_career       &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model_overtime_s_career)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: overtime ~ speaker_career_short + (1 | session_id/talk_id)\n   Data: subset(data_overtime, !is.na(speaker_career_short))\n\n     AIC      BIC   logLik deviance df.resid \n   148.8    167.3    -69.4    138.8      294 \n\nScaled residuals: \n      Min        1Q    Median        3Q       Max \n-0.005965 -0.002171 -0.001696 -0.001311  0.078898 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n talk_id:session_id (Intercept) 2882.12  53.685  \n session_id         (Intercept)   83.74   9.151  \nNumber of obs: 299, groups:  talk_id:session_id, 299; session_id, 63\n\nFixed effects:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     -12.700754   0.001747 -7271.0   &lt;2e-16 ***\nspeaker_career_shortLate career   1.103260   0.001747   631.6   &lt;2e-16 ***\nspeaker_career_shortMid career   -0.566997   0.001747  -324.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) sp__Lc\nspkr_crr_Lc 0.000        \nspkr_crr_Mc 0.000  0.000 \noptimizer (Nelder_Mead) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0404888 (tol = 0.002, component 1)\n\n# use helper function to collect output\n\nmodel_overtime_out &lt;- collect_out(model = model_overtime_s_career, null = model_overtime_s_career_null, n_factors = 2, name = \"m_overtime\", type = \"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nmodel_overtime_out %&gt;% t() %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nmodel_name\nm_overtime\n\n\nAIC\n148.762\n\n\nn_obs\n299\n\n\nlrt_pval\n0\n\n\nlrt_chisq\n108.48\n\n\nintercept_estimate\n-12.701\n\n\nintercept_estimate_prop\n0\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-12.704\n\n\nintercept_ci_higher\n-12.697\n\n\nn_factors\n2\n\n\nest_speaker_career_shortLate career\n1.103\n\n\nlowerCI_speaker_career_shortLate career\n1.1\n\n\nhigherCI_speaker_career_shortLate career\n1.107\n\n\npval_speaker_career_shortLate career\n0\n\n\nzval_speaker_career_shortLate career\n631.607\n\n\nest_speaker_career_shortMid career\n-0.567\n\n\nlowerCI_speaker_career_shortMid career\n-0.57\n\n\nhigherCI_speaker_career_shortMid career\n-0.564\n\n\npval_speaker_career_shortMid career\n0\n\n\nzval_speaker_career_shortMid career\n-324.618\n\n\n\n\n\n\n\nThe model output shows that late career speakers were more likely to speak overtime and mid-career speakers were the least likely to speak overtime."
  },
  {
    "objectID": "qmd/9_other_gender_disparity.html#critical-question",
    "href": "qmd/9_other_gender_disparity.html#critical-question",
    "title": "10  What other gender disparities do we observe in oral sessions?",
    "section": "10.4 Critical question",
    "text": "10.4 Critical question\nNext, we assessed whether the likelihood of receiving a ‘critical’ question is affected by the gender and age of the questioner or the gender and career stage of the speaker.\n\n10.4.1 Receiving\n\n# explore data\ntable(data_analysis$question_type_e, data_analysis$questioner_gender) %&gt;% kbl() %&gt;%  kable_classic_2()\n\n\n\n\n\nF\nM\n\n\n\n\n0\n458\n499\n\n\n1\n19\n23\n\n\n\n\n\n\ntable(data_analysis$question_type_e, data_analysis$questioner_age) %&gt;% kbl() %&gt;%  kable_classic_2()\n\n\n\n\n\n1\n2\n3\n\n\n\n\n0\n381\n294\n102\n\n\n1\n5\n10\n13\n\n\n\n\n\n\ntable(data_analysis$question_type_e, data_analysis$speaker_gender) %&gt;% kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nF\nM\n\n\n\n\n0\n593\n349\n\n\n1\n19\n23\n\n\n\n\n\n\ntable(data_analysis$question_type_e, data_analysis$speaker_career_short) %&gt;% kbl() %&gt;%  kable_classic_2()\n\n\n\n\n\nEarly career\nLate career\nMid career\n\n\n\n\n0\n434\n82\n361\n\n\n1\n13\n3\n18\n\n\n\n\n\n\n# build initial models\n\n## speaker gender\nmodel_critical_s_gender &lt;- glmer(question_type_e ~ speaker_gender + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_gender)), family = \"binomial\") \n\nmodel_critical_s_gender_null &lt;- glmer(question_type_e ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_gender)), family = \"binomial\") \n\nanova(model_critical_s_gender, model_critical_s_gender_null) # not significant, sort of trend\n\nData: subset(data_analysis, !is.na(speaker_gender))\nModels:\nmodel_critical_s_gender_null: question_type_e ~ 1 + (1 | session_id/talk_id)\nmodel_critical_s_gender: question_type_e ~ speaker_gender + (1 | session_id/talk_id)\n                             npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_critical_s_gender_null    3 317.43 332.11 -155.72   311.43          \nmodel_critical_s_gender         4 318.13 337.69 -155.06   310.12 1.3065  1\n                             Pr(&gt;Chisq)\nmodel_critical_s_gender_null           \nmodel_critical_s_gender           0.253\n\n## speaker career stage\nmodel_critical_s_career &lt;- glmer(question_type_e ~ speaker_career_short + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_career_short)), family = \"binomial\") \n\nmodel_critical_s_career_null &lt;- glmer(question_type_e ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(speaker_career_short)), family = \"binomial\") \n\nanova(model_critical_s_career, model_critical_s_career_null) # not significant\n\nData: subset(data_analysis, !is.na(speaker_career_short))\nModels:\nmodel_critical_s_career_null: question_type_e ~ 1 + (1 | session_id/talk_id)\nmodel_critical_s_career: question_type_e ~ speaker_career_short + (1 | session_id/talk_id)\n                             npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_critical_s_career_null    3 263.75 278.20 -128.88   257.75          \nmodel_critical_s_career         5 267.34 291.42 -128.67   257.34 0.4086  2\n                             Pr(&gt;Chisq)\nmodel_critical_s_career_null           \nmodel_critical_s_career          0.8152\n\n## nothing significant\n\n\n\n10.4.2 Giving\n\n## questioner gender\nmodel_critical_q_gender &lt;- glmer(question_type_e ~ questioner_gender + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_gender)), family = \"binomial\") \n\nmodel_critical_q_gender_null &lt;- glmer(question_type_e ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_gender)), family = \"binomial\") \n\nanova(model_critical_q_gender, model_critical_q_gender_null) # not significant\n\nData: subset(data_analysis, !is.na(questioner_gender))\nModels:\nmodel_critical_q_gender_null: question_type_e ~ 1 + (1 | session_id/talk_id)\nmodel_critical_q_gender: question_type_e ~ questioner_gender + (1 | session_id/talk_id)\n                             npar    AIC    BIC  logLik deviance Chisq Df\nmodel_critical_q_gender_null    3 317.28 332.00 -155.64   311.28         \nmodel_critical_q_gender         4 318.42 338.04 -155.21   310.42 0.863  1\n                             Pr(&gt;Chisq)\nmodel_critical_q_gender_null           \nmodel_critical_q_gender          0.3529\n\n## questioner age\nmodel_critical_q_age &lt;- glmer(question_type_e ~ questioner_age + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_age)), family = \"binomial\") \n\nmodel_critical_q_age_null &lt;- glmer(question_type_e ~ 1 + (1|session_id/talk_id), data = subset(data_analysis, !is.na(questioner_age)), family = \"binomial\") \n\nanova(model_critical_q_age, model_critical_q_age_null) # significant, older higher probability\n\nData: subset(data_analysis, !is.na(questioner_age))\nModels:\nmodel_critical_q_age_null: question_type_e ~ 1 + (1 | session_id/talk_id)\nmodel_critical_q_age: question_type_e ~ questioner_age + (1 | session_id/talk_id)\n                          npar    AIC    BIC  logLik deviance  Chisq Df\nmodel_critical_q_age_null    3 203.28 217.36 -98.642   197.28          \nmodel_critical_q_age         5 197.07 220.53 -93.536   187.07 10.212  2\n                          Pr(&gt;Chisq)   \nmodel_critical_q_age_null              \nmodel_critical_q_age         0.00606 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmodel_critical_q_age_out &lt;- collect_out(model = model_critical_q_age, null = model_critical_q_age_null, n_factors = 2, name = \"m_critical_q_age\", type = \"qa\", save = \"yes\", dir = \"../results/question-asking/\")\n\nmodel_critical_q_age_out %&gt;% t() %&gt;% kbl() %&gt;%  kable_classic_2()\n\n\n\n\nmodel_name\nm_critical_q_age\n\n\nAIC\n197.072\n\n\nn_obs\n805\n\n\nlrt_pval\n0.006\n\n\nlrt_chisq\n10.212\n\n\nintercept_estimate\n-10.888\n\n\nintercept_estimate_prop\n0\n\n\nintercept_pval\n0\n\n\nintercept_ci_lower\n-10.893\n\n\nintercept_ci_higher\n-10.883\n\n\nn_factors\n2\n\n\nest_questioner_age2\n2.188\n\n\nlowerCI_questioner_age2\n2.183\n\n\nhigherCI_questioner_age2\n2.193\n\n\npval_questioner_age2\n0\n\n\nzval_questioner_age2\n826.121\n\n\nest_questioner_age3\n3.068\n\n\nlowerCI_questioner_age3\n3.063\n\n\nhigherCI_questioner_age3\n3.073\n\n\npval_questioner_age3\n0\n\n\nzval_questioner_age3\n1158.491\n\n\n\n\n\n\n# only age questioner significant\n\nThese results show is that the likelihood of a person asking or receiving a critical question was affected by the age category of the question asker. Note that the inference of these models is however limited, since critical questions were rare and subject to observer bias (N = 43)."
  },
  {
    "objectID": "qmd/10_additional_documents.html#code-of-conduct",
    "href": "qmd/10_additional_documents.html#code-of-conduct",
    "title": "11  Additional documents",
    "section": "11.1 Code of Conduct",
    "text": "11.1 Code of Conduct\nAll registrants had to agree to adhere to the Code of Conduct."
  },
  {
    "objectID": "qmd/10_additional_documents.html#protocol-question-asking",
    "href": "qmd/10_additional_documents.html#protocol-question-asking",
    "title": "11  Additional documents",
    "section": "11.2 Protocol question asking",
    "text": "11.2 Protocol question asking\nThe protocol used to collect data by observers on question asking."
  },
  {
    "objectID": "qmd/10_additional_documents.html#instructions-to-session-hosts",
    "href": "qmd/10_additional_documents.html#instructions-to-session-hosts",
    "title": "11  Additional documents",
    "section": "11.3 Instructions to session hosts",
    "text": "11.3 Instructions to session hosts\nThe (general) instructions given to session hosts assigned to manipulated sessions."
  },
  {
    "objectID": "qmd/10_additional_documents.html#full-post-congress-survey",
    "href": "qmd/10_additional_documents.html#full-post-congress-survey",
    "title": "11  Additional documents",
    "section": "11.4 Full post-congress survey",
    "text": "11.4 Full post-congress survey\nFull post-congress survey with summary statistics, including all questions and answers, including those not elaborated on in the manuscript"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Equity, diversity and inclusion at the Behaviour 2023 Congress",
    "section": "",
    "text": "1 This page\nOn this webpage, you will find the results from the manuscript “Equity, diversity and inclusivity at the Behaviour 2023 Congress”. This page will follow the same structure as the Results section of the manuscript, with additional details from the methods to display the important code snippets to e.g. define the statistical models, create the plots, etc. Additional scripts that were used for data curation, clean-up, additional exploration, etc. can be found in the github repository.\n\n\n\nOrganizing Team Behaviour 2023\n\n\n\n\n2 Introduction\nWe, as part of the Organizing Committee, investigated inequality at scientific conferences regarding multiple aspects of social identity using a case study: the international Behaviour 2023 congress that took place in Bielefeld, Germany, August 2023. The congress was attended by approximately 800 scientists including 661 presenters who gave talks in 98 different parallel sessions.\nThe Organizing Committee of the congress took multiple approaches in an attempt to boost inclusivity at the congress (see Methods for full details). To protect all attendees and ensure everyone understands expected and unacceptable behaviour during the congress, all congress attendees were obliged to agree to a Code of Conduct (CoC) when registering (Appendix A). To aid people with auditory, visual, mobility and / or dietary needs, we provided information and offered help on the website and in the congress registration form, which also helped us prepare helpful logistics and tools during the congress, such as quiet rooms. As underrepresented researchers from the Global South are more financially limited to attend international congresses, we offered a limited number of full travel grants to X researchers based in the Global South. During the congress, attendees were able to express concerns, report discrimination and inform the Organizing Committee of any harassment to an Awareness Team. Free childcare was offered and various parent-children rooms were reserved for attendees and partners. Further, we convened a symposium on “Equality, diversity and equity in behaviour, ecology and evolution” and organized EDI-related workshops given by external moderators to boost discussion and spread awareness. To facilitate the use of the correct pronouns, attendees could opt to print their pronouns on their nametags. Lastly, the Committee ensured that there was gender and ethnic diversity in (invited) plenary speakers, ensuring all continents (North, South and Central America, Europe, Africa, Oceania, and Asia) were represented and that at least half of the plenary speakers were female. XX something about other initiatives that could have been taken but we didn’t manage to, to show we’re not just trying to say we did a great job XX.\n\n\n3 This study\nTo assess diversity and inclusivity at the congress, we collected various types of data at several time points. First, we asked researchers who submitted an abstract to voluntarily provide us with information on their social identity in the pre-congress survey (N = 729 responses). A few months later, we repeated this pre-congress survey during congress registration (N = 727 responses). During the congress, we collected data on question asking behaviour to address gender disparity in question asking behaviour (N = 1278 questions). Moreover, we experimentally assessed whether session hosts choosing the first questioner to be a woman encourages other women to ask their questions (N = 209 manipulated talks with 784 questions). After the congress finished, we circulated a post-congress survey (N = 391 responses) which included questions on people’s congress experience, motivations to ask questions, reasons why they did not ask a question, perceptions of EDI-related in academia, as well as qualitative feedback on the inclusivity initiatives taken at the congress.\n\n\n\nLogo Behaviour 2023"
  }
]